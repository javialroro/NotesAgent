II-B4. Calculo de dimensiones:
m, Cantidad de pixeles en fila/columna. k, Tamano delkernel. p, Tamano delpadding. s, cantidad de pasos. m−k+2p
s+ 1 =Dimension resultante
II-C. Pesos
Si se tuviera una imagen de 224x224x3, con un tamano
de kernel 11, stride de 4 y padding de 0, y le aplicamos la
formula anterior da 55. Y con la arquitectura de AlexNet 2
obtenemos que aprendimos 96 de profundidad. II-D. Pesos Compartidos
Si ya tenemos un filtro que extrae cierta caracteristica, y
sirve para una posicion; tambien sirve para otra posicion. Por
lo que, vamos a usar el mismo filtro para toda la imagen. II-E. Transfer learning
Se menciona que en el paper de AlexNet despues de aplicar
su arquitectura y lo referente a ella. Y se dan cuenta que en
las primeras capas hay figuras o informacion similar. Por lo
que se introduce el termino detransfer learning, que consiste
en pasar el peso de las primeras capas a otra red, para ahorrar
tiempo de entrenamiento. III. ARQUITECTURASCONVOLUCIONALES
Se componen deConvolutional layer,Pooling layery
deDense layer. Se deben tomar desiciones sobre nuestra
arquitectura, por ejemplo, si la convolucion reduce el input
debo decidir si hago o no el pooling. Y estas desiciones
determinan el comportamiento del tamano de la imagen, pero
si la imagen es muy reducida, le llega poca informacion a
lafully connected. Se introdujoel termino de stack, que es,
INPUT→[[conv→relu]∗n→Pool?]∗m→[fc→
relu]∗k→fc, m≥0, k≥0, y se menciona que enpapers la cantidad de convoluciones y relu se usa la formula
de3≥n≥0. III-A. ¿Que arquitectura preferimos? Se prefiere a las arquitecturas con convoluciones pequenas,
ya que las convoluciones grandes nos llevan a que las neuronas
se computen de forma lineal y que la cantidad de pesos sea
mayor. III-B. Algunas "reglas"
El tamano de la imagen deberia ser divisible por 2. Las convoluciones deben usar campos receptivos peque-
nos 3x3, con un stride de 1. Parapooling layer es comunmax poolingde F=2, S=3. s, cantidad de pasos.