Apuntes de clase #2
Luis Felipe Calderon Perez
Escuela de Ingenieria en Computacion
Tecnologico de Costa Rica
Cartago, Costa Rica
2021048663
9-10-2025
Resumen—Este documento presenta los apuntes de la decima
semana del curso de inteligencia artificial. Se realizoun repaso
de las redes neuronales convolucionales (CNN), partiendo de las
limitaciones de las redesfully connectedy la necesidad de extraer
informacion espacial de las imagenes. Se estudioel funciona-
miento de la estructura de las capas convolucionales,pooling
yfully-connectedyfeature map. Finalmente, se discutieron los
principios para disenar arquitecturas convolucionales eficientes,
considerando tamanos de filtro,stride,paddingy reduccion de
parametros. Index Terms—CNN, capas convolucionales,transfer learning,
arquitecturas convolucionales. I. BREVE REPASO DE LA CLASE ANTERIOR
I-A. Redes Convolucionales
Recibimos un input de caracteristicas, el cual transformamos
con una serie de capas ocultas. Se habia visto hasta el momento las redesfully connected. Teniamos el problema de que aprendemos una secuencia de
pixeles, lo que nos lleva a errores si movemos los objetos de
lugar o si aplicamos rotacion a los objetos. Lo correcto seria
sacar la informacion de la imagen para tomar una decision. Tambien se hablodel dataset de CIFAR-10, donde hay
imagenes con 3 canales de 32x32. Y nos damos cuenta de
que no es escalable, ya que se tendrian 120,000 parametros
que ajustar unicamente en la entrada. Por ello se buscan metodos mas eficientes; entonces lle-
gamos a las ConvNet, en donde las neuronas se organizan
en 3 dimensiones: largo, ancho y profundidad (canales). Las
Figura 1. Arquitectura CNN vs NN [3]
mismas estaran conectadas a pequenas regiones de la capa
anterior, y esto reduce el tamano de la imagen a un vector. En cada cara del cubo de las neuronas se tienen n filtros
de tamano acorde a las imagenes de entrada y obtenemos un
Feature map, que es el resultado de aplicar el filtro a la imagen
anterior. I-B.