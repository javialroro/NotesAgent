•Sustituir o recolectar mejores features que representen de
manera adecuada el problema. IV. A LTAVARIANZA
Se presenta cuando el modelo se ajusta demasiado a los
datos de entrenamiento pero falla al generalizar en el con-
junto de validacion. Esto provoca overfitting , donde pequeñas
variaciones en los datos de entrada pueden generar malas
predicciones. A. Causas
•El modelo es demasiado complejo y aprende patrones
irrelevantes o ruido. •Exceso de dimensionalidad: agregar muchas variables
aumenta el riesgo de overfitting. •Muy pocos ejemplos en el conjunto de entrenamiento,
especialmente en problemas con clases desbalanceadas. B. Posibles soluciones
•Reducir la complejidad del modelo (ej. usar menos capas
o un modelo mas simple). •Disminuir la dimensionalidad eliminando variables irrel-
evantes. •Obtener mas ejemplos de entrenamiento para mejorar la
representacion de todas las clases. •Aplicar tecnicas de regularizacion que penalizan la com-
plejidad del modelo, como:
–L1 y L2 (penalizacion sobre los parametros). –Dropout (apagar ciertas neuronas durante el entre-
namiento). V. R EGRESIONLOGISTICA
Aunque su nombre incluya “regresion”, la regresion
logistica es un modelo de clasificacion, no de regresion. Se utiliza principalmente para problemas binarios , donde las
etiquetas ytoman los valores 0o1. A. Diferencia con la regresion lineal
•Enregresion lineal se predicen valores continuos en los
reales ( R). •Enregresion logistica se predice la probabilidad de
pertenecer a una clase u otra. El resultado final es una
clasificacion:0o1. Por ejemplo, con una variable como el tamaño de una
calabaza:
•Regresion lineal: predice el precio aproximado en valores
reales. •Regresion logistica: predice si la calabaza es naranja ( 1)
o no lo es ( 0). Fig. 7. Regresion lineal vs logistica
B. Distribucion de Bernoulli
Cada etiqueta yies una variable aleatoria que sigue una
distribucion de Bernoulli.