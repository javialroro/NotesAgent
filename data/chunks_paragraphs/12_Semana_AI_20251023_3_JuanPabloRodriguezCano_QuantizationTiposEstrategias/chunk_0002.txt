LLaMA 2 es un modelo muy popular y notorio por tener
un tamaño muy grande, tiene 70 mil millones de parametros,
cada uno esta representado por un punto flotante de 32 bits,
lo que resulta en 28GB que deberian estar en memoria si
se quisiera utilizar en una maquina local. Esto claramente
no es viable porque la mayoria de maquinas comerciales
cuentan con una capacidad menor a eso. Ademas, las opera-
ciones que se se hacen con datos de punto flotante son muy
lentas en comparacion a datos representados por enteros. La
cuantizacion hace una reduccion de los bits requeridos para
representar cada parametro y lo convierte a enteros, que se
pueden representar en las siguientes configuraciones: 8, 5, 2y hasta 1 bit. La cuantizacion resulta en un menor tiempo de
inferencia y menor consumo de energia, ademas de facilitar
la opcion de correr estos modelos en sistemas pequeños como
dispositivos moviles o sistemas embebidos. A. Representacion de numeros
Se suelen utilizar numeros en bloques de 8 bits para los
enteros, para representar numeros negativos se utiliza el com-
plemento a2 en los computadores. En contraste, para los punto
flotantes se utiliza el ieee-754, cuyo tamaño de representacion
es de 32 bits, se utiliza la siguiente formula. v= (−1)sign×2E−127×(1 +23X
i=1b23−i2−i)
Para no perder tanta informacion se tiene el siguiente
mecanismo:
1) Antes de que las entradas lleguen a la siguiente capa se
cuantizan los pesos
2) Estos pesos se limitan a ciertos rangos, dependiendo de
la cantidad de bits de la cuantizacion. Lo que se quiere
es que la distribucion sea equivalente. 3) Se hacen las operaciones con los datos de tipo entero. 4) Al salir de la capa, se de-cuantizan los pesos para que las
siguientes capas operen con numeros de punto flotante,
sin ”saber” que fueron cuantizados. III.