una epoch. Cada batch permite calcular la gradiente y actua-
lizar los parametros sin esperar a procesar todo el dataset.
Dependiendo de la estrategia, se puede actualizar los parame-
tros despues de cada batch o acumular gradientes antes de la
actualizacion.
C1. Batch Gradient Descent (Vanilla): ElBatch Gra-
dient Descent calcula la gradiente utilizando todo el dataset:
∇L=1
NNX
i=1∂L
∂θi
y actualiza los parametros solo despues de procesar el
conjunto completo.
Ventajas:
Gradiente estable y pasos consistentes.
Ayuda a evitar minimos locales y aporta robustez en la
optimizacion.
Desventajas:
Requiere todo el dataset en memoria.
Las actualizaciones son lentas para datasets grandes.
La gradiente muy estable puede ocultar señalesutiles.
Figura 3. Batch Gradient Descent
C2. Stochastic Gradient Descent (SGD): ElStochastic
Gradient Descent actualiza los parametros despues de cada
muestra del dataset (o un pequeño conjunto aleatorio de
muestras).
Ventajas:
Detecta rapidamente si el algoritmo puede converger.
´Util para datasets muy grandes.
Desventajas:
Las actualizaciones pueden ser muy ruidosas.
La trayectoria de los parametros es oscilatoria.
Muchas actualizaciones pueden ser costosas computacio-
nalmente.
w←w−α∂L
∂w
Figura 4. Stochastic Gradient DescentC3. Mini-batch Gradient Descent: ElMini-batch Gra-
dient Descent combina las estrategias anteriores: calcula la
gradiente sobre batches de tamaño intermedio.
Ventajas:
Reduce el ruido respecto a SGD y es mas estable.
Mas eficiente que Batch GD.
Mejora la explotacion de hardware (vectorizacion,
GPUs).
Figura 5. Mini-batch Gradient Descent
REFERENCIAS
[1] Belcak, P., et al, ”Small Language Models are the Future of Agentic
AI”2025.
[2] E. Brynjolfsson et al., “Canaries in the Coal Mine? Six Facts about the
Recent Employment Effects of Artificial Intelligence” 2025.