IV. Backpropagation
Permite calcular cuanto contribuye cada peso al
error final de la red, actualizando los parametros en
direccion opuesta a la propagacion hacia adelante.
Este proceso es esencial para que la red aprenda y
mejore su desempeÃ±o durante el entrenamiento.
A. Procesos del Entrenamiento
â€¢Forward Propagation:Consiste en calcular la
salida de la red enviando los datos desde la
capa de entrada hacia las capas siguientes, hasta
obtener el resultado final.
â€¢Backpropagation:Implica propagar el error
desde la capa de salida hacia las capas anteriores,
calculando las derivadas parciales con respecto
a los pesos y sesgos para ajustar los parametros
del modelo.
Fig. 24.Forward y Back Propagation
B. Optimizacion del grafo
En este ejemplo se considera una red neuronal en
la que cada capa contieneunicamente una neurona,
suponiendo que la funcion de activacion utilizada es
laSigmoide, como se muestra en la Figura??.
Fig. 25.Grafo de la red neuronal
â€¢Denominamos a las capas antes değ¿ ğ‘–,ğ‘ğ‘™hasta
ğ‘ğ‘™âˆ’ğ‘›.
â€¢Definimos el MSE como:
ğ¿ğ‘–=(ğ‘ğ‘™âˆ’ğ‘¦ğ‘–)2
â€¢Dividimos la neurona en 2 capas:
1)Entrada:ğ‘§ğ‘™=ğ‘¤ğ‘™ğ‘ğ‘™âˆ’1+ğ‘ğ‘™dondeğ‘ğ‘™âˆ’1
corresponde a los inputsğ‘¥.2)Salida:ğ‘ğ‘™=ğ‘”(ğ‘§ğ‘™)dondeğ‘”es nuestra
funcion de activacion.
Vamos a actualizar los parametros değ‘§ğ‘™, que sonğ‘¤ğ‘™
yğ‘ğ‘™. Para esto emplearemos laregla de la cadena,
usando la salida de la activacion de la capa anterior.
Profundizando a nivel de neurona, se muestra la
siguiente figura.
Fig. 26.Grafo de la capa al y Li a detalle
C. Vector gradiente
El vector gradiente se define como el conjunto
de derivadas parciales de los parametros (pesos y
sesgos) de la red neuronal. Al calcularlo, es comun
encontrar operaciones repetidas, lo que se aprovecha
en el algoritmo de backpropagation para optimizar
los calculos.
Fig. 27.Vector Gradiente