Fig. 5. Tangente hiperbolica
V. F UNCIONES DE ACTIVACION
Las funciones de activacion son un componente esencial
en las redes neuronales, ya que permiten introducir la no
linealidad necesaria para modelar relaciones complejas entre
los datos. A continuacion, se describen las funciones mas rel-
evantes junto con sus principales caracteristicas matematicas. A. Lineal
La funcion lineal se define como:
f(x) =ax
Su derivada es constante ( f′(x) =a), por lo que el modelo
no puede aprovechar el descenso del gradiente para aprender
patrones complejos. Debido a su caracter estrictamente lineal,
no introduce capacidad de generalizacion ni no linealidad en
la red. B. Sigmoide
La funcion sigmoide produce salidas entre 0 y 1, es siempre
positiva, acotada y estrictamente creciente:
σ(x) =1
1 +e−x
A pesar de su utilidad inicial, presenta el problema del
vanishing gradient : la derivada tiende a cero en los extremos
de la funcion, lo que hace que el aprendizaje sea lento o
incluso se detenga en redes profundas. C. Tangente Hiperbolica (Tanh)
La funcion tangente hiperbolica tiene un rango de salida
entre (−1,1)como vemos en la Figura 5 y su forma es similar
a la sigmoide, pero centrada en el origen:
tanh( x) =ex−e−x
ex+e−x
Esto permite representar tanto valores positivos como nega-
tivos, lo que facilita la convergencia del modelo. Sin embargo,
al igual que la sigmoide, tambien sufre del problema del
gradiente desvanecido en los extremos.D. Parametric ReLU (PReLU)
La funcionParametric ReLU (PReLU) es una variante
de la funcion ReLU tradicional, como se muestra en la
figura ??. A diferencia de la ReLU estandar, esta introduce
un parametro αque se aprende durante el entrenamiento y
controla la pendiente en la region negativa. De esta manera,
el modelo puede ajustar automaticamente el grado de “fuga”
en los valores menores que cero, evitando el problema de las
neuronas muertas .