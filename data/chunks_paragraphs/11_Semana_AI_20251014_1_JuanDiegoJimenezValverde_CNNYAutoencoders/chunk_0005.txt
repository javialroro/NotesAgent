Esta arquitectura demostro que aumentar la profundidad
mejora el rendimiento si se mantienen filtros pequeños y
consistentes. 6) ResNet:Introduce lasconexiones residuales, que per-
miten el paso de informacion entre capas no adyacentes. Esto
evita la degradacion del gradiente en redes muy profundas y
mejora la capacidad de entrenamiento. 7) DenseNet:Conecta cada capa con todas las anteriores,
favoreciendo la reutilizacion de caracteristicas y reduciendo
la cantidad de parametros necesarios. Este enfoque mejora la
eficiencia y el flujo de informacion a lo largo de la red. III. MATERIA DE CLASE
A. Problemas en las Redes Neuronales Convolucionales
1) Explicabilidad del Modelo:Uno de los principales de-
safios actuales es la falta de interpretabilidad en las redes
Fig. 1. Representacion de embeddings mediante t-SNE. profundas. Losfeaturesaprendidos por las capas internas
suelen ser dificiles de entender por los humanos, lo que
complica saber que esta “viendo” realmente el modelo. 2) Visualizacion y Analisis de Activaciones:Una forma de
entender mejor el funcionamiento interno es observar:
•Visualizacion de activaciones:muestra que regiones de
la imagen activan ciertas neuronas. •Visualizacion de filtros:permite observar los pesos
de los kernels. En las primeras capas, estos muestran
patrones reconocibles (bordes, colores, texturas), mientras
que en capas profundas se vuelven mas abstractos. Estos metodos ayudan a detectar si el modelo esta aprendiendo
caracteristicas relevantes o solo ruido. 3) Embeddings y Reduccion de Dimensionalidad:Las re-
des pueden transformar imagenes en representaciones vectori-
ales llamadasembeddings. Estas representaciones condensan
la informacion relevante de una imagen, permitiendo separar
clases en el espacio de caracteristicas. Al reducir la dimen-
sionalidad (manteniendo las distancias relativas), podemos
visualizar las relaciones entre clases.