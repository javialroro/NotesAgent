•Loss function: (fw,b(xi)−yi)2mide la penalidad o error
de cada ejemplo individual. •Error cuadratico: penaliza mas los errores grandes. •Cost function: promedio de la loss function en todo el
dataset; es una medida global del desempeno del modelo. •Objetivo: minimizar Lajustando los parametros wyb. Interpretacion
•Lpequeno →mejor modelo. •Lgrande →peor modelo. •Reducir Limplica mejorar la capacidad predictiva del
modelo. Funciones convexas vs. no convexas
•Convexa: garantiza un unico minimo global. •No convexa: pueden aparecer minimos locales y glob-
ales. D. Repaso de Derivadas
Reglas basicas
•f(x) =k⇒ f′(x) = 0
Ejemplo: f(x) = 2 ⇒ f′(x) = 0
•f(x) =x⇒ f′(x) = 1
•f(x) =kx⇒ f′(x) =k
Ejemplo: f(x) = 2 x⇒ f′(x) = 2
Potencias
•f(x) =xn⇒ f′(x) =nxn−1
Ejemplo: f(x) =x2⇒ f′(x) = 2 x
Suma
•f(x) =u(x) +v(x)⇒ f′(x) =u′(x) +v′(x)
Ejemplo: u(x) = 2 x, v (x) = 3 x⇒ f(x) =
5x, f′(x) = 5
Producto
•f(x) =u(x)v(x)⇒ f′(x) =u′(x)v(x)+u(x)v′(x)
Constante sumada
•f(x) =u(x) +z⇒ f′(x) =u′(x)
Ejemplo: f(x) = 2 x+ 5 ⇒ f′(x) = 2
Derivadas parciales
•Seaf(x, y) = 2 x+ 3y
∂f
∂x= 2,∂f
∂y= 3
E. Descenso del Gradiente
Concepto basico
•La cantidad de pasos se calcula como: pendiente ×α
(learning rate). •Ejemplo: si x= 1, el gradiente esdy
dx= 2x= 2. •Para acercarnos al minimo, nos movemos en la direccion
del gradiente negativo con un paso de tamano α. Regla de actualizacion
xnuevo =xantiguo−α·(2x)
•Donde 2xes el gradiente. •El proceso se repite hasta que el gradiente sea 0 (punto
de minimo).Importancia del α(learning rate)
•El tamano del paso αdebe ser pequeno (ejemplo: 0.1)
para no sobrepasar el minimo. •Al acercarnos al minimo, los saltos se reducen porque el
gradiente disminuye. •Unαmuy grande puede provocar oscilaciones o incluso
alejarse del minimo. •Unαdemasiado pequeno ralentiza la convergencia. Nota
•Ellearning rate (α) es un hiperparametro que debe
seleccionarse cuidadosamente. Fig. 1. Impacto del learning rate en gradient descent
F . ¿Por queusar MSE y no MAE?