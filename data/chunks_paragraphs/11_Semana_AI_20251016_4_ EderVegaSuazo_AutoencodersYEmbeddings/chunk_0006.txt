III-E0b. Deteccion de anomalias (ejemplo bancario):
Entrenar con transacciones validas. Para una transaccion nueva
x: calcularerr=L rec(x,x). Si el errorerres mayor a un
umbral, lo marca como posible fraude. Seleccion del umbral
τmediante ROC o validacion manual. Importante evaluar tasa
de falsos positivos y costo operativo. III-E0c. Denoise y super-resolution:Para super-
resolution el objetivo puede ser una imagen de alta resolucion
xHRy la entradax LR. Arquitecturas con skip-connections
(U-Net style) mejoran la preservacion de detalles. Se
recomienda usar una figura comparativa de entrada/resultado
en el informe experimental (ver Figura 3). Figura 3: Ejemplo sugerido: comparacion entrada ruidosa /
salida reconstruida / referencia. Nota: aplicaciones forenses (p. ej. mejora de camaras) un
compañero plantea las consideraciones legales sobre manipu-
lacion de evidencia. III-E0d. Segmentacion (U-Net):U-Net concatena ma-
pas de caracteristicas del encoder en el decoder. Esto restaura
informacion espacial perdida por pooling y mejora mapeo de
mascaras para segmentacion de objetos. (Se sugiere incluir una
figura de arquitectura U-Net y un ejemplo de mascara en la
entrega.)
Figura 4: Representacion de U-Net
III-F. Espacios latentes: visualizacion y utilidad (ampliado)
Visualizarzcon t-SNE/UMAP facilita ver separabilidad por
clases. Cuando los clusters son nitidos un clasificador simple
sobrezfuncionara bien. En V AE la continuidad del espacio
permite interpolar entre muestras y generar imagenes plausi-
bles no vistas. Ver Figura 5 para un ejemplo de visualizacion
t-SNE. Figura 5: Ejemplo de visualizacion t-SNE de vectores latentes. III-G. Transicion a NLP: tokenizacion y embeddings
III-G0a. Tokenizacion:Estrategias: palabra completa,
subword (BPE), caracter, bytes. Subword reduce OOV y
controla longitud de secuencia. Ver Figura 6 para un esquema
de tokenizacion subword. III-G0b. Embeddings:Cada token se mapea a un vector
e∈Rdmediante la capaEmbedding.