II-C. Capacidades Emergentes
El entrenamiento masivo de los LLMs les confiere capa-
cidades avanzadas queemergensin haber sido entrenados
directamente para ellas:
Razonamiento y planificacion. Aprendizaje en el prompt(In-context Learning): Adap-
tan el comportamiento a partir de ejemplos dados en la
entrada. Multitarea: Realizan traduccion, clasificacion y codifi-
cacion sin reentrenamiento. III. RETRIEVAL-AUGMENTEDGENERATION(RAG)
RAG es un paradigma que conecta un LLM con un mo-
dulo de recuperacion (retriever) para inyectarconocimiento
externo, actualizado y verificabledurante la generacion de
respuestas. III-A. Proceso y Flujo de RAG
1.Preparacion (Chunking): Los documentos se dividen
en fragmentos (chunks), que suelen contener entre200
y 500 tokens, a menudo conoverlappara preservar el
contexto. 2.Indexacion: Cadachunkse convierte en unembedding
y se almacena en unabase de datos vectorial(por
ejemplo, FAISS, Qdrant, Pinecone). 3.Consulta y recuperacion: La pregunta del usuario se
transforma en unembedding, se calcula la similitud
con los vectores indexados y se seleccionan lostop-k
chunksmas cercanos semanticamente. 4.Aumento y generacion: Loschunksrecuperados se
integran en una plantilla estructurada (prompt) como
contexto adicional, asegurando que la respuesta del
LLM sea precisa y fundamentada. III-B. Ventajas y Limitaciones
RAG ofrece lareduccion de alucinaciones, laactua-
lizacion continua del conocimientoy la aplicabilidad en
dominios especializados. No obstante, los sistemas RAG si-
guen siendopasivos; su funcion se limita a complementar la
respuesta del LLM con datos recuperados. Figura 2. Diagrama del flujo de un sistema RAG, desde la indexacion hasta
la generacion de la respuesta. Figura 3. Agente inteligente. IV. DELLMAAGENTEINTELIGENTE
LosAgentes Inteligentesbasados en LLMs superan la
pasividad de los sistemas RAG. Estos agentes puedenrazonar,
planificaryactuarde manera autonoma, interactuando con
el mundo real mediante herramientas externas. IV-A.