Apuntes Semana 4 Clase #2
28/08/2025
Alex Steven Naranjo Masis
Instituto Tecnol ´ogico de Costa Rica
Cartago, Costa Rica
Email: alnaranjo@estudiantec.cr
Resumen —Para esta clase se repasaron temas de la clase
anterior como lo son KNN, regresi ´on lineal, Mean Square Error,
Descenso del gradiente y un repaso general de derivadas. Y luego
del repaso continuamos viendo temas como lo son: Derivadas
Parciales con respecto a w y b en la funci ´on de p ´erdida con
el fin de actualizarlos y ajustar la funci ´on, y por ´ultimo vimos
Epoch y Batch.
Index Terms —KNN, Regresi ´on Lineal, Mean Square Error,
MAE, Descenso del Gradiente, Epoch y Batch
I. N OTICAS DE LA SEMANA
A. Small Language Models are the Future of Agentic AI
En el art ´ıculo se dice que los modelos de lenguaje peque ˜nos
(SLMs) son m ´as adecuados que los grandes (LLMs) para
ciertos sistemas inteligentes aut ´onomos (agentic AI), especial-
mente en tareas especializadas y repetitivas. [1]
B. Canaries in the Coal Mine? Six Facts about the Recent
Employment Effects of Artificial Intelligence
El estudio analiza c ´omo la adopci ´on de la inteligencia
artificial generativa ha afectado al mercado laboral en EE.UU.,
utilizando datos administrativos mensuales de n ´ominas de
ADP, el mayor procesador de n ´ominas del pa ´ıs, el cual abarca
millones de trabajadores en decenas de miles de empresas. [2]
II. R EPASO CLASE ANTERIOR
A. K Nearest Neighbor (kNN)
En resumen, cuando obtenemos una nueva instancia, me-
dimos contra todos los elementos del dataset, y tomamos las
distancias m ´as cercanas, y en base a eso determin ´abamos la
clase de la nueva instancia.
Contamos con el hiperpar ´ametro K.
Es un algoritmo de lazy learning, porque realmente no se
aprende de los datos.
A1. Ventajas:
Sencillo de implementar.
Es flexible: Aplica tanto para regresion como clasifica-
ci´on.
A2. Desventajas:
Las caracter ´ısticas irrelevantes pueden distorsionar las
distancias
Es computacionalmente costoso.
Poco eficiente en grandes vol ´umenes de datos.B. Regresi ´on Lineal
Lo que queremos hacer es encontrar la l ´ınea que mejor se
ajuste a los datos, para poder realizar una predicci ´on de un
valor.
B1. Variables:
Variables independientes: Son las caracter ´ısticas de la
muestra.
Variables dependientes: Es el valor a predecir y es
afectada por las varibales independientes
Con esto lo que queremos hacer es encontrar un modelo
estad ´ıstico lineal: fw,b(X) =wX +b
Donde:
X es un vector D-dimensional.
w es un vector D-dimensional.
b un n ´umero real.
wX es un producto punto, d ´andonos como resultado un
escalar.
El modelo est ´a parametrizado por wyb, por lo que
debemos encontrar los valores ´optimos de wybque har ´an
que la funci ´on realice las predicciones m ´as precisas. Pero ojo,
Optimo ̸=Perfecto
Figura 1. Tipos de Regresi ´on
C. Funci ´on de P ´erdida
Necesitamos de un m ´etodo qu ´e nos permita cuantificar qu ´e
tan bien se ajusta nuestro modelo a los datos. Funci ´on de
P´erdida = Medida del error del modelo

D. Error Cuadr ´atico Medio (MSE)
Es el resultado del modelo contra la etiqueta. Sumamos
todos los errores de los samples y lo promediamos.
L=1
NNX
i=1(fw,b(xi)−yi)2
D1. Conceptos Clave):
Loss Function: (fw,b(xi)−yi)2es la medida de pena-
lidad que cuantifica el error de cada ejemplo.
Error Cuadr ´atico: Penaliza los errores grandes.
Cost Function: Es el promedio de la Loss Function sobre
todo el dataset.
Objetivo: Minimizar L para ajustar los parametros w,b.
El motivo por el cual queremos minimizar L, es porque
entre menor sea L, significa que tenemos un mejor modelo, y
entre m ´as grande significa que tenemos un peor modelo.
E. ¿Por qu ´e MSE y no MAE?
Es debido a qu ´e es cuadr ´atica, y esto nos asegura que vamos
a tener un punto m ´ınimo. Y t ´ambien es porque la funci ´on no
MAE no es smooth, por lo que no nos va a permitir obtener
las derivadas en todos los puntos, lo que induce a errores de
c´alculo
F . Derivadas Generales
Regla Funci ´onf(x) Derivada f′(x)
Constante k 0
Identidad x 1
Constante multiplicativa kx k
Potencia xnnxn−1
Suma u(x) +v(x) u′(x) +v′(x)
Producto u(x)v(x) u′(x)v(x) +u(x)v′(x)
Constante sumada u(x) +z u′(x)
Derivadas parciales f(x, y) = 2x+ 3y∂f
∂x= 2,∂f
∂y= 3
Cuadro I
REPASO DE DERIVADAS B ´ASICAS
G. Descenso del gradiente
El descenso del gradiente es un algoritmo iterativo de opti-
mizaci ´on para encontrar el m ´ınimo de una funci ´on. Funciona
actualizando repetidamente los par ´ametros en la direcci ´on
opuesta al gradiente de la funci ´on de costo.
G1. Regla de actualizaci ´on:
xnuevo =xantiguo−α·(2x)
G2. Importancia del α:Es el learning rate, debe ser
peque ˜no para no pasarnos del punto m ´ınimo. Este es un
hiperpar ´ametro
Figura 2. Comparaci ´on de distintos valores para alpha
III. C ONTENIDO DE LA CLASE
A. Funci ´on de P ´erdida y sus Derivadas Parciales
Para optimizar los par ´ametros wybde nuestro modelo,
necesitamos actualizar sus valores de manera que la funci ´on
de p ´erdida se minimice. Para esto, evaluamos c ´omo cada
par´ametro afecta la p ´erdida utilizando derivadas parciales con
respecto a wyb.
Considerando la funci ´on de p ´erdida basada en el error
cuadr ´atico medio (MSE) para nuestro modelo lineal fw,b(x) =
wx+b, tenemos:
L(w, b) =1
NNX
i=1((wxi+b)−yi)2
Las derivadas parciales de Lcon respecto a wybse
calculan como:
∂L
∂w=2
NNX
i=1((wxi+b)−yi)xi
∂L
∂b=2
NNX
i=1((wxi+b)−yi)
Estas derivadas nos indican la direcci ´on y magnitud del
ajuste necesario para cada par ´ametro, permitiendo aplicar
algoritmos de optimizaci ´on como el gradient descent para
actualizar wyb.
B. Epoch
Una epoch es una iteraci ´on completa sobre todo el conjunto
de entrenamiento. Es un hiperpar ´ametro que define cu ´antas ve-
ces se recorrer ´a el dataset completo durante el entrenamiento,
por ejemplo, epochs = 5 .
Si tenemos 10 000 muestras y ejecutamos 5 epochs, signi-
fica que se procesar ´an todas las muestras 5 veces en total. La
actualizaci ´on de los par ´ametros puede realizarse al finalizar
cada epoch o de manera m ´as frecuente utilizando batches.
C. Batch
Unbatch es un subconjunto del conjunto de entrenamiento
que se utiliza para calcular la gradiente y actualizar los
par´ametros del modelo.
Por ejemplo, si tenemos 10 000 muestras y un batch
size = 1 000 , necesitaremos 10 batches para completar

una epoch. Cada batch permite calcular la gradiente y actua-
lizar los par ´ametros sin esperar a procesar todo el dataset.
Dependiendo de la estrategia, se puede actualizar los par ´ame-
tros despu ´es de cada batch o acumular gradientes antes de la
actualizaci ´on.
C1. Batch Gradient Descent (Vanilla): ElBatch Gra-
dient Descent calcula la gradiente utilizando todo el dataset:
∇L=1
NNX
i=1∂L
∂θi
y actualiza los par ´ametros solo despu ´es de procesar el
conjunto completo.
Ventajas:
Gradiente estable y pasos consistentes.
Ayuda a evitar m ´ınimos locales y aporta robustez en la
optimizaci ´on.
Desventajas:
Requiere todo el dataset en memoria.
Las actualizaciones son lentas para datasets grandes.
La gradiente muy estable puede ocultar se ˜nales ´utiles.
Figura 3. Batch Gradient Descent
C2. Stochastic Gradient Descent (SGD): ElStochastic
Gradient Descent actualiza los par ´ametros despu ´es de cada
muestra del dataset (o un peque ˜no conjunto aleatorio de
muestras).
Ventajas:
Detecta r ´apidamente si el algoritmo puede converger.
´Util para datasets muy grandes.
Desventajas:
Las actualizaciones pueden ser muy ruidosas.
La trayectoria de los par ´ametros es oscilatoria.
Muchas actualizaciones pueden ser costosas computacio-
nalmente.
w←w−α∂L
∂w
Figura 4. Stochastic Gradient DescentC3. Mini-batch Gradient Descent: ElMini-batch Gra-
dient Descent combina las estrategias anteriores: calcula la
gradiente sobre batches de tama ˜no intermedio.
Ventajas:
Reduce el ruido respecto a SGD y es m ´as estable.
M´as eficiente que Batch GD.
Mejora la explotaci ´on de hardware (vectorizaci ´on,
GPUs).
Figura 5. Mini-batch Gradient Descent
REFERENCIAS
[1] Belcak, P., et al, ”Small Language Models are the Future of Agentic
AI”2025.
[2] E. Brynjolfsson et al., “Canaries in the Coal Mine? Six Facts about the
Recent Employment Effects of Artificial Intelligence” 2025.