Apuntes de clase: Redes Neuronales con el dataset MNIST
Fabián Díaz Barboza
Estudiante Ing. Computación
Tecnológico de Costa Rica
Cartago, Costa Rica
fdiaz@estudiantec.cr
23/09/2025
1El Dataset MNIST y la Representación de
Características
1.1 Descripción del Dataset MNIST
Imágenes en blanco y negro (1 canal).
10 clases (dígitos 0–9).
Tamaño estándar:28 ×28píxeles (entrada común-
mente utilizada).
Conjunto: 60000 ejemplos de entrenamiento y 10000
de prueba.
1.2 Proceso de Aplanamiento (Flattening)
Una imagen de entrada X∈R28×28se convierte en un
vector columna medianteflatten:
x∈R784,28×28 = 784.
Cada uno de los 784 elementos es una característica
(feature) que alimenta el modelo.
Figura 1: Ejemplo de la representación de un dígito en
MNIST como matriz28 ×28y su aplanamiento a un
vector de 784 características.1.3Píxeles Activos e Inactivos: La Semántica del
Input
Un píxel con intensidad 0 se considera “apagado” y
valores altos indican un píxel “encendido”.
Incluso la regresión logística binaria más simple exige
784pesos(w i) + 1sesgo(b) = 785parámetros,
lo que muestra la complejidad del espacio de entrada.
2La Regresión Logística Binaria: La Neurona
Fundamental
2.1 Clasificación Binaria como Problema Inicial
La regresión logística estima la probabilidad de que
una entrada pertenezca a la clase positiva; la salida está
en(0,1).
2.2 Ecuaciones Fundamentales de la Neurona
Potencial de activación:
z=w⊤x+b.
Función sigmoide:
g(z) =1
1 +e−z.
Salida del modelo:
ˆy=h(x) =g(w⊤x+b).
Figura: diagrama esquemático de la neurona
(Entradas→combinación lineal→activación→
salida)
Figura 2: Diagrama esquemático que interpreta la regre-
sión logística como la neurona más simple.
1

3Extensión a la Clasificación Multinomial y la
Codificación One-Hot
3.1Ejemplo de clase: 10 Regresiones Logísticas, una
por alumno
Para manejar las 10 clases se puede entrenar una regre-
sión logística por estudiante (una por clase); la capa de
salida tendría 10 neuronas (una por clase).
3.2 Codificación One-Hot de las Etiquetas (y)
La etiqueta escalar se codifica como un vector one-hot
enR10.
Clase (dígito) Vector One-Hot (y∈R10) Esperada
0[1,0,0,0,0,0,0,0,0,0]Neurona 0
2[0,0,1,0,0,0,0,0,0,0]Neurona 2
9[0,0,0,0,0,0,0,0,0,1]Neurona 9
Cuadro 1: Codificación one-hot de etiquetas (ejemplos).
4 Compactación por Álgebra Lineal
4.1 Formulación Matricial de Pesos y Sesgos
Stackeando los vectores de pesos obtenemos la matriz
de pesos y el vector de sesgos:
W∈R10×784, b∈R10.
La combinación lineal de la capa de salida se escribe
como:
z=Wx+b, z∈R10.
Elemento Símbolo Dimensión
Entradax784×1
Matriz de pesosW10×784
Sesgosb10×1
Potencial de activaciónz10×1
Cuadro 2: Dimensiones en la formulación matricial para
MNIST.
Figura 3: Matriz de pesos Wen la capa fully connected:
cada fila corresponde a una neurona de salida y cada
columna a un píxel de entrada.4.2Ejemplo Numérico de Clase: De Vector a Matriz
V.B.1. Cálculo de una sola regresión (vector de
4 features):
w=
3
2
4
5
, b= 2, x=
3
4
5
6
.
z=w⊤x+b= (3·3) + (2·4) + (4·5) + (5·6) + 2 = 69.
ˆy=σ(z).
V.B.2. Cálculo de varias regresiones a la vez (2
neuronas):
W=3 2 4 5
4 3 2 1
, b=2
3
, x=
3
4
5
6
.
z=Wx+b=69
43
.
5Arquitectura de las Redes Neuronales Profun-
das
5.1 Definición y Estructura Típica
Unared neuronal artificiales un modelo de cómputo
inspirado en el cerebro humano, compuesto por unidades
llamadasneuronas artificiales. Cada neurona recibe
un conjunto de entradas x, aplica una combinación lineal
con sus pesos wy un sesgo b, y luego pasa el resultado
por una función de activacióng:
h(x) =g(w⊤x+b).
Capa de entrada:recibe los 784 píxeles (flatten).
Capas ocultas:transforman la información en re-
presentaciones abstractas.
Capa de salida:entrega la predicción (10 neuronas
para MNIST).
Figura 4: Ejemplo esquemático de una red neuronal con
capa de entrada, capa(s) oculta(s) y capa de salida.
2

5.2 El Rol del Sesgob
Retomando, el parámetro b(bias o sesgo) podriamos
verlo como undesplazamientoen la función de acti-
vación. Sin b, todas las funciones aprendidas por la red
tenderían a pasar por el origen, lo que limita la flexibilidad
del modelo.
En el caso de MNIST:
Tenemos10regresiones logísticas (una por cada cla-
se).
Cada regresión tiene un vector de pesos wi∈R784y
un sesgob i.
Enconjunto,lospesosformanlamatriz W∈R10×784
y los sesgos forman un vectorb∈R10.
Es importante corregir una confusión que se habló en
clase: no existe un único bde dimensión784por ejemplo.
En cambio, hayun sesgo por neurona de salida. Cada
componente biactúa como umbral independiente para la
neurona i, permitiendo desplazar su función de activación
y ajustar su probabilidad de disparo de forma individual.
5.3 Fully Connected (Completamente Conectadas)
Las capasfully connected (FC)son aquellas en las
que cada neurona de una capa se conecta con todas las
neuronas de la capa anterior.
En nuestro ejemplo de MNIST:
Cada neurona de salida (de las 10) recibe conexión
de los 784 píxeles de entrada.
Cada conexión tiene su propio peso, y además cada
neurona tiene su sesgob i.
Esta estructura convierte el modelo en un clasifica-
dor mucho más potente que una sola regresión logística
binaria, porque permite:
1.Aprender múltiples fronteras de decisión en paralelo.
2.Combinar la información de todos los píxeles de
forma diferenciada para cada clase.
3. Ajustar umbrales específicos gracias a losb i.
En otras palabras, una red fully connected extiende el
poder de una regresión logística binaria: al apilar capas
con activaciones no lineales, las salidas de una capa se
convierten en features no lineales que alimentan la si-
guiente, permitiendo construir clasificadores mucho más
expresivos.
5.3.1 De la Multiclase al Clasificador Binario
Una arquitectura útil consiste en usar primero las 10 re-
gresiones logísticas (capa multiclase) y luego aplicar sobre
su salida un clasificador binario adicional. Por ejemplo,
para determinar si la imagen corresponde al dígito “5” o
no, la decisión puede tomarse a partir de las 10 salidas
(o de una combinación entrenada de ellas), en lugar de
hacerlo directamente sobre los píxeles. De este modo, las
capas previas actúan como extraedores de características
no lineales que potencian una decisión binaria final más
robusta.5.4 Propiedades Esenciales de la Red
1.Nolinealidad:lasfuncionesdeactivación(sigmoide,
ReLU, etc.) permiten que la red modelice relaciones
no lineales entre entradas y salidas.
2.Capasyprofundidad:amayorprofundidad,mayor
capacidad para representar abstracciones jerárquicas.
3.Diferenciabilidad:la diferenciabilidad de las fun-
ciones internas es requisito para aplicar retropropaga-
ción y optimizar los parámetros mediante gradiente
descendente.
6 Conclusiones
En conclusión de la clase, las redes neuronales son como
una evolución natural de la regresión logística: partien-
do de la clasificación binaria, pasando por la extensión
multinomial y compactando parámetros mediante álge-
bra lineal, se llega a arquitecturas fully connected que
permiten mayor expresividad y paralelización. La ecua-
ciónz=Wx+bnos sintetiza el paso fundamental hacia
la representación matricial; pero el verdadero salto en
capacidad proviene de combinar esa formulación con fun-
ciones de activación no lineales y con múltiples capas
diferenciables, lo que habilita la retropropagación y el
entrenamiento eficiente de modelos capaces de abstraer
características complejas de datos como MNIST.
3