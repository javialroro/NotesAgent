Inteligencia Artificial
Apuntes de Clase — 14 de agosto de 2025
1stKendall Rodr ´ıguez Camacho
Escuela de Ingenieria en Computaci ´on
Instituto Tecnol ´ogico de Costa Rica
Cartago, Costa Rica
Kenrodriguez@estudiantec.cr
Abstract —En este documento se presentan los apuntes cor-
respondientes a la clase del 14 de agosto de 2025 del curso de
Inteligencia Artificial. En primer lugar, se incluye un resumen de
la sesi ´on anterior, en el que se revisan conceptos de IA y Machine
Learning. Posteriormente, se introduce el ´algebra lineal aplicada
mediante Python, empleando librer ´ıas como PyTorch, NumPy y
Pandas, con el objetivo de familiarizarse con las herramientas y
su implementaci ´on pr ´actica.
Index Terms —Machine Learning, MLOps, Models, Deep
Learning, Generative AI, Vectores, Tensores, Matrices
I. I NTRODUCTION
La Inteligencia Artificial incluye diversas t ´ecnicas, entre
ellas el Machine Learning o aprendizaje autom ´atico, que se
encarga de que los modelos aprendan de los datos. En esta
clase se present ´o un resumen de los diferentes paradigmas
de resoluci ´on de problemas en IA, los distintos tipos de
aprendizaje y de las etapas que componen el ciclo de vida
de un modelo de ML. Adem ´as, se introdujo el ´algebra lineal
de forma pr ´actica utilizando Python y librer ´ıas como NumPy,
Pandas y PyTorch, trabajando con conceptos como tensores,
matrices y vectores.
II. N OTICIAS
A. ChatGPT 5, ¿un fracaso?
A pocos d ´ıas de su lanzamiento por parte de OpenAI, Chat-
GPT 5 comenz ´o a recibir cr ´ıticas por parte de la comunidad.
Muchos usuarios cuestionaron si realmente representaba una
mejora frente a ChatGPT 4, se ˜nalando que en varias ocasiones
el modelo tarda demasiado en generar una respuesta y, en
algunos casos, produce errores o respuestas de calidad inferior
a lo esperado.
B. Alexandr Wang y la inversi ´on de Meta
Alexandr Wang, un empresario de 28 a ˜nos, es el fundador
de Scale AI, una startup especializada en inteligencia artificial
y en el etiquetado masivo de datos para entrenar modelos. En
2025, Meta realiz ´o una inversi ´on significativa en la compa ˜n´ıa,
destacando el inter ´es de la empresa en reforzar su estrategia
en IA.
Wang se convirti ´o en uno de los multimillonarios m ´as
j´ovenes creados por s ´ı mismos, y su trabajo en Scale AI
lo posiciona como una figura relevante en el desarrollo y
aplicaci ´on de tecnolog ´ıas de inteligencia artificial avanzada.III. T IPOS DE APRENDIZAJE
En Machine Learning existen distintos tipos de aprendizaje,
cada uno con aplicaciones y caracter ´ısticas particulares:
•Supervisado: Se dispone de datos con entradas y salidas
conocidas X={Xi, Yi}, i= 1..N. El modelo aprende a
predecir la salida Ya partir de la entrada X.
•No supervisado: Solo se tienen entradas sin etiquetas. El
modelo identifica patrones o estructuras ocultas en los
datos.
•Semi-supervisado: Algunos datos est ´an etiquetados y
otros no. ´Util cuando etiquetar todos los datos es costoso.
•Auto-supervisado: El modelo genera etiquetas a partir de
los propios datos, sin necesidad de intervenci ´on manual.
Es ampliamente utilizado en procesamiento de lenguaje
natural (NLP) y en redes neuronales concurrentes. Ejem-
plo: predecir la siguiente palabra en una oraci ´on.
•Aprendizaje por refuerzo: El agente aprende mediante
recompensas y penalizaciones, ajustando sus acciones
para maximizar la recompensa futura. Ejemplo: entrenar
un agente para jugar Mario Bros, donde el modelo
aprende a avanzar, evitar obst ´aculos y recolectar recom-
pensas.
•Few-shot: Aprende a partir de pocos ejemplos (3-4
muestras). Ejemplo: modelos de lenguaje que generan
respuestas correctas con muy pocos ejemplos.
•One-shot: Aprende con un solo ejemplo. Ejemplo: re-
conocimiento facial usando una sola foto de referencia.
•Zero-shot: Generaliza a tareas nuevas sin ejemplos direc-
tos, bas ´andose en conocimientos previos.
MACHINE LEARNING VISTO DESDE LA CIENCIA
Desde la perspectiva cient ´ıfica, el fin es generar
conocimiento, entender patrones y crear teor ´ıas. Las m ´etricas
se utilizan para cuantificar qu ´e tan bien funciona un modelo,
aportando una base objetiva para comparar resultados y enfo-
ques.
En este contexto, los roles principales incluyen:
•Data Scientist: Experimenta con modelos, selecciona al-
goritmos, ajusta hiperpar ´ametros y eval ´ua resultados.
•Research Scientist: Investiga nuevos algoritmos, publica
art´ıculos cient ´ıficos y desarrolla teor ´ıas para avanzar la
inteligencia artificial.

MACHINE LEARNING VISTO DESDE LA INGENIER ´IA
Desde la perspectiva de la ingenier ´ıa, el enfoque est ´a en
implementar, mantener y optimizar modelos en producci ´on.
Entre las principales tareas se incluyen transformar modelos
para reducir su tama ˜no y aumentar la velocidad, crear pipelines
de datos que alimenten los modelos de forma autom ´atica y
monitorear m ´etricas de rendimiento y tiempos de respuesta.
Adem ´as, las pr ´acticas de MLOps se aplican para opera-
cionalizar el aprendizaje autom ´atico, de manera similar a c ´omo
DevOps se utiliza para el desarrollo y despliegue de software.
IV. P ARADIGMAS DE RESOLUCI ´ON DE PROBLEMAS
Entre los principales paradigmas de resoluci ´on de problemas
en Inteligencia Artificial se incluyen:
float
A. Problemas de b ´usqueda
Se busca encontrar el camino m ´as eficiente hacia una
soluci ´on.
Fig. 1. Ejemplo de problemas de b ´usqueda.
B. Problemas de optimizaci ´on
Cuando existe un gran n ´umero de soluciones posibles, hallar
la mejor soluci ´on absoluta puede ser dif ´ıcil.
1) Soluci ´on local: Mejor soluci ´on dentro de un ´area es-
pec´ıfica.
2) Soluci ´on global: Mejor soluci ´on en todo el espacio de
posibles soluciones.
Fig. 2. Ejemplo de soluciones locales y globales en optimizaci ´on.
C. Predicci ´on y clasificaci ´on
1) Predicci ´on:Estimar un valor futuro o desconocido, en-
contrando patrones seg ´un los datos disponibles. Por ejemplo,
cu´anta gasolina quedar ´a en un carro.
2) Clasificaci ´on: Asignar elementos a categor ´ıas pre-
definidas, bas ´andose en sus caracter ´ısticas. Por ejemplo, iden-
tificar el modelo de un carro seg ´un sus partes.
Fig. 3. Ejemplo de predicci ´on y clasificaci ´on.
D. Agrupamiento (Clustering)
Descubrir patrones o grupos naturales en los datos, uti-
lizando diferentes aspectos para formar grupos con distintas
formas.

Fig. 4. Ejemplo de agrupamiento (clustering).
V. M ODELOS
1) Determinista: Un modelo determinista produce siempre
el mismo resultado para un mismo conjunto de entradas.
Ejemplo: ¿Hay luz a mediod ´ıa? Para las mismas condiciones,
la respuesta siempre ser ´a la misma: s ´ı.
2) Estoc ´astico: Un modelo estoc ´astico puede producir
diferentes resultados para el mismo conjunto de entradas,
dependiendo de probabilidades o factores aleatorios. Ejemplo:
¿Cu´al ser ´a el clima a mediod ´ıa? Aunque las condiciones
iniciales sean similares, el clima puede variar: puede estar
soleado, nublado o lloviendo.
Fig. 5. Comparaci ´on entre modelos determinista y estoc ´astico usando el
ejemplo del clima.
VI. J ERARQU ´IA DE LA INTELIGENCIA ARTIFICIAL
1) Inteligencia Artificial (IA): Algoritmos que imitan la
inteligencia humana, capaces de tomar decisiones o resolver
problemas.
2) Machine Learning (ML): M´etodos estad ´ısticos que per-
miten a los modelos aprender de los datos, como regresi ´on´o
´arboles de decisi ´on.
3) Deep Learning (DL): Redes neuronales profundas uti-
lizadas para problemas complejos, incluyendo visi ´on por com-
putadora y procesamiento de lenguaje natural (NLP).
4) Generative AI (GenAI): Modelos capaces de generar
contenido nuevo, como texto, im ´agenes o audio, a partir de
patrones aprendidos.
Fig. 6. Jerarqu ´ıa de la IA
VII. P IPELINE DE MACHINE LEARNING
1) Data Acquisition: Recolectar datos relevantes y de cal-
idad.
2) Data Preparation: Limpiar y transformar los datos,
entreg ´andolos en un formato adecuado para su an ´alisis, elimi-
nando duplicados, valores faltantes o incorrectos, y aplicando
normalizaci ´on o escalamiento de los valores.
3) Feature Engineering: Crear y seleccionar las variables
(features) m ´as relevantes para entregar al modelo ´unicamente
las necesarias para su entrenamiento y an ´alisis.
4) Model Selection: Elegir el modelo que mejor se adapta
al problema, considerando no solo su desempe ˜no, sino tambi ´en
su explicabilidad, es decir, qu ´e tan f ´acil es interpretar y
entender c ´omo toma decisiones.
5) Model Training: Entrenar el modelo y ajustar los hiper-
par´ametros, utilizando t ´ecnicas como Grid Search. Durante
este proceso, el modelo realiza optimizaci ´on basada en los
datos para mejorar su desempe ˜no y reducir errores.
6) Model Deployment: Poner el modelo en producci ´on,
integrarlo con aplicaciones y monitorear su desempe ˜no.

Fig. 7. Pipeline t ´ıpico de Machine Learning desde la adquisici ´on de datos
hasta el despliegue del modelo.
VIII. M ANEJO Y MANIPULACI ´ON DE DATOS CON
PYTORCH Y PANDAS EN PYTHON
En esta secci ´on se abordan las t ´ecnicas b ´asicas para procesar
y manipular datos utilizando PyTorch, junto con Pandas y
NumPy. Se explicar ´a c´omo crear y transformar tensores,
realizar operaciones sobre ellos y cargar datos desde archivos
CSV para su an ´alisis y uso en modelos de aprendizaje au-
tom´atico.
A. Anaconda y manejo de ambientes
Anaconda es una distribuci ´on que incluye lenguajes como
Python y R, junto con herramientas para gestionar paquetes
y entornos de forma aislada. Su principal utilidad es permitir
manejar dependencias en diferentes ambientes, evitando que
un proyecto interfiera con otro, lo cual es muy ´util en ciencia
de datos y machine learning.
Por ejemplo, para instalar PyTorch en un ambiente es-
pec´ıfico se puede usar:
# Activar el ambiente
conda activate ML # o el nombre del ambiente
# Instalar PyTorch dentro del ambiente
activado
pip install torch
Esto asegura que PyTorch se instale ´unicamente en el
ambiente seleccionado, sin afectar otros proyectos o config-
uraciones.
B. Configuraci ´on
Para trabajar con datos y tensores, se necesita importar
algunas librer ´ıas clave:import torch # Para crear y manipular
tensores
import numpy as np # Para arreglos y
operaciones numericas
import pandas as pd # Para manejar datos
tabulares
C. Creaci ´on de tensores
Un tensor es un arreglo de n ´umeros que puede tener una o
varias dimensiones. - Si tiene una dimensi ´on, se llama vector.
- Con dos dimensiones se llama matriz. - Con m ´as de dos
dimensiones, simplemente se denomina tensor de orden k.
PyTorch permite crear tensores ya inicializados. Por ejem-
plo,arange(n) genera un vector con valores de 0 hasta
n−1, almacenado en memoria principal y listo para opera-
ciones en CPU.
Cada valor dentro del tensor se llama elemento. Por ejem-
plo, el tensor xcreado con arange(12) tiene 12 elementos.
Se puede inspeccionar el n ´umero total de elementos con
numel() y la forma del tensor (tama ˜no de cada eje) con
shape :
x = torch.arange(12, dtype=torch.float32)
x.numel() # 12 elementos
x.shape # (12,)
1) Redimensionamiento (reshape): El m ´etodo reshape
permite cambiar la forma de un tensor sin copiar sus datos.
Por ejemplo, un vector de 12 elementos puede transformarse
en una matriz de 3 filas y 4 columnas:
X = x.reshape(3, 4)
X# Matriz de 3x4
2) Tensores pre-inicializados: Se pueden crear tensores
ya con valores espec ´ıficos, como ceros, unos o n ´umeros
aleatorios, ´utiles por ejemplo para par ´ametros de modelos:
zeros = torch.zeros((2, 3, 4)) # Tensor de
ceros
ones = torch.ones((2, 3, 4)) # Tensor de
unos
randn = torch.randn(3, 4) # Valores
aleatorios
La forma 2×3×4indica 2 bloques, cada uno con 3 filas
y 4 columnas.
3) Creaci ´on de Tensores desde listas de Python: PyTorch
permite convertir listas de Python o arreglos de NumPy
directamente en tensores:
A = torch.tensor([[2, 1, 4, 3],
[1, 2, 3, 4],
[4, 3, 2, 1]], dtype=torch.
float32)
A
Esto genera un tensor de 3 filas y 4 columnas con los valores
especificados.

D. Indexaci ´on y segmentaci ´on (slicing)
PyTorch permite acceder a elementos, filas, columnas o
submatrices de manera similar a NumPy:
fila_ultima = A[-1] # Ultima fila
submatriz = A[1:3] # Filas 1 y 2
A[1, 2] = 9 # Asigna 9 al elemento en la
fila 1, columna 2
A[:2, :] = 35 # Asigna 35 a todas las
columnas de las dos primeras filas
Esto funciona tambi ´en para tensores de m ´as de dos dimen-
siones.
E. Operaciones elemento a elemento
En PyTorch, las operaciones aritm ´eticas se aplican elemento
por elemento, generando un nuevo tensor:
x = torch.tensor([1.0, 2, 4, 8])
y = torch.tensor([2, 2, 2, 2])
add, sub, mul, div, exp = x + y, x - y, x *y,
x / y, x **y
Esto permite realizar suma, resta, multiplicaci ´on, divisi ´on y
potencia de forma directa sobre cada elemento.
F . Concatenaci ´on de tensores
Se pueden unir varios tensores en uno solo usando
torch.cat , especificando el eje sobre el cual concatenar.
Por ejemplo, concatenando dos matrices a lo largo de las filas
(dim=0 ) se suman las filas, y a lo largo de las columnas
(dim=1 ) se suman las columnas:
X = torch.arange(12, dtype=torch.float32).
reshape((3,4))
Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3,
4], [4, 3, 2, 1]])
torch.cat((X, Y), dim=0) # Concatenar filas
torch.cat((X, Y), dim=1) # Concatenar
columnas
Adem ´as, se puede construir tensores binarios mediante
comparaciones. Por ejemplo, X == Y genera un tensor donde
cada elemento es 1 si coincide y 0 si no:
X == Y # Comparacion elemento a elemento
G. Indexaci ´on l´ogica
Se pueden crear m ´ascaras booleanas para seleccionar el-
ementos que cumplan cierta condici ´on. Por ejemplo, com-
parando dos tensores se obtiene un tensor de valores True
oFalse :
mask = X == Y # True donde los elementos
coinciden, False en caso contrarioH. Broadcasting
Elbroadcasting permite realizar operaciones entre tensores
de distintas formas, expandiendo autom ´aticamente sus dimen-
siones sin duplicar datos:
a = torch.arange(3).reshape((3, 1)) # Forma 3
x1
b = torch.arange(2).reshape((1, 2)) # Forma 1
x2
broadcast = a + b # Tensor resultante de
forma 3x2
I. Operaciones in-place
Las operaciones in-place modifican directamente el tensor
original, ahorrandose memoria. Esto es ´util cuando se maneja
muchos par ´ametros y se quiere evitar crear copias innecesarias.
before = id(Y) # Se guarda la
direccion de memoria original de Y
Y = Y + X # Se crea un nuevo
tensor con la suma; Y ahora apunta a nueva
memoria
id(Y) == before # False, la memoria de
Y cambio
Z = torch.zeros_like(Y) # Se crea un tensor Z
con la misma forma que Y, lleno de ceros
Z[:] = X + Y # Modifica el
contenido de Z directamente (in-place),
sin cambiar su direccion de memoria
Se recomienda usar in-place para eficiencia, pero con
cuidado si varias variables apuntan al mismo tensor, para evitar
inconsistencias.
J. Conversi ´on a NumPy
PyTorch permite convertir tensores a arreglos de NumPy y
viceversa, sin duplicar los datos en memoria:
A_np = A.numpy() # Tensor a arreglo
NumPy
type (A_np)
A_back = torch.from_numpy(A_np) # Arreglo
NumPy a tensor
type (A_back)
K. Carga de datos desde CSV
Para trabajar con datos externos, se puede usar pandas
y luego convertir a tensores de PyTorch. Se pueden aplicar
codificaci ´on *one-hot* y completar valores faltantes:
import pandas as pd
import torch
df = pd.read_csv(’../data/house_tiny.csv’)
# Leer CSV
inputs = df.iloc[:, :2]
# Seleccionar
columnas
inputs = pd.get_dummies(inputs, dummy_na=True)
# One-hot encoding

inputs = inputs.fillna(inputs.mean())
# Completar valores faltantes
X_csv = torch.tensor(inputs.to_numpy(dtype=
float )) # Convertir a tensor
X_csv
IX. C ONCEPTOS B ´ASICOS DE ´ALGEBRA LINEAL
En esta secci ´on, como continuaci ´on de la anterior, se pre-
sentan los fundamentos matem ´aticos que sustentan la manip-
ulaci ´on de datos. Se introducen escalares, vectores, matrices
y tensores, junto con sus operaciones b ´asicas.
A. Escalar
Un escalar es un valor num ´erico ´unico que representa una
sola cantidad. En PyTorch se puede representar como un tensor
con un solo elemento:
escalar = 6
B. Vector
Un vector es un arreglo unidimensional de escalares. Cada
elemento del vector es un escalar.
import numpy as np
vector = np.array([1, 2, 3])
Vector:
[1 2 3]
C. Matriz
Al igual que los escalares son tensores de orden 0 y los
vectores son tensores de orden 1, una matriz es un tensor de
orden 2. Es un arreglo bidimensional de escalares.
import numpy as np
matriz = np.array([[1, 2, 3],
[4, 5, 6],
[7, 8, 9]])
Matriz:
[[1 2 3]
[4 5 6]
[7 8 9]]
D. Tensor
Cuando se trabaja con datos de m ´as de dos dimensiones, se
utilizan tensores. Son arreglos de orden 3 o superior.
import numpy as np
tensor = np.array([[[1, 2, 3], [4, 5, 6]],
[[7, 8, 9], [10, 11, 12]]])tensor:
[[[ 1 2 3]
[ 4 5 6]]
[[ 7 8 9]
[10 11 12]]]
E. Hadamard Product
El producto Hadamard corresponde a la multiplicaci ´on
elemento a elemento de dos matrices o tensores de igual forma.
En Python se puede realizar usando el operador *.
A = np.array([[1, 2],
[3, 4]])
B = np.array([[5, 6],
[7, 8]])
C = A *B
C:
[[ 5 12]
[21 32]]
F . Propiedades b ´asicas de la aritm ´etica de tensores
Sumar o multiplicar un escalar con un tensor produce un
tensor de la misma forma que el original, donde cada elemento
se ve afectado por el escalar:
a = 2
X = torch.arange(24).reshape(2, 3, 4)
a + X # Suma escalar elemento a
elemento
(a*X).shape # Multiplicacion escalar,
mantiene la forma
tensor([[[ 2, 3, 4, 5],
[ 6, 7, 8, 9],
[10, 11, 12, 13]],
[[14, 15, 16, 17],
[18, 19, 20, 21],
[22, 23, 24, 25]]])
torch.Size([2, 3, 4])
1) Reducci ´on:Se pueden sumar los elementos de un tensor
usando sum() :
x = torch.arange(3, dtype=torch.float32)
x, x. sum() # Vector [0,1,2] y su suma
tensor([0., 1., 2.]), tensor(3.)
Para un tensor multidimensional, sum() por defecto reduce
todos los ejes:
A = torch.arange(6, dtype=torch.float32).
reshape(2, 3)
A.shape, A. sum() # Suma de todos los
elementos

torch.Size([2, 3]), tensor(15.)
Se puede especificar un eje para sumar a lo largo de filas o
columnas:
A, A. sum(axis=0), A. sum(axis=1)
tensor([[0., 1., 2.],
[3., 4., 5.]]),
tensor([3., 5., 7.]),
tensor([3., 12.])
Reducir a m ´ultiples ejes simult ´aneamente es equivalente a
sumar todos los elementos:
A.sum(axis=[0,1]) == A. sum()
True
La media se calcula con mean() , equivalente a la suma
dividida por el n ´umero de elementos:
A.mean(), A. sum()/A.numel()
A.mean(axis=0), A. sum(axis=0)/A.shape[0]
tensor(2.5000), tensor(2.5000)
tensor([1.5, 2.5, 3.5]), tensor([1.5, 2.5,
3.5])
2) Suma sin reducci ´on:Si se quiere conservar la forma del
tensor tras sumar:
sum_A = A. sum(axis=1, keepdims=True)
A_normalized = A / sum_A # Broadcasting para
normalizar filas
A, sum_A, A_normalized
tensor([[0., 1., 2.],
[3., 4., 5.]]),
tensor([[ 3.],
[12.]]),
tensor([[0.0000, 0.3333, 0.6667],
[0.2500, 0.3333, 0.4167]])
3) Suma acumulada: Se puede calcular la suma acumulada
concumsum() :
A.cumsum(axis=0) # Suma acumulada a lo largo
de las filas
tensor([[0., 1., 2.],
[3., 5., 7.]])
REFERENCES
[1] Steven Pachecho Portuguez, Clase sobre Algebra Lineal y manipulaci ´on
de datos con PyTorch, Pandas y NumPy , Tecnol ´ogico de Costa Rica,
2025.