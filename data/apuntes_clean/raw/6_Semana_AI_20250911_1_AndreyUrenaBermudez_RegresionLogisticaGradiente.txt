Apuntes Semana 6
Apuntes del 11 de setiembre de 2025
Andrey Ureña Bermúdez – 2022017442
Inteligencia Artificial
andurena@estudiantec.cr
Resumen—En este documento, se resume la clase del 11 de
setiembre de 2025, en la cuál se realizó primeramente un repaso
de lo visto en la clase anterior. De manera general, este documento
recopila información sobre verosimilitud en la regresión logística,
la función de costo e información sobre un notebook de regresión
logística compartido por el profesor.
Index Terms—Verosimilitud, regresión logística, gradiente des-
cendiente, función sigmoide, derivada.
I. NOTA SOBRETAREAI
Se hace recordatorio sobre darle importancia y no descuidar
el trabajo escrito de la tarea, así como su documentación, pues
de este se dará elfeedbackpara los escritos que haya que
realizar en tareas próximas y etapas del proyecto.
II. REPASO SOBRE CLASE DEL MARTES
II-A. Verosimilitud
Es la probabilidad de observar cada uno de los datos
cambiando ciertos parámetros. Lo que se busca es maximizar
para llegar al punto de máxima probabilidad.
La diferencia entre MSE ymaximum likelihoodradica en su
aplicación: para la predicción de valores continuos, se utiliza
MSE, mientras que para modelar probabilidades, se utiliza
maximum likelihood.
Así, nuestra función de costo es:
L=NY
i=1fw,b(xi)yi·(1−f w,b(xi))(1−y i)(1)
Se vió el desarrollo de cada uno de los casos dey ien
fw,b(xi)yi(1−f w,b(xi))(1−y i):
Casoy i= 1:
fw,b(xi)yi(1−f w,b(xi))(1−y i)=fw,b(xi)1(1−f w,b(xi))0
(1)
=fw,b(xi)1(2)
Acá el modelo nos da el valor directo. Ejemplo:Calabaza
no es naranja:
wx+b= 1,458
fw,b(xi) =σ(wx i+b) =σ(1,458) = 0,81
=fw,b(xi)1(1−f w,b(xi))0
=fw,b(xi)1=σ(1,458)
= 0,81
La probabilidad de quex ino sea naranja es0,81.Casoy i= 0:
fw,b(xi)yi(1−f w,b(xi))(1−y i)=fw,b(xi)0(1−f w,b(xi))1
(1)
= (1−f w,b(xi))1(2)
Con la misma fórmula puedo estudiar de que ocurra o no
un evento. Ejemplo:Calabaza es naranja:
wx+b=−1,32
fw,b(xi) =σ(wx i+b) =σ(−1,32) = 0,21
=fw,b(xi)0(1−f w,b(xi))1
= (1−f w,b(xi))1= (1−σ(−1,32))
= (1−0,21) = 0,79
La probabilidad de quex isea naranja es0,79.
Al final lo que obtenemos es la probabilidad de que la
muestrax itenga la etiquetay i.
II-B. Derivada de la función de costo
Primero, se debe calcular la probabilidad de quex itome la
etiqueta dey i, así con cada muestra.
Dado que esto implica la multiplicación de probabilidades,
el cálculo de la derivada se vuelve complejo. Para sim-
plificarlo, se busca una expresión equivalente que evite la
multiplicación, lo cual se logra aplicando logaritmos.
II-C. Logaritmos
ln(an) =n·ln(a)
ln(a·b) = ln(a) + ln(b)
ln(an·bn) =n·ln(a) +n·ln(b)
II-D. Aplicación de logaritmo a la verosimilitud
L=Qfw,b(xi)yi·(1−f w,b(xi))(1−y i)
ln(L) =Pln(fw,b(xi)yi) + ln((1−f w,b(xi))(1−y i))
ln(L) =Pyi·ln(f w,b(xi)) + (1−y i)·ln(1−f w,b(xi))
Esto lo vamos a llamarlog-likelihood. Es mucho más fácil
de computar y derivar, además de que quita errores al momento
de computar las multiplicaciones de probabilidades. Ahora esta
es la función de costo que se va a usar.
Para minimizar maximizando lo que se puede hacer es darle
vuelta a la función, para eso se multiplica por−1:
L=1
NX
yi·ln(f w,b(xi)) + (1−y i)·ln(1−f w,b(xi))

L=−1
NhX
yi·ln(f w,b(xi)) + (1−y i)·ln(1−f w,b(xi))i
Ahora puedo minimizar la función, lo que permite aplicar
el descenso del gradiente que se ha estado trabajando.
Figura 1. Gráfica minimizando L
Aquí lo ideal es intentar que ellossllegue a cero; si se
obtiene unlossnegativo, significa que algo se está haciendo
mal.
II-E. Actualización de parámetros
Es necesario actualizar los parámetroswyb, ya que son
los que permiten modificar los resultados de las probabilidades
obtenidas.
Para actualizar el parámetrowse necesita:∂L
∂w
Para actualizar el parámetrobse necesita:∂L
∂b
II-F. Composición de funciones
Se va a utilizar el concepto de composición de funciones
para que el cálculo de derivadas sea más sencillo.
Derivada función de costo para un sample:
L=y i·ln(f w,b(xi)) + (1−y i)·ln(1−f w,b(xi))
Modelo:f w,b(x) =a(z(x))
a(x) =σ(x) =1
1+e−x
z(x) =wx+b
El resultado de combinar ambas es:
L=y i·ln(a(z(x))) + (1−y i)·ln(1−a(z(x)))
Cuando se habla de la técnica de composición de funciones
se aplica la regla de la cadena. Se deben calcular las derivadas
parciales:
L=y i·ln(a(z(x))) + (1−y i)·ln(1−a(z(x)))
∂L
∂w=∂L
∂a·∂a
∂z·∂z
∂w
∂L
∂b=∂L
∂a·∂a
∂z·∂z
∂b1) Cálculo de derivadas parciales
Importante recordar que elLque se está usando es:
L=−[y i·ln(a(z(x))) + (1−y i)·ln(1−a(z(x)))]
Primero se inicia calculando la derivada parcial deLcon
respecto a la función sigmoide:
∂L
∂a=−
yi·1
a(x)·a(x)′
+
(1−y i)·1
1−a(x)·(1−a(x))′
=−yi
a(x)·1
+(1−y i)
1−a(x)· −1
=−yi
a(x)
−(1−y i)
1−a(x)
=−yi
a(x)+(1−y i)
1−a(x)
Ahora, se calcula la derivada parcial de la función sigmoide
respecto az. Importante recordar que la derivada de sigmoide
esσ(x)·(1−σ(x)), por lo que la derivada parcial sería:
∂a
∂z=σ(z(x))·(1−σ(z(x)))
Por último, se debe calcular de manera individual la deri-
vada parcial de cada uno de los parámetros con respecto a la
regresión lineal:
z(x) =wx+b
∂z
∂w=x
∂z
∂b= 1
Ya que se hizo el cálculo de cada derivada de manera
individual, se prosigue a realizar las multiplicaciones:
Figura 2. Derivada parcial de L respecto a z
Figura 3. Derivada parcial de L respecto a w y b

Se procede a actualizar parámetros:
z(x) =wx+b
w=w−α∂L
∂w
b=b−α∂L
∂b
Dondeαes un hiperparámetro (learning rate).
III. CÓDIGO
Se muestra un notebook con el fin de comprender mejor
cómo hacer una regresión logística. Enlace a notebook.
Figura 4. Código Clasificación
Como se muestra en lafigura 4, se hace la importación de
librerías necesarias, muchas de las cuales pertenecen a sklearn.
Luego, se hace la clasificación con make_classification, el
cual es un método para crear un dataset de clasificación. En
este caso, se indica que sea de 1000 samples, con 2 features
informativas, sin features redundantes y con un solo clúster
por clase.
Posteriormente, se visualiza el conjunto de datos utilizando
plt.scatter, donde los puntos se colorean según su clase (y).
Luego, se crea un DataFrame con pd.DataFrame que contiene
las dos características (Feature_1 y Feature_2) y la variable
objetivo (Target).
Finalmente, se divide el dataset en entrenamiento y prueba
con train_test_split, reservando el 80 % de los datos para entre-
namiento y el 20 % para prueba, asegurando reproducibilidad
con random_state=225.
Lafigura 5muestra la implementación manual de la re-
gresión logística. La clase recibe como parámetros la cantidad
de epochs a ejecutar, el learning rate que se aplicará y los
parámetros de la regresión w y b, que serán ajustados durante
el entrenamiento.
Primero, se define la funciónsigmoide, utilizada para
convertir la predicción lineal en una probabilidad. Luego, se
implementa la función de costo binary_cross_entropy_loss,
que calcula la pérdida negativa con el objetivo de minimizarla
durante el entrenamiento.
En la funciónfit, se reciben todos los features y las etiquetas
correspondientes. Antes de iniciar el ajuste de los parámetros,
se inicializan aleatoriamente los valores de w, cuyo tamaño
corresponde al número de features, ya que cada uno necesita
un peso asociado. Luego, se ejecuta el ciclo de entrenamiento
por la cantidad de epochs definida, donde primero se calcula
la predicción lineal, que luego pasa por la función sigmoide
para obtener una probabilidad. A partir de esta probabilidad,se actualizan los valores de w y b, y se calcula el error en
cada iteración.
La funciónpredictse encarga de predecir la clase de una
nueva muestra en base al umbral que se define. Una vez
obtenida la probabilidad, se asigna la clase correspondiente.
Figura 5. Código Regresión Logística
Finalmente, el modelo se implementa instanciando la clase
y entrenándola con X train y y train. Luego, se evalúa con
X test y y test, calculando la accuracy y generando un clas
sification report para medir su desempeño, el cuál muestra
métricas como el nivel de accuracy, precision, recall, f1-score
y support.
Figura 6. Enter Caption
De igual forma, en lugar de implementar la regresión
logística manualmente, se puede utilizar el método que facilita
sklearn, tal y como se muestra en la figura 6. En este caso,
se instancia el modelo de regresión logística con

IV. CONCLUSIÓN
A lo largo de este documento se profundizó en los funda-
mentos de la regresión logística, en particular en el uso de
la verosimilitud como función de costo y en la aplicación del
logaritmo para simplificar su derivación. Se revisaron ejemplos
prácticos que ilustran cómo interpretar probabilidades según
los valores de entrada, y se abordó el proceso de actualización
de parámetros mediante gradiente descendente. Además, el
repaso permitió conectar la teoría con la implementación
práctica en Python, reforzando la comprensión del modelo y
su utilidad en la clasificación de datos. Con esto, se sientan
las bases para continuar con técnicas más avanzadas de apren-
dizaje supervisado.