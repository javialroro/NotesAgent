APUNTES DE CLASE
Inteligencia Artificial â€” Semana 9 â€” 02 de Octubre
Priscilla Jim Â´enez Salgado
Escuela de Ingenier Â´Ä±a en Computaci Â´on, Tecnol Â´ogico de Costa Rica
Cartago, Costa Rica â€”2021022576@estudiantec.cr
Abstractâ€”Este documento hace un repaso general
y claro sobre las funciones de activaci Â´on m Â´as
utilizadas en las redes neuronales, adem Â´as de
explicar conceptos importantes sobre c Â´omo est Â´an
diseËœnadas y c Â´omo han evolucionado las redes
neuronales artificiales. Se presentan funciones
como ReLU, Sigmoide y Softmax, entre otras, con
su base matem Â´atica. Tambi Â´en se repasa el con-
cepto de perceptr Â´on y las redes multicapa, y se
comentan algunos retos cl Â´asicos en el Â´area, como
el problema del XOR y la llamada â€œmaldici Â´on de
la dimensionalidadâ€.
I.Review de la lectura
En clase el profesor coment Â´o de forma muy b Â´asica
la lectura From Language to Action: A Review
of Large Language Models as Autonomous Agents
and Tool Users. Se ËœnalÂ´o que lo importante para
el pr Â´oximo quiz del martes es entender lo esen-
cial: los modelos de lenguaje (LLMs) ya no solo
generan texto, sino que tambi Â´en funcionan como
agentes aut Â´onomos capaces de razonar, planificar,
usar memoria e interactuar con herramientas ex-
ternas. La lectura diferencia entre sistemas de un
solo agente y sistemas multi-agente, donde varios
modelos colaboran para resolver problemas m Â´as
complejos. Adem Â´as, se destacan sus aplicaciones
en investigaci Â´on, programaci Â´on, salud, rob Â´otica y
simulaciones, as Â´Ä± como los principales retos, entre
ellos la memoria limitada, la seguridad, la Â´etica y
la necesidad de mejores evaluaciones.
A.Noticias de la semana
En clase se habl Â´o del lanzamiento de Sora 2, el
nuevo modelo de generaci Â´on de video creado por
OpenAI como respuesta al Nano Banana de Google.A diferencia del primer Sora, que ten Â´Ä±a resultados
poco realistas, esta nueva versi Â´on produce videos
mÂ´as naturales y coherentes, adem Â´as de incluir audio
gracias a su capacidad multimodal. El profesor
mostr Â´o un ejemplo hecho con la herramienta y
explic Â´o que incluso podr Â´Ä±a usarse para presenta-
ciones acad Â´emicas. Tambi Â´en se mencion Â´o la nueva
aplicaci Â´on â€œSora by OpenAIâ€, una plataforma donde
las personas pueden crear y compartir videos con in-
teligencia artificial a partir de simples descripciones
o prompts.
Fig. 1: Sora by OpenAI
II.Aspectos Administrativos
El profesor comparti Â´o las notas de los trabajos
pendientes y brind Â´o retroalimentaci Â´on individual a
cada grupo de trabajo. Sin embargo, aÂ´ un queda por
entregar la calificaci Â´on del quiz 4 realizado y de
la tarea presentada el pasado mi Â´ercoles, que est Â´an
pendientes de revisi Â´on. Adem Â´as, se indic Â´o que la
prÂ´oxima semana se asignar Â´a el proyecto del curso.
A.Repaso
â€“ El perceptr Â´on:Puede entenderse de forma simi-
lar a una regresi Â´on log Â´Ä±stica, aunque se diferencia en
la funci Â´on de p Â´erdida que utiliza. Durante la historia
de la inteligencia artificial se produjo el llamado
â€œinvierno de la IAâ€, en parte debido al problema
delXOR, ya que este no pod Â´Ä±a ser representado

adecuadamente por un modelo de regresi Â´on log Â´Ä±stica
ni por un perceptr Â´on simple.
â€“ Predicci Â´on de compuertas l Â´ogicas:
Fig. 2.Compuertas l Â´ogicas
En la figura se ilustran las compuertas l Â´ogicasOR
yANDmediante gr Â´aficos bidimensionales.
â€¢OR(ğ‘‹ 1,ğ‘‹2):Los tri Â´angulos indican la salida 1 y
los cÂ´Ä±rculos la salida 0. Esta compuerta devuelve
1 siempre que al menos una de las entradas sea
igual a 1.
â€¢AND(ğ‘‹1,ğ‘‹2):Corresponde a una compuerta
AND donde la primera entrada est Â´a negada. La
salida es 1 (tri Â´angulo) Â´ unicamente cuando la
primera entrada es 0 y la segunda es 1.
â€¢AND(ğ‘‹ 1,ğ‘‹2):Representa la compuerta AND
con la segunda entrada negada. El resultado es
1 (tri Â´angulo) solo cuando la primera entrada es 1
y la segunda es 0.
En cada gr Â´afico, la l Â´Ä±nea punteada marca ell Â´Ä±mite
de decisi Â´onque distingue entre las dos clases de
salida (0 y 1). Esta representaci Â´on facilita la com-
prensi Â´on de c Â´omo las compuertas l Â´ogicas realizan la
clasificaci Â´on de sus entradas en un espacio bidimen-
sional.
â€“ Problema del XOR:
Fig. 3.Problema del XOR
El principal inconveniente es que el problema no
es linealmente separable, por lo que el algoritmo
del perceptr Â´on simple no pod Â´Ä±a ofrecer una soluci Â´on
adecuada. Es en este punto donde surgen las redes
neuronales o perceptrones multicapa, ya que estos s Â´Ä±tienen la capacidad de abordar problemas no lineales.
Gracias a ello, se ampl Â´Ä±a significativamente el rango
de problemas que pueden resolverse con este m Â´etodo.
â€“ Inspiraci Â´on Biol Â´ogica:
Fig. 4.Inspiraci Â´on biol Â´ogica
Las redes neuronales se inspiran en c Â´omo funcionan
las neuronas en nuestro cerebro. Cada neurona est Â´a
conectada con otras a trav Â´es de sus dendritas, y en el
nÂ´ ucleo es donde se procesa la informaci Â´on.
Si lo comparamos con una regresi Â´on log Â´Ä±stica, las
dendritas ser Â´Ä±an como las entradas de datos (inputs),
y el nÂ´ ucleo representar Â´Ä±a la funci Â´on lineal que procesa
esa informaci Â´on. Al final, la neurona decide si deja
pasar o no esa se Ëœnal.
â€“ Funciones de activaci Â´on:En la regresi Â´on
logÂ´Ä±stica esa transformaci Â´on se conoce como funci Â´on
no lineal, espec Â´Ä±ficamente la sigmoide. SegÂ´ un la se Ëœnal
recibida, la neurona se activa o no, permitiendo que
la informaci Â´on continÂ´ ue, la transforme o la bloquee.
Fig. 5.Funciones de activaci Â´on
â€¢Funci Â´on ReLU:La funci Â´onğ‘”(ğ‘¥)=max(0,ğ‘¥)
estÂ´a limitada por debajo de cero y es estricta-
mente creciente. Es muy eficiente en modelos
deDeep Learning, pero presenta el problema
de las llamadasneuronas muertas, ya que no es
derivable en todos los puntos y, en algunos casos,
el gradiente puede llegar a ser cero, impidiendo
la actualizaci Â´on de los pesos.

Fig. 6.Ejemplo ReLu
â€¢Leaky ReLU:Esta funci Â´on asigna una peque Ëœna
constante al valor m Â´Ä±nimo permitido, lo que
ayuda a evitar el problema de las neuronas
muertas. Aunque representa una mejora respecto
a la ReLU original, no se considera la soluci Â´on
definitiva.
ğ‘”(ğ‘¥)=(
0.01ğ‘¥, ğ‘¥<0
ğ‘¥, ğ‘¥â‰¥0
ğœ•ğ‘”(ğ‘¥)
ğœ•ğ‘¥=(
0.01, ğ‘¥<0
1, ğ‘¥â‰¥0
Fig. 7.Ejemplo Leaky ReLu
â€¢Parametric ReLU (PReLU):Esta funci Â´on per-
mite aprender un par Â´ametro que controla si
la se Ëœnal continÂ´ ua en la parte negativa. Dicho
parÂ´ametro se entrena junto con el resto de la red,
lo que brinda mayor flexibilidad al modelo.
ğ‘”(ğ‘¥)=(
ğ‘¤ğ‘¥, ğ‘¥<0
ğ‘¥, ğ‘¥â‰¥0
ğœ•ğ‘”(ğ‘¥)
ğœ•ğ‘¥=(
ğ‘¤, ğ‘¥<0
1, ğ‘¥â‰¥0
â€¢Funci Â´on Tanh:Tiene una forma parecida a la
sigmoide, pero su salida est Â´a acotada en el rango
(âˆ’1,1), lo que permite manejar valores positivos
y negativos.
â€¢Binary Step Function:Devuelve 1 si la entrada
es mayor que cero y 0 si es menor o igual a cero.
â€¢Funci Â´on lineal:B Â´asicamente deja pasar la salida
sin aplicar ninguna transformaci Â´on adicional.â€¢Funciones SELU y ELU:Son de la misma
familia. Aunque requieren mayor costo computa-
cional, ofrecen un rendimiento muy eficiente.
â€¢Funci Â´on Sigmoide:Convierte la entrada en un
valor entre 0 y 1. Es muy usada cuando se necesita
interpretar las salidas como probabilidades.
â€“ Perceptr Â´on multicapa (MLP):
Fig. 8.Perceptr Â´on
ElPerceptr Â´on Multicapa (MLP)es una evoluci Â´on
del perceptr Â´on simple que permite resolver problemas
mÂ´as complejos, especialmente aquellos que no son
linealmente separables.
El profesor lo explic Â´o de manera sencilla con la
imagen: en lacapa de entrada(Input Layer) se
encuentran los datos originales, representados como
ğ‘‹ğ‘–, que no cambian porque son las entradas del
sistema. Luego aparecen lascapas ocultas(Hidden
Layers), que son las responsables de realizar los
cÂ´alculos, transformaciones y operaciones internas,
dÂ´andole a la red la capacidad de aprender relaciones
mÂ´as complejas. Finalmente, est Â´a lacapa de salida
(Output Layer), que entrega el resultado final y
cuyo tama Ëœno depende del problema que se est Â´e
resolviendo.
La gran ventaja del MLP es que, gracias a sus
mÂ´ ultiples capas y funciones de activaci Â´on, introduce
no linealidad, lo que le permite resolver problemas
que el perceptr Â´on simple no pod Â´Ä±a. Adem Â´as, se en-
trena utilizando lapropagaci Â´on del error(backprop-
agation), que consiste en calcular cu Â´anto se equivoc Â´o
la red y ajustar los pesos mediante descenso de
gradiente, mejorando as Â´Ä± el rendimiento del modelo.
Ahora nos preguntamos, Â¿c Â´omo se calcula una
pasada en la red?

El proceso comienza con la expresi Â´onâ„(0)=
ğ‘ ğ‘–ğ‘”ğ‘šğ‘œğ‘–ğ‘‘(ğ‘‹ğ‘Š0+ğ‘0), dondeâ„(0)corresponde a la
primera capa oculta. Lo que se hace es calcular
primero la regresi Â´on linealğ‘‹ğ‘Š0+ğ‘0, luego aplicar
la funci Â´on sigmoide al resultado, y con eso se obtiene
el valor del primerHidden Layer.
Despu Â´es, para la siguiente capa oculta, el pro-
cedimiento es pr Â´acticamente el mismo:â„(1)=
ğ‘ ğ‘–ğ‘”ğ‘šğ‘œğ‘–ğ‘‘(â„(0)ğ‘Š1+ğ‘1). En este caso, el valor de
â„(0)pasa a ser la entrada de la siguiente capa.
Este mismo proceso se repite hasta llegar a la Â´ ultima
capa, que se expresa comoâ„(ğ‘›)=ğ‘ ğ‘–ğ‘”ğ‘šğ‘œğ‘–ğ‘‘(â„(ğ‘›âˆ’
1)ğ‘Šğ‘›+ğ‘ğ‘›).
En otras palabras, cada capa oculta toma como
entrada el resultado de la capa anterior, y mediante
una combinaci Â´on lineal m Â´as la activaci Â´on, se van
construyendo paso a paso los valores hasta la salida
final de la red.
â€“ Salida independiente y distribuci Â´on:Cada salida
puede asociarse a una variable distinta. SegÂ´ un el
caso, la distribuci Â´on puede ser de tipo categ Â´orica
(como en el uso desoftmax) o continua (como en
una regresi Â´on).
Fig. 9.Salida independiente
Fig. 10.Distribuci Â´on
â€“ Capa de salida:Es la parte final de la red y se
calcula con la f Â´ormulaâ„(ğ‘›)=ğ‘”(â„(ğ‘›âˆ’1)ğ‘Šğ‘›+ğ‘ğ‘›).
BÂ´asicamente, lo que hace es tomar la salida de
la Â´ ultima capa oculta, multiplicarla por los pesos,
sumarle un sesgo y luego pasarla por una funci Â´on
de activaci Â´on. Esa funci Â´onğ‘”(ğ‘¥)no siempre es la
sigmoide, puede ser otra dependiendo del tipo detarea:softmaxsi se trata de una clasificaci Â´on mÂ´ ultiple,
o lineal si es un problema de regresi Â´on. Lo importante
es que sea una funci Â´on no lineal, ya que eso es lo que
le da a la red la capacidad de resolver
Fig. 11.Capa de salida
â€“ Funci Â´on costo:Es unafunci Â´on matem Â´aticaque
calcula el nivel de error del modelo, y cuyo objetivo
principal es minimizar dicho error durante el proceso
de entrenamiento.
Fig. 12.Funci Â´on de costo
â€“ Maldici Â´on de dimensionalidad:Pasa cuando tra-
bajamos con datos que tienen much Â´Ä±simas variables
o dimensiones. Al ir aumentando esas dimensiones,
los datos empiezan a dispersarse y quedan muy
separados entre s Â´Ä±, lo que hace m Â´as dif Â´Ä±cil encontrar
patrones claros. En otras palabras, el modelo tiene
que calcular en un espacio cada vez m Â´as grande y
con menos densidad de informaci Â´on, lo que complica
el aprendizaje.
Fig. 13.Maldici Â´on de la dimensionalidad
â€“ Comportamiento jer Â´arquico:Se utiliza este
enfoque porque imita la forma en que los humanos
aprenden: comienzan con conceptos simples y luego
los combinan para formar ideas m Â´as complejas.
Esto permite generar mejoras exponenciales en las
funciones y aprovechar mejor el aprendizaje.
â€¢Permite construir funciones polin Â´omicas.

â€¢Utiliza la composici Â´on de funciones, reutilizando
funciones simples para crear otras de mayor nivel.
â€¢Ofrece una representaci Â´on compacta, donde con
pocos pesos se pueden modelar funciones com-
plejas.
â€¢Ejemplo:una red neuronal puede aproximar otra
funci Â´on.
Fig. 14.Comportamiento jer Â´arquico
â€“ Mapas de caracter Â´Ä±sticas en CNN:En una red
neuronal convolucional (CNN), las capas no trabajan
solo con los p Â´Ä±xeles, sino que van aprendiendo rep-
resentaciones cada vez m Â´as complejas de la imagen.
Al inicio, en las primeras capas, se detectan cosas
muy b Â´asicas como bordes o l Â´Ä±neas. Luego, en las
capas intermedias, ya aparecen formas un poco m Â´as
claras como partes de ojos o bocas. Finalmente, en las
Â´ ultimas capas, la red es capaz de reconocer objetos
completos, por ejemplo un rostro.
Fig. 15.Extracci Â´on progresiva de caracter Â´Ä±sticas en una CNN
â€“ Representaciones Vectoriales:En proce-
samiento de lenguaje natural, las palabras se repre-
sentan como vectores de alta dimensi Â´on, esto permite
que palabras con funciones similares se agrupen en
el espacio vectorial.
Fig. 16.Visualizaci Â´onIII.Continuaci Â´on de Funciones de activaci Â´on
Las funciones de activaci Â´on son un elemento
fundamental en las redes neuronales, ya que permiten
introducir la no linealidad necesaria para representar
relaciones complejas en los datos. A continuaci Â´on,
se presentan las funciones m Â´as importantes junto con
sus principales caracter Â´Ä±sticas matem Â´aticas.
Fig. 17.Ejemplos de funciones de activaci Â´on: a la izquierda la
funci Â´on lineal y a la derecha la funci Â´on tangente hiperb Â´olica (tanh),
usada en redes neuronales para introducir no linealidad.
A. Funci Â´on Lineal
La funci Â´on lineal se define comoğ‘“(ğ‘¥)=ğ‘¥. La
derivada es constante, por lo que el modelo no puede
usar el descenso del gradiente ni aprender de los
datos.
Fig. 18.Funci Â´on lineal
Fig. 19.Ejemplo
B. Sigmoide
Tiene una activaci Â´on que var Â´Ä±a entre 0 y 1, siem-
pre positiva, acotada y estrictamente creciente. Sin
embargo, presenta el problema de que su derivada se
aproxima a cero en los extremos de la funci Â´on, lo que

provoca gradientes muy peque Ëœnos. Esto hace que el
entrenamiento se vuelva lento o se detenga, fen Â´omeno
conocido comovanishing gradient.
Fig. 20.Ejemplo
C. Tangente Hiperb Â´olica
La funci Â´onTanHtiene un rango de valores entre
âˆ’1 y 1. Su comportamiento es similar al de la funci Â´on
sigmoide, con la diferencia de que est Â´a centrada en
el origen, lo que permite que los valores negativos
tambi Â´en sean considerados. Sin embargo, al igual
que la sigmoide, presenta el problema delgradiente
desvanecidoen los extremos, lo que puede dificultar
el entrenamiento de redes profundas.
Fig. 21.Ejemplo
D. Funci Â´on Softmax
La funci Â´onSoftmaxconvierte la capa de salida
(output layer) en una distribuci Â´on de probabilidad,
ya que normaliza los valores mediante una sumatoria.
Su definici Â´on es la siguiente:
ğœ(ğ‘¥)ğ‘—=ğ‘’ğ‘¥ğ‘—
Ãğ¾
ğ‘˜=1ğ‘’ğ‘¥ğ‘˜
Es comÂ´ unmente utilizada en problemas de clasifi-
caciÂ´on, donde el vector de entrada se conoce como
logits. Adem Â´as, se emplea junto con la funci Â´on de
pÂ´erdidaCross-Entropy Loss.
Fig. 22.Ejemplo softmax
â€¢Â¿Por qu Â´e usarğ‘’ğ‘¥?Porque es una funci Â´on
estrictamente creciente y evita valores negativos
en la salida.
â€¢Cross-Entropy Loss:Tambi Â´en llamadaLog-
LossoLogistic Loss, se utiliza como funci Â´on de
pÂ´erdida en Softmax. Representa probabilidades
en un espacio logar Â´Ä±tmico dentro del rango[0,1]
y es num Â´ericamente estable.
La pÂ´erdida se define como:
ğ¿=log(ğ‘ƒ(ğ‘Œ=ğ‘¦ ğ‘–|ğ‘‹=ğ‘¥ğ‘–))
y en el caso de clasificaci Â´on multiclase:
ğ¿=âˆ’log 
ğ‘’ğ‘ ğ‘˜
Ãğ¶
ğ‘—=1ğ‘’ğ‘ ğ‘—!
Fig. 23.Ejemplo
E. Â¿Cu Â´al funci Â´on de activaci Â´on utilizar?
La elecci Â´on de la funci Â´on de activaci Â´on depende
del tipo de problema que se est Â´e resolviendo. Las
funcionesSigmoidyTanhsuelen presentar el in-
conveniente delvanishing gradient, lo que dificulta
el entrenamiento en redes profundas. Por ello, se
recomienda iniciar con la funci Â´onReLU, ya que es
rÂ´apida de calcular y ampliamente utilizada enDeep
Learning. En caso de que no funcione adecuada-
mente, se pueden emplear variantes comoLeaky
ReLUoParametric ReLU, que buscan superar estas
limitaciones.

IV. Backpropagation
Permite calcular cu Â´anto contribuye cada peso al
error final de la red, actualizando los par Â´ametros en
direcci Â´on opuesta a la propagaci Â´on hacia adelante.
Este proceso es esencial para que la red aprenda y
mejore su desempe Ëœno durante el entrenamiento.
A. Procesos del Entrenamiento
â€¢Forward Propagation:Consiste en calcular la
salida de la red enviando los datos desde la
capa de entrada hacia las capas siguientes, hasta
obtener el resultado final.
â€¢Backpropagation:Implica propagar el error
desde la capa de salida hacia las capas anteriores,
calculando las derivadas parciales con respecto
a los pesos y sesgos para ajustar los par Â´ametros
del modelo.
Fig. 24.Forward y Back Propagation
B. Optimizaci Â´on del grafo
En este ejemplo se considera una red neuronal en
la que cada capa contiene Â´ unicamente una neurona,
suponiendo que la funci Â´on de activaci Â´on utilizada es
laSigmoide, como se muestra en la Figura??.
Fig. 25.Grafo de la red neuronal
â€¢Denominamos a las capas antes değ¿ ğ‘–,ğ‘ğ‘™hasta
ğ‘ğ‘™âˆ’ğ‘›.
â€¢Definimos el MSE como:
ğ¿ğ‘–=(ğ‘ğ‘™âˆ’ğ‘¦ğ‘–)2
â€¢Dividimos la neurona en 2 capas:
1)Entrada:ğ‘§ğ‘™=ğ‘¤ğ‘™ğ‘ğ‘™âˆ’1+ğ‘ğ‘™dondeğ‘ğ‘™âˆ’1
corresponde a los inputsğ‘¥.2)Salida:ğ‘ğ‘™=ğ‘”(ğ‘§ğ‘™)dondeğ‘”es nuestra
funci Â´on de activaci Â´on.
Vamos a actualizar los par Â´ametros değ‘§ğ‘™, que sonğ‘¤ğ‘™
yğ‘ğ‘™. Para esto emplearemos laregla de la cadena,
usando la salida de la activaci Â´on de la capa anterior.
Profundizando a nivel de neurona, se muestra la
siguiente figura.
Fig. 26.Grafo de la capa al y Li a detalle
C. Vector gradiente
El vector gradiente se define como el conjunto
de derivadas parciales de los par Â´ametros (pesos y
sesgos) de la red neuronal. Al calcularlo, es comÂ´ un
encontrar operaciones repetidas, lo que se aprovecha
en el algoritmo de backpropagation para optimizar
los c Â´alculos.
Fig. 27.Vector Gradiente

D. M Â´ultiples neuronas
Fig. 28.Grafo con mayor dimensionaldad
â€¢Super Â´Ä±ndice:Se Ëœnala la capa a la que pertenece
una variable. Ejemplo:ğ‘(ğ‘™)corresponde a la capa
ğ‘™.
â€¢Sub Â´Ä±ndice:Identifica el nÂ´ umero de neurona
dentro de una capa espec Â´Ä±fica. Ejemplo:ğ‘(ğ‘™)
ğ‘—se
refiere a lağ‘—- Â´esima neurona en la capağ‘™.
â€¢Pesos:Se representan con dos sub Â´Ä±ndices: el
primero indica la neurona destino y el segundo
la neurona de origen. Ejemplo:ğ‘¤(ğ‘™)
ğ‘—,ğ‘˜representa el
peso que conecta la neuronağ‘(ğ‘™âˆ’1)
ğ‘˜con la neurona
ğ‘(ğ‘™)
ğ‘—.
A continuaci Â´on, en la siguiente figura se ilustra
cÂ´omo una neurona de la capağ‘™recibe entradas desde
varias neuronas de la capa anterior(ğ‘™âˆ’1). Este
proceso se puede dividir en dos pasos:
â€¢Preactivaci Â´on:
ğ‘§(ğ‘™)
ğ‘—=ğ‘(ğ‘™)
ğ‘—+ğ‘›ğ‘™âˆ’1âˆ‘ï¸
ğ‘˜=1ğ‘¤(ğ‘™)
ğ‘—,ğ‘˜ğ‘(ğ‘™âˆ’1)
ğ‘˜
dondeğ‘(ğ‘™)
ğ‘—es el sesgo de la neurona yğ‘¤(ğ‘™)
ğ‘—,ğ‘˜los
pesos de conexi Â´on.
â€¢Activaci Â´on:
ğ‘(ğ‘™)
ğ‘—=ğ‘”(ğ‘§(ğ‘™)
ğ‘—)
dondeğ‘”representa la funci Â´on de activaci Â´on
aplicada.
Fig. 29.EjemploE. C Â´alculo de funci Â´on de perdida
Para esta secci Â´on, ellossglobal se obtiene sumando
las diferencias entre la salida de cada neurona en
la capa de activaci Â´onğ‘—y su valor esperadoğ‘¦ ğ‘—,
recorriendo todas las neuronas de la capağ‘™.
ğ¿ğ‘–=ğ‘›ğ‘™âˆ‘ï¸
ğ‘—=1(ğ‘(ğ‘™)
ğ‘—âˆ’ğ‘¦ğ‘—)2
En la siguiente figura se muestra un ejemplo
donde se evalÂ´ ua la salida de una capa de activaci Â´on
utilizando esta funci Â´on de p Â´erdida.
Fig. 30.Ejemplo
Cambios a la Regla de la Cadena
Como las funcionesğ¿ ğ‘–,ğ‘§(ğ‘™)
ğ‘—yğ‘(ğ‘™)
ğ‘—han sido mod-
ificadas, es necesario plantear nuevas derivadas que
permitan actualizar los par Â´ametros de cada neurona
ğ‘—.
En la ecuaci Â´on se observa que, para ajustar un
peso espec Â´Ä±ficoğ‘¤(ğ‘™)
ğ‘—,ğ‘˜, debemos calcular sus derivadas
parciales. Sin embargo, gracias al concepto decach Â´e,
la Â´ unica derivada que cambia al actualizar un peso
diferente esğ›¿ğ‘§(ğ‘™)
ğ‘—
ğ›¿ğ‘¤(ğ‘™)
ğ‘—,ğ‘˜, mientras que el resto permanece
constante para toda la capa.
Las derivadas se expresan de la siguiente forma:
ğ›¿ğ¿ğ‘–
ğ›¿ğ‘ğ‘™
ğ‘—=((ğ‘ğ‘™
1âˆ’ğ‘¦ 1)2+(ğ‘ğ‘™
2âˆ’ğ‘¦ 2)2+Â·Â·Â·+(ğ‘ğ‘™
ğ‘›âˆ’ğ‘¦ğ‘›)2)
ğ›¿ğ¿ğ‘–
ğ›¿ğ‘ğ‘™
ğ‘—=2(ğ‘ğ‘™
ğ‘—âˆ’ğ‘¦ğ‘—)
ğ›¿ğ‘(ğ‘™)
ğ‘—
ğ›¿ğ‘§(ğ‘™)
ğ‘—=ğ‘”(ğ‘§(ğ‘™)
ğ‘—)(1âˆ’ğ‘”(ğ‘§(ğ‘™)
ğ‘—))
ğ›¿ğ‘§(ğ‘™)
ğ‘—
ğ›¿ğ‘¤(ğ‘™)
ğ‘—,ğ‘˜=ğ‘(ğ‘™âˆ’1)
ğ‘˜

Con esto se logra actualizar los pesos de la capağ‘™,
aunque las derivadas no cambian, s Â´Ä± deben manejarse
mÂ´asÂ´Ä±ndices a medida que la red crece en complejidad.
Capağ‘™âˆ’1
Cuando el c Â´alculo debe extenderse hacia una capa
anterior, como la capağ‘™âˆ’1, el procedimiento se vuelve
mÂ´as complejo. Esto ocurre porque, segÂ´ un el tama Ëœno
de la siguiente capa, el algoritmo requiere combinar
mÂ´as conexiones y par Â´ametros, lo que incrementa la
dificultad del c Â´alculo.
ğ›¿ğ¿ğ‘–
ğ›¿ğ‘(ğ‘™âˆ’1)
ğ‘˜=ğ‘›ğ‘™âˆ‘ï¸
ğ‘—=1ğ›¿ğ‘§(ğ‘™)
ğ‘—
ğ›¿ğ‘(ğ‘™âˆ’1)
ğ‘˜ğ›¿ğ‘(ğ‘™)
ğ‘—
ğ›¿ğ‘§(ğ‘™)
ğ‘—ğ›¿ğ¿ğ‘–
ğ›¿ğ‘(ğ‘™)
ğ‘—