Apuntes Semana 12
Apuntes del 10 de octubre de 2025
Andrey Ureña Bermudez – 2022017442
Inteligencia Artificial
Escuela de Computacion, Instituto Tecnologico de Costa Rica
Correo: andurena@estudiantec.cr
Abstract—Estos apuntes corresponden a la Semana 12 del
curso de Inteligencia Artificial, impartido por el profesor Steven
Pacheco Portugues en el Instituto Tecnologico de Costa Rica.
Se abordan los temas relacionados con los modelos de lenguaje
de gran escala (LLM), la tokenizacion, embeddings, y la intro-
duccion al paradigma deRetrieval-Augmented Generation(RAG)
y agentes inteligentes. Ademas, se presentan los anuncios del
curso y el cronograma restante del semestre.
I. INTRODUCCION
Durante esta sesion, se revisaron aspectos fundamentales
de los modelos de lenguaje modernos y su relacion con las
arquitecturas de inteligencia artificial actuales. Tambien se
analizaron conceptos claves para comprender como los LLM
procesan texto, transforman informacion en vectores, y aplican
tecnicas de recuperacion de conocimiento externo mediante
RAG. Finalmente, se discutieron las implicacioneseticas y el
uso responsable de estos sistemas.
II. ANUNCIOS DEL CURSO
•Se asigno laTarea 04sobre agentes, con fecha de entrega
el 6 de noviembre. La revision sera presencial y consiste
en la creacion de un agente funcional.
•Se presento el cronograma para el cierre del semestre,
organizado por semanas:
– Semana 13:
∗Martes 28 de octubre:Quiz 6 y temaQuantiza-
tion – Unsupervised.
∗Jueves 30 de octubre:TemaUnsupervised – PCA
y entrega del Proyecto I.
– Semana 14:
∗Martes 4 de noviembre:Revision presencial del
Proyecto I.
∗Jueves 6 de noviembre:Revision presencial del
Proyecto I y entrega de la Tarea 04:Agentes.
– Semana 15:
∗Martes 11 de noviembre:Clase virtual sobre
Unsupervised – PCA, asignacion del Proyecto II
y la Tarea 05:Autoencoder – Quantization.
∗Jueves 13 de noviembre:Revision virtual de la
tarea de agentes.
– Semana 16:
∗Martes 18 de noviembre:TemaRiesgos de la
Inteligencia Artificial.
Figura 1. Ejemplo de un modelo de red neuronal preentrenado.
– Semana 17:Semana colchon (sin actividades pro-
gramadas).
– Semana 18:
∗Martes 2 de diciembre:Examen I.
∗Jueves 4 de diciembre:Entrega del Proyecto II.
III. REPASO DE CONCEPTOS
A. Modelos de Lenguaje de Gran Escala (LLM)
Los LLM se han convertido en la base de los sistemas mod-
ernos de inteligencia artificial. Permiten generar, comprender
y razonar sobre texto, codigo, imagenes y audio.
Cada entrada (input) es representada mediante valores
numericos en punto flotante que describen caracteristicas. El
tratamiento varia segun si la entrada corresponde a texto,
numeros o simbolos.
B. Tokenizacion
La tokenizacion convierte las palabras, signos o simbolos
en representaciones numericas llamadastokens. Estos tokens
permiten al modelo procesar texto de manera eficiente.
Existen varios tipos de tokenizacion, resumidos en la
Tabla I.

Tabla I
TIPOS COMUNES DE TOKENIZACION Y SUS PRINCIPALES VENTAJAS.
Tipo EjemploVentaja princi-
pal
Palabra “Los medios” Simplificada
Caracter “L”, “o”, “s” Sin OOVs
Subpalabra (BPE,
WordPiece)“super” + “vivencia”Equilibra vocab-
ulario/contexto
Byte-level bytes UTF-8Soporta cualquier
simbolo
Espacio en blanco “Hola”, “mundo” Rapido y simple
Tras la tokenizacion, los tokens se representan como vec-
tores en un espacio continuo. Esto permite medir similitud
semantica entre palabras.
C. Metricas de similitud
Las metricas mas utilizadas incluyen:
•Distancia euclidiana:mide que tan separados estan dos
puntos en el espacio vectorial.
•Similitud del coseno:
Sim(a, b) =a·b
||a||||b||
Evalua elangulo entre los vectores; unangulo menor
implica mayor similitud.
Tabla II
EJEMPLO SIMPLIFICADO DE TOKENIZACION:LAS PALABRAS SE
TRANSFORMAN EN TOKENS CON IDENTIFICADORES NUMERICOS.
Palabra Token ID Numerico
Los los 105
LLM llm 2124
aprenden aprenden 893
patrones patrones 5749
D. Embeddings
Losembeddingsson representaciones numericas densas que
asignan a cada token un vector en un espacio continuo de
alta dimension. Capturan significado semantico y relaciones
contextuales entre palabras u oraciones completas, permitiendo
comparaciones mas profundas entre ideas o documentos.
E. Capacidades de los LLM
Debido a su entrenamiento a gran escala y arquitecturas
basadas en Transformers, los LLM presentan capacidades
emergentes:
•Comprension contextual.
•Generacion coherente de texto.
•Razonamiento y planificacion basica.
•Aprendizaje en el prompt (in-context learning).
•Multitarea sin reentrenamiento.
•Conocimiento estatico derivado de los datos de entre-
namiento.
•Costos computacionales elevados.IV. MATERIA NUEVA: RETRIEVAL-AUGMENTED
GENERATION(RAG)
Un sistema RAG conecta un LLM con un modulo
de recuperacion de informacion (retriever) para incorporar
conocimiento externo relevante durante la generacion de re-
spuestas.
A. Chunks
El texto se divide en fragmentos denominadoschunks, que
suelen contener entre 200 y 500 tokens. Cada fragmento se
transforma en un vector mediante un modelo de embeddings,
capturando su significado semantico.
B. Consulta o recuperacion
Dada una consulta, el sistema convierte la pregunta en
un embedding y calcula la similitud con los embeddings
indexados, devolviendo los mas cercanos semanticamente.
C. Aumento y generacion
Los fragmentos recuperados se integran en el prompt envi-
ado al LLM, proporcionando contexto adicional que guia la
respuesta hacia informacion verificada y relevante.
D. Ventajas principales
•Reduccion de alucinaciones.
•Actualizacion continua del conocimiento.
•Eficiencia de costos en entrenamiento.
•Aplicabilidad en dominios especializados.
•Asistentes empresariales enriquecidos.
•Soporte a la investigacion y atencion al cliente.
V. LLMTRADICIONAL VSAGENTE INTELIGENTE
Un LLM tradicional puede ofrecer informacion general,
pero carece de personalizacion y accion. Por ejemplo, si se le
consulta “¿Cuantos dias de vacaciones me quedan?”, no podra
responder con precision al no tener acceso a datos personales.
En cambio, un agente inteligente integra:
•Memoria:recuerda preferencias y contextos previos.
•Herramientas:accede a APIs externas (clima, vuelos,
calendario).
•Planificacion:organiza y ejecuta tareas en funcion de
objetivos.
•Accion:transforma planes en resultados concretos.
Este paradigma refleja la evolucion hacia sistemas que
razonan y actuan, mas alla de solo responder texto.
VI. ESCALAMIENTO RESPONSABLE
Es fundamental evaluar cuando realmente se requiere es-
calar de un modelo LLM a un sistema de agentes o mul-
tiagentes. Esto implica garantizar seguridad, privacidad y el
usoetico de los datos. Los agentes deben ser diseñados bajo
principios de transparencia y responsabilidad.

VII. CONCLUSION
Los temas revisados durante esta semana refuerzan la com-
prension de como los modelos de lenguaje modernos procesan
informacion y como se estan extendiendo hacia arquitecturas
mas complejas yutiles, como los sistemas RAG y los agentes
inteligentes. Estas herramientas representan un paso clave
hacia una inteligencia artificial mas contextual, adaptable y
responsable.
REFERENCIA
Pacheco Portuguez, S. (2025).Presentacion del curso de
Inteligencia Artificial. Instituto Tecnologico de Costa Rica.