Apuntes de clase #2
Luis Felipe Calderon Perez
Escuela de Ingenieria en Computacion
Tecnologico de Costa Rica
Cartago, Costa Rica
2021048663
9-10-2025
Resumen—Este documento presenta los apuntes de la decima
semana del curso de inteligencia artificial. Se realizoun repaso
de las redes neuronales convolucionales (CNN), partiendo de las
limitaciones de las redesfully connectedy la necesidad de extraer
informacion espacial de las imagenes. Se estudioel funciona-
miento de la estructura de las capas convolucionales,pooling
yfully-connectedyfeature map. Finalmente, se discutieron los
principios para disenar arquitecturas convolucionales eficientes,
considerando tamanos de filtro,stride,paddingy reduccion de
parametros.
Index Terms—CNN, capas convolucionales,transfer learning,
arquitecturas convolucionales.
I. BREVE REPASO DE LA CLASE ANTERIOR
I-A. Redes Convolucionales
Recibimos un input de caracteristicas, el cual transformamos
con una serie de capas ocultas.
Se habia visto hasta el momento las redesfully connected.
Teniamos el problema de que aprendemos una secuencia de
pixeles, lo que nos lleva a errores si movemos los objetos de
lugar o si aplicamos rotacion a los objetos. Lo correcto seria
sacar la informacion de la imagen para tomar una decision.
Tambien se hablodel dataset de CIFAR-10, donde hay
imagenes con 3 canales de 32x32. Y nos damos cuenta de
que no es escalable, ya que se tendrian 120,000 parametros
que ajustar unicamente en la entrada.
Por ello se buscan metodos mas eficientes; entonces lle-
gamos a las ConvNet, en donde las neuronas se organizan
en 3 dimensiones: largo, ancho y profundidad (canales). Las
Figura 1. Arquitectura CNN vs NN [3]
mismas estaran conectadas a pequenas regiones de la capa
anterior, y esto reduce el tamano de la imagen a un vector.
En cada cara del cubo de las neuronas se tienen n filtros
de tamano acorde a las imagenes de entrada y obtenemos un
Feature map, que es el resultado de aplicar el filtro a la imagen
anterior.
I-B. Arquitectura de CNN
Requiere de 3 capas principales
1.Convolutional layer
Computa el filtro contra una imagen.
Recibe de entrada el ancho, largo y canales.
Tiene n filtros que extraen caracteristicas de las ima-
genes.
Aplica una capa de activacion
Tiene como parametros(wx)n+b
2.Pooling layer
Reduce de la imagen en ancho y largo.
Se encarga de aplicar eldownsamplinga lo largo del
ancho y largo.

No tiene parametros.
Se introduce periodicamente en medio de capas con-
volucionales.
Lo mas usual es usarMax Pooling.
Formula dimensionalidad
• Entrada WxHxD.
• k, tamano de kernel.
• s,stride.
• D, mantiene la profundidad.
w2=w−k
s+ 1
3.Fully-Connected
Esta parte clasifica, ya que calcula la probabilidad de
pertenecer a una clase; transformando una imagen de
pixeles a probabilidad de pertenecer a una clase.
LasCNNsnos permiten resolver
Clasificacion de imagenes.
Segmentacion de objetos.
Segmentacion de instancias.
Procesamiento de imagenes.
II. CAPA DECONVOLUCION
Tiene varias caracteristicas, estan compuestas por varios
filtros (w), va a tener un comportamiento local, lo deslizamos
y va a seguir extrayendo las mismas features alrededor de la
imagen. Esto permite que los pesos se ajusten para que sirvan
en una posicion como en otra.
Figura 2. Arquitectura AlexNet [2]
Figura 3. Funcionamiento kernel capa de convolucion [3]
II-A. Filtro o Kernel
Es una matriz bidimensional de numeros, que transforma
una imagen en el momento en que deslice ese filtro, produ-
ciendo una imagen como salida. Para aplicar los filtros se va
a tener algo similar a un caso donde una imagen de entrada
tiene mucho ruido y se le aplica un Gaussian Kernel (tiene
una campana de Gauss) y da una imagen resultante como si
tuviera un Blur. Dependiendo del filtro usado, obtenemos otras
formas como resultado e influimos en el procesamiento.
II-B. Local Receptive fields
Una neurona esta conectada a un campo en especifico del
input. Esto es muy eficiente, ya que podria haber filtros que
extraigan lineas verticales, otros horizontales y asi.
II-B1. Campo receptivo:Es un filtro de nxn, donde cada
neurona estaraenfocada en un solo campo receptivo.
II-B2. Stride:Es la forma clasica de deslizar el filtro,
segun una cantidad de pasos que realiza el filtro sobre la
imagen durante la convolucion.
II-B3. Padding:Tecnica para agregar pixeles alrededor
del borde de la imagen, permitiendo controlar el tamano de
salida de la convolucion. Se recomienda que el padding se
llene con 0, ya que si se usa 1 podria generar ruido o mala
data. Y su formula esk−1
2, k = tamano del filtro.

II-B4. Calculo de dimensiones:
m, Cantidad de pixeles en fila/columna.
k, Tamano delkernel.
p, Tamano delpadding.
s, cantidad de pasos.
m−k+2p
s+ 1 =Dimension resultante
II-C. Pesos
Si se tuviera una imagen de 224x224x3, con un tamano
de kernel 11, stride de 4 y padding de 0, y le aplicamos la
formula anterior da 55. Y con la arquitectura de AlexNet 2
obtenemos que aprendimos 96 de profundidad.
II-D. Pesos Compartidos
Si ya tenemos un filtro que extrae cierta caracteristica, y
sirve para una posicion; tambien sirve para otra posicion. Por
lo que, vamos a usar el mismo filtro para toda la imagen.
II-E. Transfer learning
Se menciona que en el paper de AlexNet despues de aplicar
su arquitectura y lo referente a ella. Y se dan cuenta que en
las primeras capas hay figuras o informacion similar. Por lo
que se introduce el termino detransfer learning, que consiste
en pasar el peso de las primeras capas a otra red, para ahorrar
tiempo de entrenamiento.
III. ARQUITECTURASCONVOLUCIONALES
Se componen deConvolutional layer,Pooling layery
deDense layer. Se deben tomar desiciones sobre nuestra
arquitectura, por ejemplo, si la convolucion reduce el input
debo decidir si hago o no el pooling. Y estas desiciones
determinan el comportamiento del tamano de la imagen, pero
si la imagen es muy reducida, le llega poca informacion a
lafully connected. Se introdujoel termino de stack, que es,
INPUT→[[conv→relu]∗n→Pool?]∗m→[fc→
relu]∗k→fc, m≥0, k≥0, y se menciona que enpapers la cantidad de convoluciones y relu se usa la formula
de3≥n≥0.
III-A. ¿Que arquitectura preferimos?
Se prefiere a las arquitecturas con convoluciones pequenas,
ya que las convoluciones grandes nos llevan a que las neuronas
se computen de forma lineal y que la cantidad de pesos sea
mayor.
III-B. Algunas "reglas"
El tamano de la imagen deberia ser divisible por 2.
Las convoluciones deben usar campos receptivos peque-
nos 3x3, con un stride de 1.
Parapooling layer es comunmax poolingde F=2, S=3.
s, cantidad de pasos.
III-C. Menciones finales
Al final se mencionan arquitecturas similares a LeNet para
tomar en cuenta para el proyecto, tales como, AlexNet, AFNet,
GoogleNet (reduce parametros), VGG16 y ResNet.
Nota:Embedding, informacion distribuida en espacio vec-
torial que retorna mi NN.
REFERENCIAS
[1] “ResNet, AlexNet, VGGNet, Inception: Understanding various archi-
tectures of Convolutional Networks,” CV-Tricks.com, Aug. 01, 2022.
https://cv-tricks.com/cnn/understand-resnet-alexnet-vgg-inception/
[2] R. R. Abril, “Redes convolucionales,” La Maquina Oraculo, Jul. 2025,
[Online]. Available: https://lamaquinaoraculo.com/deep-learning/redes-
neuronales-convolucionales/
[3] S. A. P. Portuguez, “Apuntes de la clase de inteligencia artificial,”
Cartago, Costa Rica, agosto 2025, clase del 9 de octubre del 2025.