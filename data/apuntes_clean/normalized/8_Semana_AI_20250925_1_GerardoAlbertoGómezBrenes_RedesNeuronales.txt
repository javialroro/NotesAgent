Apuntes Semana 8, Clase
Gerardo Alberto Gomez Brenes - 2022089271
Resumen—Resumen compacto y organizado de la clase. In-
cluye: motivacion breve, pautas para la entrega, conceptos clave
de redes neuronales y las formulas que el profesor mencionoo
utilizocomo referencia.
I. MOTIVACION Y NOTAS GENERALES
Se compartioun ejemplo de robotica donde un modelo
adapta el comportamiento del robot ante la perdida o modi-
ficacion de una pata. Esto ilustra la capacidad de adaptacion
(aprendizaje por refuerzo y transferencia) y su potencial en
aplicaciones como protesis. Mensaje practico: hay areas de ML
que no son solo modelos de lenguaje; robotica y manipulacion
son opciones reales.
Se enfatizotambien la forma correcta de entregar tareas:
documentos cortos y autocontenidos, con figuras y tablas
dentro del texto, y maximo unas pocas paginas para esta
actividad (usar el grupo para dudas).
II. PAUTAS PARA LA ENTREGA
El trabajo debe ser claro y compacto. Incluir dentro del
documento:
Resultados relevantes (figuras, tablas) y su interpretacion
breve.
Referencias a notebooks o repositorios solo como com-
plemento, no como sustituto.
Seleccion critica de graficos: mostrar los que aporten a
la conclusion.
III. ENTRADA Y PRIMER MODELO:REGRESION
LOGISTICA
Imagenes28×28se representan como vectores de784
pixeles. La regresion logistica usa la transformacion lineal
seguida de la sigmoide:
z=wTx+b, σ(z) =1
1 +e−z.
Para clasificacion binaria (ej.: “¿es 5 o no?”)σ(z)da una
probabilidad entre 0 y 1.
IV. DE BINARIO A MULTICLASE Y NOTACION MATRICIAL
Para 10 clases se puede entrenar una regresion por clase o
usar una salida vectorial. Notacion comun:
Z=XWT+b,
donde, por ejemplo,Wpuede tener forma10×784(10
neuronas de salida y 784 entradas). Con unbatchde tamano
Bla entradaXesB×784y el resultadoZesB×10.
Ejemplo numerico: con 10 salidas y 784 entradas hay10×
784 = 7 840parametros solo en esa capa.V. ONE-HOT,SOFTMAX Y DECISION
Las etiquetas multi-clase se representan como vectoresone-
hot. Para obtener una distribucion de probabilidad sobre clases
se usasoftmax:
softmax(z) i=exp(z i)P
jexp(z j).
La prediccion final corresponde al indice con mayor probabi-
lidad.
VI. ARQUITECTURA:CAPAS Y CONEXIONES
Una red densa (fully connected / dense) conecta todas las
salidas de una capa con todas las entradas de la siguiente.
Anadir capas y activaciones no lineales permite resolver rela-
ciones no lineales que un perceptron simple no puede (ejemplo
clasico: XOR).
VII. FUNCIONES DE ACTIVACION Y GRADIENTES
Funciones mencionadas en clase:
Sigmoide:σ(z) = 1/(1 +e−z). Derivada:σ′(z) =
σ(z) (1−σ(z)). Tiene problemas devanishing gradient
en extremos.
ReLU:ReLU(z) = max(0, z). Es eficiente, pero puede
generar neuronas “muertas” cuando la derivada es cero.
Leaky ReLU: variante con pequena pendiente negativa
para evitar neuronas muertas.
tanh(z): acotada en(−1,1), util en algunos contextos.
VIII. FORWARD,PERDIDA Y RETROPROPAGACION
Elforwardcalcula salidas capa a capa. Con una funcion de
perdidaLse aplica retropropagacion para obtener derivadas
parciales∂L/∂wy actualizar parametros. Regla de actualiza-
cion (descenso de gradiente):
w←w−η∂L
∂w,
dondeηes la tasa de aprendizaje. Para el perceptron se
mencionoelhinge loss:
Lhinge= max(0,1−y(wTx+b)).
La retropropagacion usa la regla de la cadena para propagar
sensibilidades hacia atras; por eso es necesario que las capas
sean diferenciables.

IX. COSTOS COMPUTACIONALES Y DIMENSIONALIDAD
Aumentar neuronas y capas incrementa parametros y costo
de optimizacion. Lamaldicion de la dimensionalidadcomplica
la busqueda de soluciones optimas. Ejemplo: si una capa tiene
256 neuronas y la siguiente tiene 10, los pesos entre ellas son
10×256 = 2 560.
Recomendacion practica: reducir dimensiones innecesarias
(filtrado de features, PCA) cuando sea posible.
X. NOTAS SOBRE REPRESENTACIONES Y
CNN/EMBEDDINGS
Para imagenes, las CNN aplican kernels que extraen pa-
trones locales (bordes, texturas, formas). En lenguaje,embed-
dingscondensan palabras o frases en vectores de dimension
fija; la similitud semantica se mide por distancia en ese espacio
vectorial.