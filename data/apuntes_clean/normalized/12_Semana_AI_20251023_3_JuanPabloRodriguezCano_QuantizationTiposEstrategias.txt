Apuntes Semana 12
Apuntes del 23 de octubre
Juan Pablo Rodriguez Cano
IC-6200 Inteligencia Artificial
Tecnologico de Costa Rica
jp99@estudiantec.cr
Abstract—La cuantizacion en una tecnica en redes neuronales
para reducir el tamaño de los parametros de los modelos,
principalmente transformando los datos de punto flotante a
enteros, lo cual ademas reduce el tiempo de computacion de
operaciones. Esta tecnica es esencial para distribuir modelos en
sistemas comerciales y ampliar la cantidad de plataformas que
puedan correr estos modelos.
Index Terms—cuantizacion, punto flotante, reduccion de
paramtetros.
I. ACTIVIDAD DEIEEE
Es un evento anual que se dara esta vez en noviembre
en la sabana. Es una oportunidad para conocer sobre temas
innovadores en inteligencia artificial y biologia molecular. Es
una oportunidad para crear contactos dentro de la industria
ya que los presentadores suelen ser receptivos al publico y
disponen de tiempo para hablar.
II. QUANTIZATION
Una vez que entrenado un modelo de redes neuronales, se
debe colocar en un sistema para la distribucion de este. Para
esto existen varias tecnicas, entre ellas, una opcion comun es
utilizar el framework ONNX, que toma modelos escritos en
diferentes lenguajes y bibliotecas y se crea una version que
maximiza la eficiencia de recursos y computacion utilizando
C++.
El mecanismo por el cual se disminuye es la cuantizacion y
se enfoca en el hecho que los parametros de los modelos son
representados con tipos de datos de punto flotante, se reducen
para hacer los modelos mas densos con tecnicas especiales
para no afectar mucho la precision de la inferencia. Aunque
no es posible no introducir error, es necesario asumir esta
desventaja para desplegar los modelos.
LLaMA 2 es un modelo muy popular y notorio por tener
un tamaño muy grande, tiene 70 mil millones de parametros,
cada uno esta representado por un punto flotante de 32 bits,
lo que resulta en 28GB que deberian estar en memoria si
se quisiera utilizar en una maquina local. Esto claramente
no es viable porque la mayoria de maquinas comerciales
cuentan con una capacidad menor a eso. Ademas, las opera-
ciones que se se hacen con datos de punto flotante son muy
lentas en comparacion a datos representados por enteros. La
cuantizacion hace una reduccion de los bits requeridos para
representar cada parametro y lo convierte a enteros, que se
pueden representar en las siguientes configuraciones: 8, 5, 2y hasta 1 bit. La cuantizacion resulta en un menor tiempo de
inferencia y menor consumo de energia, ademas de facilitar
la opcion de correr estos modelos en sistemas pequeños como
dispositivos moviles o sistemas embebidos.
A. Representacion de numeros
Se suelen utilizar numeros en bloques de 8 bits para los
enteros, para representar numeros negativos se utiliza el com-
plemento a2 en los computadores. En contraste, para los punto
flotantes se utiliza el ieee-754, cuyo tamaño de representacion
es de 32 bits, se utiliza la siguiente formula.
v= (−1)sign×2E−127×(1 +23X
i=1b23−i2−i)
Para no perder tanta informacion se tiene el siguiente
mecanismo:
1) Antes de que las entradas lleguen a la siguiente capa se
cuantizan los pesos
2) Estos pesos se limitan a ciertos rangos, dependiendo de
la cantidad de bits de la cuantizacion. Lo que se quiere
es que la distribucion sea equivalente.
3) Se hacen las operaciones con los datos de tipo entero.
4) Al salir de la capa, se de-cuantizan los pesos para que las
siguientes capas operen con numeros de punto flotante,
sin ”saber” que fueron cuantizados.
III. TIPOS DE CUANTIZACION
1) Asimetrica→el valor de 0 corresponde al valor menor
y el maximo es el peso maximo
2) Simetrica→el cero es el peso 0, el valor absoluto
maximo de los pesos se mapea a un extremo, si es
negativo se mapea al valor mas negativo dentro de los
valores posibles con los bits
A. Cuantizacion Asimetrica
xq=clamp(xf
s+z; 0; 2n−1)
xf=valorflotante
z=−1×β
s
*s es el parametro de escalado
s=α−β
2b−1
xf=s(x q−z)→permite volver al valor original con un
grado de error

B. Cuantizacion Simetrica
rango:[−(2n−1),(2n−1)]
s=abs(α)
2n−1−1
xf=sx q
IV. ESTRATEGIAS DE SELECCION DEL RANGO
•Cuantizacion Dinamica
–Calculo estadistico de cual sera el valor de esa capa
–se utiliza en la etapa post-training quantization
•Post training quantization
–hay que tratar los pesos atipicos porque puede con-
finar los demas pesos en un rango muy pequeño e
introduce mas error
–Se puede utilizar el percentil en vez del min y max
–agregamos observers que se encargan de hacer la
estadisticas, calibran todas las salidas de la capa
–se hace con los datos de prueba
•Quantization Aware Training (QAT)
–insertar modulos irreales en la computacion de grafo
del modelo para similar el efector de cuantizacion
durante el entrenamiento.
–La funcion de perdida es usada para actualizar los
pesos que constantemente sufren.
-