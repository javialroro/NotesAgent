Apuntes Semana 4 Clase #2
28/08/2025
Alex Steven Naranjo Masis
Instituto Tecnologico de Costa Rica
Cartago, Costa Rica
Email: alnaranjo@estudiantec.cr
Resumen —Para esta clase se repasaron temas de la clase
anterior como lo son KNN, regresion lineal, Mean Square Error,
Descenso del gradiente y un repaso general de derivadas. Y luego
del repaso continuamos viendo temas como lo son: Derivadas
Parciales con respecto a w y b en la funcion de perdida con
el fin de actualizarlos y ajustar la funcion, y porultimo vimos
Epoch y Batch.
Index Terms —KNN, Regresion Lineal, Mean Square Error,
MAE, Descenso del Gradiente, Epoch y Batch
I. N OTICAS DE LA SEMANA
A. Small Language Models are the Future of Agentic AI
En el articulo se dice que los modelos de lenguaje pequeños
(SLMs) son mas adecuados que los grandes (LLMs) para
ciertos sistemas inteligentes autonomos (agentic AI), especial-
mente en tareas especializadas y repetitivas. [1]
B. Canaries in the Coal Mine? Six Facts about the Recent
Employment Effects of Artificial Intelligence
El estudio analiza como la adopcion de la inteligencia
artificial generativa ha afectado al mercado laboral en EE.UU.,
utilizando datos administrativos mensuales de nominas de
ADP, el mayor procesador de nominas del pais, el cual abarca
millones de trabajadores en decenas de miles de empresas. [2]
II. R EPASO CLASE ANTERIOR
A. K Nearest Neighbor (kNN)
En resumen, cuando obtenemos una nueva instancia, me-
dimos contra todos los elementos del dataset, y tomamos las
distancias mas cercanas, y en base a eso determinabamos la
clase de la nueva instancia.
Contamos con el hiperparametro K.
Es un algoritmo de lazy learning, porque realmente no se
aprende de los datos.
A1. Ventajas:
Sencillo de implementar.
Es flexible: Aplica tanto para regresion como clasifica-
cion.
A2. Desventajas:
Las caracteristicas irrelevantes pueden distorsionar las
distancias
Es computacionalmente costoso.
Poco eficiente en grandes volumenes de datos.B. Regresion Lineal
Lo que queremos hacer es encontrar la linea que mejor se
ajuste a los datos, para poder realizar una prediccion de un
valor.
B1. Variables:
Variables independientes: Son las caracteristicas de la
muestra.
Variables dependientes: Es el valor a predecir y es
afectada por las varibales independientes
Con esto lo que queremos hacer es encontrar un modelo
estadistico lineal: fw,b(X) =wX +b
Donde:
X es un vector D-dimensional.
w es un vector D-dimensional.
b un numero real.
wX es un producto punto, dandonos como resultado un
escalar.
El modelo esta parametrizado por wyb, por lo que
debemos encontrar los valoresoptimos de wybque haran
que la funcion realice las predicciones mas precisas. Pero ojo,
Optimo =Perfecto
Figura 1. Tipos de Regresion
C. Funcion de Perdida
Necesitamos de un metodo que nos permita cuantificar que
tan bien se ajusta nuestro modelo a los datos. Funcion de
Perdida = Medida del error del modelo

D. Error Cuadratico Medio (MSE)
Es el resultado del modelo contra la etiqueta. Sumamos
todos los errores de los samples y lo promediamos.
L=1
NNX
i=1(fw,b(xi)−yi)2
D1. Conceptos Clave):
Loss Function: (fw,b(xi)−yi)2es la medida de pena-
lidad que cuantifica el error de cada ejemplo.
Error Cuadratico: Penaliza los errores grandes.
Cost Function: Es el promedio de la Loss Function sobre
todo el dataset.
Objetivo: Minimizar L para ajustar los parametros w,b.
El motivo por el cual queremos minimizar L, es porque
entre menor sea L, significa que tenemos un mejor modelo, y
entre mas grande significa que tenemos un peor modelo.
E. ¿Por que MSE y no MAE?
Es debido a que es cuadratica, y esto nos asegura que vamos
a tener un punto minimo. Y tambien es porque la funcion no
MAE no es smooth, por lo que no nos va a permitir obtener
las derivadas en todos los puntos, lo que induce a errores de
calculo
F . Derivadas Generales
Regla Funcionf(x) Derivada f′(x)
Constante k 0
Identidad x 1
Constante multiplicativa kx k
Potencia xnnxn−1
Suma u(x) +v(x) u′(x) +v′(x)
Producto u(x)v(x) u′(x)v(x) +u(x)v′(x)
Constante sumada u(x) +z u′(x)
Derivadas parciales f(x, y) = 2x+ 3y∂f
∂x= 2,∂f
∂y= 3
Cuadro I
REPASO DE DERIVADAS BASICAS
G. Descenso del gradiente
El descenso del gradiente es un algoritmo iterativo de opti-
mizacion para encontrar el minimo de una funcion. Funciona
actualizando repetidamente los parametros en la direccion
opuesta al gradiente de la funcion de costo.
G1. Regla de actualizacion:
xnuevo =xantiguo−α·(2x)
G2. Importancia del α:Es el learning rate, debe ser
pequeño para no pasarnos del punto minimo. Este es un
hiperparametro
Figura 2. Comparacion de distintos valores para alpha
III. C ONTENIDO DE LA CLASE
A. Funcion de Perdida y sus Derivadas Parciales
Para optimizar los parametros wybde nuestro modelo,
necesitamos actualizar sus valores de manera que la funcion
de perdida se minimice. Para esto, evaluamos como cada
parametro afecta la perdida utilizando derivadas parciales con
respecto a wyb.
Considerando la funcion de perdida basada en el error
cuadratico medio (MSE) para nuestro modelo lineal fw,b(x) =
wx+b, tenemos:
L(w, b) =1
NNX
i=1((wxi+b)−yi)2
Las derivadas parciales de Lcon respecto a wybse
calculan como:
∂L
∂w=2
NNX
i=1((wxi+b)−yi)xi
∂L
∂b=2
NNX
i=1((wxi+b)−yi)
Estas derivadas nos indican la direccion y magnitud del
ajuste necesario para cada parametro, permitiendo aplicar
algoritmos de optimizacion como el gradient descent para
actualizar wyb.
B. Epoch
Una epoch es una iteracion completa sobre todo el conjunto
de entrenamiento. Es un hiperparametro que define cuantas ve-
ces se recorrera el dataset completo durante el entrenamiento,
por ejemplo, epochs = 5 .
Si tenemos 10 000 muestras y ejecutamos 5 epochs, signi-
fica que se procesaran todas las muestras 5 veces en total. La
actualizacion de los parametros puede realizarse al finalizar
cada epoch o de manera mas frecuente utilizando batches.
C. Batch
Unbatch es un subconjunto del conjunto de entrenamiento
que se utiliza para calcular la gradiente y actualizar los
parametros del modelo.
Por ejemplo, si tenemos 10 000 muestras y un batch
size = 1 000 , necesitaremos 10 batches para completar

una epoch. Cada batch permite calcular la gradiente y actua-
lizar los parametros sin esperar a procesar todo el dataset.
Dependiendo de la estrategia, se puede actualizar los parame-
tros despues de cada batch o acumular gradientes antes de la
actualizacion.
C1. Batch Gradient Descent (Vanilla): ElBatch Gra-
dient Descent calcula la gradiente utilizando todo el dataset:
∇L=1
NNX
i=1∂L
∂θi
y actualiza los parametros solo despues de procesar el
conjunto completo.
Ventajas:
Gradiente estable y pasos consistentes.
Ayuda a evitar minimos locales y aporta robustez en la
optimizacion.
Desventajas:
Requiere todo el dataset en memoria.
Las actualizaciones son lentas para datasets grandes.
La gradiente muy estable puede ocultar señalesutiles.
Figura 3. Batch Gradient Descent
C2. Stochastic Gradient Descent (SGD): ElStochastic
Gradient Descent actualiza los parametros despues de cada
muestra del dataset (o un pequeño conjunto aleatorio de
muestras).
Ventajas:
Detecta rapidamente si el algoritmo puede converger.
´Util para datasets muy grandes.
Desventajas:
Las actualizaciones pueden ser muy ruidosas.
La trayectoria de los parametros es oscilatoria.
Muchas actualizaciones pueden ser costosas computacio-
nalmente.
w←w−α∂L
∂w
Figura 4. Stochastic Gradient DescentC3. Mini-batch Gradient Descent: ElMini-batch Gra-
dient Descent combina las estrategias anteriores: calcula la
gradiente sobre batches de tamaño intermedio.
Ventajas:
Reduce el ruido respecto a SGD y es mas estable.
Mas eficiente que Batch GD.
Mejora la explotacion de hardware (vectorizacion,
GPUs).
Figura 5. Mini-batch Gradient Descent
REFERENCIAS
[1] Belcak, P., et al, ”Small Language Models are the Future of Agentic
AI”2025.
[2] E. Brynjolfsson et al., “Canaries in the Coal Mine? Six Facts about the
Recent Employment Effects of Artificial Intelligence” 2025.