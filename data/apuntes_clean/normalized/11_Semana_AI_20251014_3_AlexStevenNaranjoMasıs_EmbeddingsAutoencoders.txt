Apuntes Semana 11 Clase #1
14/10/2025
Alex Steven Naranjo Masis
Instituto Tecnologico de Costa Rica
Cartago, Costa Rica
Email: alnaranjo@estudiantec.cr
Resumen—Este documento recopila los apuntes de la clase
del martes 14 de octubre de 2025 para el curso de Inteligencia
Artificial. Se abordaron los fundamentos de las Redes Neuronales
Convolucionales (CNN), explicando el funcionamiento de los
filtros, el campo receptivo, elstride, elpaddingy las capas
depoolingpara la extraccion de caracteristicas en imagenes.
Ademas, se estudiaron arquitecturas clasicas como LeNet, Alex-
Net, GoogleNet/Inception, VGG16, ResNet y DenseNet. Finalmen-
te, se introdujeron los conceptos deembeddings, visualizacion de
activaciones y autoencoders, analizando sus aplicaciones en re-
duccion de dimensionalidad, deteccion de anomalias, eliminacion
de ruido y super-resolucion, junto con consideraciones practicas
de entrenamiento y seleccion de hiperparametros.
Index Terms—Redes Neuronales Convolucionales, Pooling,
Embeddings, Visualizacion, Autoencoder, Deep Learning
I. INTRODUCCION
Las Redes Neuronales Convolucionales (CNN) son un pilar
en la vision por computadora moderna, pues permiten extraer
automaticamente caracteristicas jerarquicas de las imagenes.
Comprender sus componentes como filtros, campos receptivos,
stride,paddingypoolinges esencial para diseñar arquitecturas
eficientes. Por su parte, los autoencoders complementan este
aprendizaje al representar la informacion de forma comprimi-
da, sin necesidad de etiquetas externas, y habilitan tareas de
aprendizaje no supervisado/semisupervisado.
II. FUNDAMENTOS DEREDESNEURONALES
CONVOLUCIONALES(CNNS)
A. Filtros (Kernels) y Campos Receptivos
Un filtro 2D de tamañok×kse desliza sobre la imagen (o
mapa de activacion) para producir unfeature map. Para una
entrada RGBH×W×C inyC outfiltros, cada filtro tiene
tamañok×k×C iny produce un canal en la salida.
Filtro Gaussiano:suaviza la imagen (blur) y reduce
ruido; resalta contornos al combinarse con operadores
de gradiente.
Campo Receptivo (RF):region de la entrada que “ve”
una neurona de una capa dada. Aumenta con la profun-
didad. Si encadenamos capas con kernelk iystrides i,
el RF efectivo crece de forma acumulativa.
Parametros y costo:el numero de parametros en una capa
conv esk2·Cin·Cout+C out(sesgo). La complejidad
computacional se aproxima porH out·Wout·k2·Cin·Cout.B. Parametros de la Convolucion: Stride, Padding y Tamaño
de Salida
Para una entrada 1D de longitudm, kernelk,paddingpy
strides, la salida es:
out=m+ 2p−k
s
+ 1.
En 2D se aplica por dimension (alto y ancho). Elpadding
simetrico tipico para “conservacion de tamaño” cons= 1es
p=k−1
2(sikes impar). Elstride>1reduce la resolucion
espacial.
C. Pesos Compartidos y Eficiencia
Lacomparticion de pesosaplica el mismo kernel en todas
las posiciones espaciales, reduciendo parametros frente a capas
densas. En primeras capas, la red aprende bordes y texturas;
en capas profundas, patrones semanticos mas abstractos.
D. Capa de Pooling
Reduce la resolucion espacial conservando canales:
Max Pooling:retiene el valor maximo de cada ventana.
Average Pooling:promedia los valores.
Regla practica:pooling2×2constride2 para reduccion a la
mitad. MantieneD=C iny reduceH, W.
E. Activaciones, Normalizacion y Regularizacion
Activacion:ReLU es estandar en CNN modernas (evita
saturacion y acelera entrenamiento). Tanh/sigmoid pue-
den usarse en salidas especificas.
Batch Normalization (BN):estabiliza la distribucion
de activaciones, permite mayores tasas de aprendizaje y
acelera la convergencia.
Regularizacion:Dropout (tipico en capas densas), L2
(weight decay) ydata augmentationreducen sobreajuste.
F . Capa Fully-Connected (MLP) y Clasificacion
Tras extraer mapas de activacion, se aplicaflatten(oglobal
average pooling) y capas densas para clasificacion. En proble-
mas multi-clase se usasoftmaxy perdida de entropia cruzada.
III. ARQUITECTURASCONVOLUCIONALES
A. LeNet-5
Pionera (LeCun, 1998) para digitos manuscritos (MNIST).
Dos bloques conv+pooling y capas densas. Introdujo la viabi-
lidad practica de CNNs.

B. AlexNet (2012)
Krizhevsky et al. popularizan ReLU,dropout, entrenamiento
en multiples GPUs y kernels grandes (11×11,5×5,3×3)
en entradas224×224. Disparo la adopcion dedeep learning
a gran escala.
C. ZFNet y Visualizacion Intermedia
Ajusta tamaños de kernel/stride y estudiafeature maps
internos para entender que aprende cada capa, motivando
practicas de diseño y depuracion.
D. GoogLeNet / Inception
Modulos con ramas paralelas (1×1,3×3,5×5+max
pooling); reduce parametros (de∼60M a∼4M) usando cuellos
1×1yglobal average poolingal final.
E. VGG-16
Filosofia de simplicidad: solo3×3+ profundidad (16/19
capas). A pesar de muchos parametros, es unbaselinedidacti-
co muy usado.
F . ResNet (Redes Residuales)
Skip connections(y=F(x) +x) permiten entrenar redes
muy profundas mitigandovanishing gradient. Bloquesba-
sic/bottleneckse apilan eficientemente.
G. DenseNet
Conexiones densas “todas con todas” dentro del bloque;
fomenta reutilizacion de caracteristicas, mejora el flujo de
gradiente y reduce parametros a igual rendimiento.
IV. EXPLICABILIDAD DELMODELO YEMBEDDINGS
A. Visualizacion de Activaciones y Filtros
Observarfeature mapsmuestra que regiones activan ca-
da neurona. En capas iniciales, activaciones recuerdan bor-
des/colores; en capas profundas, particulas semanticas mas
complejas.
B. Embeddings y Reduccion de Dimensionalidad
Losembeddingsson vectores enRdque capturan semantica.
Vectores de clases similares tienden a agruparse en el espacio
latente.
t-SNE:proyeccion no lineal a 2D/3D preservando vecin-
darios locales.
PCA:proyeccion lineal; ´util comobaselineo preproce-
samiento.
C. Mapas de Activacion (Heatmaps)
Heatmapsseñalan zonas que mas influyen en la prediccion
(´util en aplicaciones medicas/industriales para justificar deci-
siones).
Figura 1. Representacion deembeddingsmediante t-SNE.
Figura 2. Estructura basica de un Autoencoder.
V. AUTOENCODERS(CODIFICADORESAUTOMATICOS)
A. Estructura General y Objetivo
Encoder→Espacio Latente→Decoder
Aprenden a reconstruir la entrada. Aunque la señal de entre-
namiento es auto-supervisada (salida = entrada), se consideran
tipicamente metodos no supervisados por no requerir etiquetas
externas.
B. Componentes y Variantes
Encoder:reduce espacialidad y comprime informacion
(conv +downsampling).
Latente:vector/tensor compacto; su tamaño controla capaci-
dad vs. compresion.

Figura 3. Ejemplo conceptual de super-resolucion con autoencoder.
Decoder:reconstruye conupsamplingo convoluciones trans-
puestas.
Variantes:Denoising(entrenar con entrada ruidosa y salida
limpia),Sparse(regulariza latente),Under/Overcomplete.(No-
ta: VAEs y GANs exceden el alcance de esta clase, pero se
relacionan con lo generativo.)
C. Funciones de Perdida Comunes
MSE(Mean Squared Error): reconstruccion pixel a pixel
(continuo).
MAE: mas robusto aoutliers.
BCE/BCEWithLogits: para imagenes normaliza-
das/binarizadas.
Perceptual/SSIM(opcional): mejor correlacion percep-
tual que MSE.
D. Aplicaciones
Reduccion de dimensionalidady almacenamiento efi-
ciente en BBDD vectoriales.
Deteccion de anomalias:entrenar con datos “normales”;
altas perdidas de reconstruccion sugieren anomalias.
Eliminacion de ruido (Denoising).
Super-resolucion:reconstruir versiones de mayor reso-
lucion.
E. Hiperparametros Relevantes
Tamaño del latente:mas pequeño = mayor com-
presion/menor fidelidad; mas grande = mayor capaci-
dad/costo.
Profundidad del encoder/decodery tipo deupsampling
(nearest/bilinear vs.ConvTranspose2d).
Perdida de reconstruccion(MSE/MAE/BCE/SSIM)
segun dominio.
VI. BUENASPRACTICAS DEENTRENAMIENTO YDISEÑO
A. Preprocesamiento y Aumento de Datos
Normalizacion por canal (media/desviacion deldataset).
Data augmentationmoderado: flips,crops, ligerosjitters;
evitaoverfitting.B. Optimizacion y Regularizacion
Optimizadores:SGD+momentum (control fino), Adam
(rapida convergencia).
LR scheduling:step/cosine/plateau.
Regularizacion:L2 (weight decay),dropout(sobre todo
en densas),early stopping.
C. Reglas Practicas de Arquitectura
Dimensiones divisibles entre 2 para facilitarpooling.
Preferir kernels pequeños (3×3/5×5) y apilar
profundidad para mayor no linealidad.
Usarglobal average poolingantes de densas para reducir
parametros.
Insertar BN despues de conv y antes de ReLU para
estabilidad.
D. Notas de Implementacion
Enframeworkscomo PyTorch, la reconstruccion en deco-
ders suele emplearConvTranspose2doUpsample+1×
1conv; para clasificacion,CrossEntropyLoss(con
LogSoftmaxinterno) es estandar.
VII. CONCLUSIONES
Las CNN han transformado la vision por computadora
al extraer jerarquias de caracteristicas de manera automatica
y eficiente. No obstante, su interpretabilidad sigue siendo
un reto; tecnicas de visualizacion,embeddingsyheatmaps
ayudan a entender y validar decisiones. Los autoencoders
extienden estos conceptos hacia la compresion, reconstruccion
y generacion de datos, habilitando aplicaciones practicas como
reduccion de dimensionalidad, deteccion de anomalias y super-
resolucion. Una ingenieria cuidadosa de arquitectura,datay
entrenamiento es clave para un desempeño robusto.
REFERENCIAS
[1] Y . LeCun, L. Bottou, Y . Bengio, and P. Haffner, “Gradient-based learning
applied to document recognition,”Proceedings of the IEEE, vol. 86, no.
11, pp. 2278–2324, 1998.
[2] C. Szegedy et al., “Going deeper with convolutions,”CVPR, 2015.