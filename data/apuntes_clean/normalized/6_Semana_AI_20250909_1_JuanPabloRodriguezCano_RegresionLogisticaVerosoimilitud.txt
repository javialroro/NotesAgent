Apuntes Semana 6
Apuntes del 09 de septiembre
Juan Pablo Rodriguez Cano
IC-6200 Inteligencia Artificial
Tecnologico de Costa Rica
jp99@estudiantec.cr
Abstract —En este documento se detallan las indicaciones de
la tarea 1 de Inteligencia Artifical y se introduce el tema
de regresion logistica como un modelo de clasificacion cuyas
propiedades de funcion son aptas para modelar problemas
complejos y la optimizacion de recursos.
Index Terms —
I. P REGUNTAS DEL QUIZ
1) Describa que es ”overfitting” y ”underfitting”.
R/ ”Overfitting” es cuando el modelo tiene una mejor
metrica con el conjunto de entrenmaiento que con el
conjunto de testing, lo cual indica una pobre general-
izacion con datos nuevos. ”Underfitting” es cuando el
model no logra captar la relacion entre los features de
manera que los puntajes de metrica son bajos para el
conjunto de entrenamiento y testeo.
2) Describa k-Fold Cross-Validation
Se subdivide el conjunto de entrenamiento en k-1 partes.
En cadaepoca se entrenan k-1 partes y se utiliza el otro
subconjunto para la validacion, el
3) ¿Que es un minimo global y minimo local en una
funcion?
Un minimo local es el valor minimo de una funcion en
una vecindad reducida, mientras que el minimo global
se refiere al minimo global a traves de todo el dominio
de la funcion.
4) Desarrolle la derivada parcial de L con respecto a W de:
L=1
NX
((wxi+b)−yi)
∂L
∂w=2
NX
((wxi+b)−yi)xi)
II. I NDICACIONES DE LA TAREA
•La tarea se deber realizar en grupos de 3 personas.
•La fecha de entrega es el 16 de septiembre.
•Solo hace falta que una persona del grupo suba la tarea.
En el nombre del archivo zip debe venir el nombre de
todos.
•No se puede utilizar ninguna biblioteca que no sea numpy
o pandas
•Kagg;e es una plataforma con datasets para machine
learning para el publico y tambien presentan oportu-
nidades para participar en concursos de ML.
•La funcion de perdida y la graficacion debe se manual•El metodo describe() resume los datos analiticos que son
importantes para saber como se comportan los features
•No debe haber codigo en el informe, solo resultados,
analisis etc.
•El notebook sera evidencia del trabajo
•El objetivo es ver si la relacion con la prediccion es lineal,
y si no aplicar un feature engineering
•figuras en IEEE siempre van en la parte superior o inferior
de las columnas.
•El formato es de IEEE para conferencias
III. A CTIVIDAD DE IEEE
Es un evento anual que se dara esta vez en noviembre
en la sabana. Es una oportunidad para conocer sobre temas
innovadores en inteligencia artificial y biologia molecular. Es
una oportunidad para crear contactos dentro de la industria
ya que los presentadores suelen ser receptivos al publico y
disponen de tiempo para hablar.
IV. C ONTENIDO DE CLASE
A. Regresion Logisitca
A diferencia de la regresion lineal que es un modelo que
predice un numero real a partir de los features, la regresion
logistica es un modelo de clasificacion binaria. El resultado
de dicho modelo es la probabilidad de que suceda un evento
y esta basado en la distribucion de Bernoulli: P(X=k) =
pk(1−p)1−k
B. Funcion Sigmoide
Esta funcion es conveniente porque puede modelar com-
portamientos no lineales, el cual es un comportamiento muy

comun en la mayoria de problemas. Trae consigo una mayor
complejidad pero a su vez logra resolver problemas mas
complejos.
Su codominio es de 0 a 1 y esto es muy conveniente ya que
los valores probabilisticos comparten ese mismo espacio.
La funcion sigmoide se expresa de la siguiente manera
σ(x) =1
1 +e−x⇒σ(fw,b(x)) =1
1 +e−fw,bx
La manera en que esta funcion se convierte en un clasi-
ficador es al escoger un umbral. este umbral se utiliza para
definir un punto a partir de cual se calsifica un evento con
una etiqueta o la otra. Por lo general se suele escoger un valor
umbral de 0.5.
C. Derivada de la funcion Sigmoide
Como la regresion logistica es un clasificador, se debe
encontrar una funcion de peridica adecuada para el problema.
Para esto se debe analizar la derivada de la funcion sigmoide,
ya que es necesario para cualquier problema de optimizacion.
σ′(x) =1′(1 +e−x)−(1(1 + e−x)′)
(1 +e−x)2
⇒σ′(x) =σ(x)(1−σ(x))
Como se puede notar, la derivada se puede expresar en
terminos de la funcion misma, lo cual lo hace muy conveniente
ya que no se requieren operaciones muy complejas y con esto
se obtiene una mayor eficiencia.
D. Funcion de Perdida: Verosimilitud
En vez de utilizar MSE o MAE, se utiliza la verosimilitud.
Esta esta dada por la siguiente ecuacion
L=Y
fw,b(xi)yi(1−fw,b(xi))1−yi
El resultado que se obtiene para un punto en esta ecuacion
es la probabilidad de que su etiqueta sea yicon los pesos
wactuales. Como se quiere optimizar los pesos para los
cuales se obtiene una mejor metrica, se debe derivar esta
funcion. Sin embargo, existe un problema con esta expresion
donde una multiplicacion incluye polinomios muy grandes,
y calcular la dervida respectiva se vuelve muy complejo
y computacionalmente costoso. Ademas, como se trata de
valores probabilisticos, o sea, de 0 a 1, su multiplicacion
se vuelve extremadamente pequeña y asi la derivada de la
funcion se vuelve virtualmente cero, y esto no cambia los
pesos en el paso de entrenamiento. A esto se le conoce como el
fenomeno de ”vanishing gradients”. Por esta razon se aplican
los teoremas de logaritmo y se obtiene la siguiente expresion.
ln(L) =X
ln(fw,b(xi)yi+ln((1−fw, b (xi))1−yi)
⇒ln(L) =X
yiln(fw,b(xi) + 1−yiln((1−fw, b (xi)))
Esto se convierte en una tarea mas facil de optimizacion. Sin
embargo, la funcion de logaritmo es estrictamente creciente,por lo que hay que convertir un problema de maximizacion en
minimizacion. Para esto, simplemente se da vuelta a la funcion
delnmultiplicando por -1.