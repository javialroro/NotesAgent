1
Apuntes de la clase del 25 de setiembre de 2025
Kevin Carranza Jimenez
Escuela de Ingenieria en Computacion
Tecnologico de Costa Rica
kcarranza@estudiantec.cr
Abstract —This document summarizes the lecture held on
September 25, 2025, which included the presentation of the
company Skild.ia, focused on applying artificial intelligence algo-
rithms for robot control. It also provides a review of the previous
lecture, covering neural networks from logistic regression to their
application in binary classifiers using multinomial expressions.
The session then introduces the perceptron model, defined as a
linear regression with a Hinge Loss function. It is emphasized that
a single perceptron cannot solve non-linear functions, although
multiple perceptrons can be combined to achieve this. Finally,
the multilayer perceptron is introduced as a form of deep neural
network with biological inspiration.
Index Terms —Skild, Regresion lineal, multinomial red neu-
ronal, perceptron.
I. I NTRODUCTION
EN el desarrollo del curso, las clases recientes han
abordado los fundamentos de las redes neuronales y
su evolucion hacia modelos mas complejos. La sesion del
25 de septiembre de 2025 incluyo como tema de interes un
video de la empresa Skild.ia, que tiene como objetivo utilizar
algoritmos de inteligencia artificial para controlar robots y
estos puedan emplear cualquier tarea, trayendo la inteligencia
artificial al mundo fisico. Tambien se incluyo tanto la
revision de conceptos previamente estudiados, entre ellos la
regresion logistica para desarrollar expresiones multinomiales
y con multiples capas de estas, desarrollar redes neuronales.
Tambien la introduccion del perceptron, considerado el punto
de partida para las redes neuronales profundas.
II. S KILD .IA
Skild AI es una startup emergente dedicada al desarrollo de
una inteligencia artificial de proposito general para el control
de robots de multiples tipos (humanoides, brazos roboticos,
plataformas de locomocion, etc.).
La propuesta central de Skild AI es crear un “cerebro robotico
omni-corporal” —denominado Skild Brain— que permita
que un mismo modelo de IA controle diferentes cuerpos
roboticos sin necesidad de reentrenamientos especificos para
cada hardware.
Un aspecto clave de su diseño es la capacidad de adaptacion
a fallos o cambios drasticos en la morfologia del robot:
cuando un robot pierde una extremidad o sufre un daño, el
modelo puede reorganizar su control para seguir operando,
aprovechando la experiencia aprendida previamente [1].III. R EDES NEURONALES
Una Red Neuronal Artificial (RNA) es un modelo com-
putacional inspirado en la estructura y funcionamiento del
cerebro humano, compuesto por nodos (neuronas artificiales)
organizados en capas y conectados entre si mediante pesos.
Estas redes aprenden patrones complejos a partir de datos
de entrada a traves de un proceso iterativo de ajuste de
pesos, permitiendo resolver tareas de clasificacion, prediccion
y reconocimiento en diversos dominios [2].
A. Clasificador de MNIST
MNIST es un dataset con 60k muestras de numeros del 0 al
9 en un solo canal. En el resumen de la clase se da a entender
que se esta intentando desarrollar un clasificador utilizando
este dataset. Que cada una de estas imagenes esta compuesta
por un grupo de pixeles.
B. Regresion Logistica
En el resumen de la clase anterior se menciona que para el
clasificador de MNIST se comenzaba tratando de hacer una
clasificacion binaria respecto a la imagen. Hasta el momento
de esta clase elunico algoritmo conocido para desarrollar esta
clasificacion es la regresion logistica. Para esto se pasan la
informacion de cada uno de los pixeles de la imagen como
entrada para le regresion logistica. La situacion es que el
problema no puede ser resuelto con una regresion logisticaunicamente, si no con una regresion logistica multinomial, ya
que requerimos 10 clases y la regresion logistica solo permite
1.
C. Multinomial
Para esto, al problema requerir 10 clases, se desarrollan
10 regresiones logisticas, una por cada clase y a cada una
se le pasa como entrada la informacion de los pixeles de
la imagen, por lo que una de las regresiones logisticas dara
mayor probabilidad que las demas. Y en este punto tenemos
en la Figura 1 una arquitectura que ya podria llamarse red
neuronal, aunque todavia faltaria agregar una siguiente capa
para poder resolver problemas no lineales.
Tambien se menciona en el resumen de la clase anterior
que en lugar de calcular cada regresion lineal de forma
vectorial, cambiamos los vectores por matrices para hacer 1
sola operacion y no N. Donde N es el tamaño de la capa
siguiente utilizando conceptos dealgebra lineal. En cada una
de las filas sera representado las neuronas para la siguiente
capa y las entradas las columnas.

2
Fig. 1: Primer Red Neuronal.
El resumen de la clase anterior concluye definiendo algunas
caracteristicas de las redes neuronales, las cuales son que al
no ser lineales nos permite atacar problemas complejos, esta
compuesta por capas, estas capas son el hiper parametro de
la red neuronal y es importante que sean diferenciables. Si la
red neuronal se puede derivar se puede optimizar y que en
cada capa hay neuronas.
IV. E LPERCEPTRON
El perceptron es uno de los modelos mas simples de red
neuronal artificial, propuesto por Frank Rosenblatt en 1958.
Consiste en una unidad de procesamiento que recibe un con-
junto de entradas ponderadas, las combina linealmente y aplica
una funcion de activacion para producir una salida binaria. Su
objetivo principal es clasificar patrones linealmente separables.
Aunque limitado para problemas no lineales, constituye la base
conceptual de arquitecturas mas complejas como el perceptron
multicapa y las redes neuronales profundas [3].
A. Invierno de la AI
El invierno de la inteligencia artificial hace referencia a
periodos historicos en los que las expectativas generadas
alrededor de la investigacion en IA no se cumplieron, provo-
cando una disminucion drastica en la financiacion, el interes
academico y el desarrollo industrial en este campo. Durante
estos periodos, los avances en IA se ralentizaron debido
a limitaciones tecnologicas, falta de resultados practicos y
criticas hacia la viabilidad de los enfoques predominantes. Se
reconocen principalmente dos inviernos de la IA: el primero
a mediados de los años 1970, y el segundo a finales de los
años 1980 hasta principios de los 1990 [4].
En 1969, Marvin Minsky y Seymour Papert publicaron el
libro Perceptrons, en el que señalaron limitaciones fundamen-
tales del perceptron simple. Entre los problemas destacados,
Fig. 2: Inspiracion biologica de la red neuronal.
señalaron que este modelo no podia resolver funciones no
linealmente separables, siendo el ejemplo clasico la funcion
logica XOR. Ademas, advirtieron sobre su limitada expresivi-
dad computacional y su escasa capacidad para generalizar en
problemas mas complejos, lo que contradecia las expectativas
iniciales de que los perceptrones pudieran resolver tareas
de vision y reconocimiento de patrones. Estas observaciones
demostraron que, aunque los perceptrones eranutiles para
ciertos problemas lineales, su aplicacion practica era muy
limitada. El impacto de estas criticas fue significativo, con-
tribuyendo al primer invierno de la inteligencia artificial, hasta
que el desarrollo del perceptron multicapa y el algoritmo de
retropropagacion permitieron superar estas restricciones [5].
B. Inspiracion Biologica
Las redes neuronales artificiales se inspiran en el fun-
cionamiento de las neuronas del cerebro humano, donde cada
neurona recibe señales de multiples conexiones sinapticas, las
procesa y genera una respuesta que se transmite a otras neu-
ronas. De manera analoga, en las redes neuronales artificiales,
cada nodo o “neurona” recibe entradas ponderadas, aplica una
funcion de activacion y transmite su salida a las siguientes
capas, reproduciendo de forma simplificada el procesamiento
distribuido y paralelo del sistema nervioso biologico. Esta
inspiracion biologica se ilustra en la Figura 2, donde se
muestra la correspondencia entre una neurona biologica y su
modelo artificial.
C. Funcion de Activacion
En regresion logistica se llama funcion no-lineal (sigmoid).
Esta depende de si la señal activa o no la neurona. Dependi-
endo de la intensidad de la señal que se haya recibido, esta
dejara pasar la informacion, la bloqueara o la transformara y
existen varias funciones de activacion.
1) Funcion Sigmoide: La funcion sigmoide transforma un
valor de entrada en un rango entre 0 y 1, lo que permite
interpretarla como una probabilidad. Su desventaja principal es
la saturacion de gradientes en valores extremos, lo que dificulta
el entrenamiento en redes profundas [6].
σ(x) =1
1 +e−x

3
2) Funcion Tangente Hiperbolica (tanh): La tangente
hiperbolica es similar a la sigmoide, pero su rango va de
-1 a 1, lo que permite que las salidas esten centradas en
cero. Esto ayuda a mitigar algunos problemas de gradientes
en comparacion con la sigmoide, aunque aun puede sufrir de
saturacion [6].
tanh( x) =ex−e−x
ex+e−x
3) Funcion ReLU (Rectified Linear Unit): La funcion
ReLU es una de las mas utilizadas en redes neuronales
modernas. Define la salida como 0 para valores negativos
y como la propia entrada para valores positivos. Es com-
putacionalmente eficiente y mitiga en gran parte el problema
del desvanecimiento del gradiente, aunque puede presentar el
problema de “neurona muerta” [7].
f(x) = max(0 , x)
4) Funcion Leaky ReLU: La funcion Leaky ReLU es una
variante de la ReLU que permite pequeños valores negativos
en la salida (usualmente multiplicados por una constante
pequeña, como 0.01). Esto evita el problema de neuronas
muertas al asegurar un gradiente no nulo para entradas nega-
tivas [8].
f(x) =(
x six≥0
αx six <0
5) Funcion Softmax: La funcion Softmax convierte un
vector de valores reales en una distribucion de probabilidad,
donde cada valor queda entre 0 y 1 y la suma total es igual a 1.
Se utiliza principalmente en la capa de salida de clasificadores
multiclase [6].
σ(z)i=ezi
PK
j=1ezjparai= 1, . . . , K
V. P ERCEPTRONMULTICAPA
El Perceptron Multicapa (MLP, por sus siglas en ingles) es
una arquitectura fundamental dentro de las redes neuronales
artificiales. Esta compuesto por una capa de entrada, una o
mas capas ocultas y una capa de salida. A diferencia del
perceptron simple, que solo puede resolver problemas lineal-
mente separables, el MLP utiliza funciones de activacion no
lineales en sus neuronas ocultas, lo que le permite aproximar
funciones complejas y resolver problemas no lineales. Su
entrenamiento se realiza comunmente mediante el algoritmo
de retropropagacion (backpropagation), el cual ajusta los pesos
de las conexiones minimizando el error entre la salida predicha
y la deseada. Esta arquitectura constituye la base de los
modelos modernos de aprendizaje profundo [6], [9].
A. Maldicion de dimensionalidad
A mayor cantidad de dimensiones, aumenta la complejidad,
a su vez, aumentando la computabilidad y se vuelve mas
complicado encontrar patrones. Para esto existen algoritmos
de deduccion de dimensiones como el PCA.1) PCA: El Analisis de Componentes Principales (PCA,
por sus siglas en ingles: Principal Component Analysis) es un
metodo estadistico ampliamente utilizado para la reduccion
de dimensionalidad, que transforma un conjunto de vari-
ables posiblemente correlacionadas en un nuevo conjunto
de variables no correlacionadas denominadas componentes
principales. El procedimiento consiste en centrar los datos,
calcular la matriz de covarianza, obtener sus autovalores
y autovectores, y seleccionar los vectores asociados a los
mayores autovalores para proyectar los datos en un subespacio
de menor dimension que conserva la mayor varianza posible
de la informacion original [10], [11], [12].
B. Comportamiento Jerarquico
Los humanos aprenden cosas simples para transformarlo en
algo mas complejo, tal es el caso del MLP conformado por
multiples regresiones lineales, de lo cual se optienen ganan-
cias exponenciales en algunas funciones, como polinomios,
la composicion de funciones que permite reusar funciones
simples otras de orden superior y que mediante una repre-
sentacion compacta, en la que pocos pesos se pueden modelar
funciones complejas, como por ejemplo, una red neuronal que
se aproxime a otra.
VI. C ONCLUSION
La clase permitio la comprension de los fundamentos de las
redes neuronales, resaltando su estructura jerarquica al final
y las motivaciones biologicas que inspiran su arquitectura.
A partir del analisis del perceptron y de sus limitaciones,
se introdujo la necesidad de arquitecturas mas complejas,
como el MLP, que posibilitan la resolucion de problemas
no lineales. Esta sesion trato tanto el potencial como los
desafios de las redes neuronales, entre ellos la maldicion de
la dimensionalidad y la importancia de un diseño acorde al
problema en cuestion en terminos de capas y neuronas. Asi, la
clase proporciono las bases para comprender las arquitecturas
modernas de aprendizaje profundo.
REFERENCES
[1] K. Wiggers, “Skild ai emerges from stealth with
$300m to build a general-purpose ai brain for robots,”
TechCrunch , Sep. 2025, accessed: 2025-10-02. [Online]. Avail-
able: https://techcrunch.com/2025/09/16/skild-ai-emerges-from-stealth-
with-300m-to-build-a-general-purpose-ai-brain-for-robots/
[2] S. Haykin, Neural Networks and Learning Machines , 3rd ed. Prentice
Hall, 2009.
[3] F. Rosenblatt, “The perceptron: A probabilistic model for information
storage and organization in the brain,” Psychological Review , vol. 65,
no. 6, pp. 386–408, 1958.
[4] S. J. Russell and P. Norvig, Artificial Intelligence: A Modern Approach ,
3rd ed. Prentice Hall, 2010.
[5] M. Minsky and S. A. Papert, Perceptrons: An Introduction to Compu-
tational Geometry . MIT Press, 1969.
[6] I. Goodfellow, Y . Bengio, and A. Courville, Deep Learning . MIT Press,
2016.
[7] V . Nair and G. E. Hinton, “Rectified linear units improve restricted boltz-
mann machines,” in Proceedings of the 27th International Conference
on Machine Learning (ICML) , 2010.
[8] A. L. Maas, A. Y . Hannun, and A. Y . Ng, “Rectifier nonlinearities
improve neural network acoustic models,” in Proceedings of the 30th
International Conference on Machine Learning (ICML) , 2013.

4
[9] D. E. Rumelhart, G. E. Hinton, and R. J. Williams, “Learning repre-
sentations by back-propagating errors,” Nature , vol. 323, no. 6088, pp.
533–536, 1986.
[10] I. T. Jolliffe and J. Cadima, “Principal component analysis: a review and
recent developments,” Philosophical Transactions of the Royal Society
A: Mathematical, Physical and Engineering Sciences , vol. 374, no. 2065,
p. 20150202, 2016.
[11] C. M. Bishop, Pattern Recognition and Machine Learning . Springer,
2006.
[12] J. Shlens, “A tutorial on principal component analysis,” arXiv preprint
arXiv:1404.1100 , 2014.