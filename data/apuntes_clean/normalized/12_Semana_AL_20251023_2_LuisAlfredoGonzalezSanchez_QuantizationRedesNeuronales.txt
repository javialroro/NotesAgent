NOTAS DE CLASE
INTELIGENCIA ARTIFICIAL - 23 DE OCTUBRE - SEMANA 12
Luis Alfredo Gonzalez Sanchez
Escuela de Ingenieria en Computacion
Instituto Tecnologico de Costa Rica
Cartago, Costa Rica
2021024482 gonzal3z.luis@estudiantec.cr
Abstract —Neural network quantization is a vital technique in
AI that reduces model size and computational cost by converting
weights and activations from floating-point to lower-precision
formats, such as integers. This process enables deployment on
resource-constrained devices, like mobile or embedded systems,
while maintaining high accuracy. Different methods include
symmetric and asymmetric quantization, with strategies tailored
to specific data distributions and hardware constraints. Dy-
namic, granular, and post-training quantization further refine
this approach by adjusting intervals per layer, per sample, or
after training, respectively. These techniques involve calculating
scaling factors and zero points to effectively map high-precision
values to lower-bit representations, introducing minimal accuracy
loss. Overall, quantization enhances efficiency, reduces power
consumption, and facilitates real-time AI applications, making
it a cornerstone of practical deep learning deployment.
Index Terms —Quantization in neural networks,model com-
pression,QAT quantization techniques,integer representation
I. I NTRODUCTION
La cuantizacion en redes neuronales es una tecnica esencial
para mejorar la eficiencia del computo y reducir el tamaño
de los modelos, principalmente transformando los datos de
punto flotante a formatos de menor precision, como enteros.
Esta transformacion permite que los modelos se ejecuten de
manera mas rapida y con menor consumo de memoria, lo
cual es fundamental para desplegar inteligencia artificial en
dispositivos con recursos limitados, como moviles y sistemas
embebidos. En el presente documento, se busca resumir la
informacion vista en la clase del 23 de octubre,donde se ha
revisado como diferentes metodos de cuantizacion —desde la
simetrica y asimetrica hasta la dinamica, granulada y post-
entrenamiento— manejan la conversion de pesos, activaciones
y sesgos, optimizando el balance entre precision y eficiencia.
Ademas, se menciona como tecnicas como la cuantizacion
consciente durante el entrenamiento (QAT) ayudan a mantener
la precision del modelo al considerar el efecto de la cuanti-
zacion desde el inicio del aprendizaje.
II. B REVE DEFINICION DE ONNIX
Para continuar el tema de quantization en supervised learn-
ing , es importante entender la herramienta onnix, suponga
un modelo llm ya entrenado¿Como empieza a funcionar el
producto o sistema? La herramienta Onnix permite representarmodelos de aprendizaje automatico desarrollados en distin-
tos frameworks como PyTorch o TensorFlow en una repre-
sentacion intermedia estandar y eficiente. Esta representacion
facilita la interoperabilidad y el despliegue de modelos en
diferentes plataformas y hardware mediante optimizaciones
en C++ u otros lenguajes, asegurando que el mismo modelo
pueda ejecutarse con alto rendimiento en entornos variados.
Considerando lo anterior, las plataformas poseen diversas lim-
itaciones y rendimiento tanto en software como en hardware, si
se entrenan modelos grandes, posiblemente un celular no este
adaptado para soportar dicho modelo, para ello se observara
el concepto de quantization.
III. Q UANTIZATION
Suponga que se tiene un modelo de deep learning con
muchas capas, por ejemplo , llama 2 , con 70 mil millones
de parametros aproximadamente, si cada parametro es de 32
bits, se obtiene un tamaño aproximado de 28 gb para solo
almacenar el modelo, ¿Como podriamos cargarlo a memoria?
Una alternativa es comprar una GPU con dicho tamaño para
el procesamiento del modelo, pero GPUs que soporten esos
tamaños son costosas , lo que se busca es reducir el tamaño
del modelo, una de sus tecnicas es quantization
A. definicion
Quantization es una tecnica de compresion de modelos de
aprendizaje automatico que reduce el numero de bits utilizados
para representar los parametros del modelo, transformando
los valores de punto flotante a representaciones de menor
precision, generalmente enteros de 8, 5, 2 o incluso 1 bit.
Lo que se busca es disminuir el tamaño y la complejidad
computacional del modelo, manteniendo una precision cercana
al original. No se debe de confundir como una tecnica de
redondear pesos, sino de convertir y ajustar los tipos de datos
para optimizar el balance entre tamaño, velocidad de inferencia
y precision. Se busca un tradeoff optimo entre capacidades del
modelo vs rendimiento.
B. ventajas
•menor consumo de memoria al cargar los modelos en
memoria
•Permite insertar el modelo en sistemas con recursos
limitados / con proposito especifico, como celulares o
embebidos

Fig. 1. Partes de un numero punto flotante
•Genera un menor tiempo para hacer las inferencias, sus
datos son mas simples
•Menor consumo energia debido a menor complejidad de
computacion
IV. B REVE REPASO A LAS OPERACIONES CON BITS
Se dara un breve repaso a la manipulacion de bits en
sistemas computacionales para entender mejor el proceso de
quantization
Con2nbits se pueden representar 2nvalores distintos. Esto
significa que, por ejemplo, con 3 bits es posible representar
23= 8 numeros diferentes.
Un ejemplo basico que se vio en clase es de conversion de
binario a decimal es el numero 6, que en binario se escribe
como 110. La conversion se realiza sumando las potencias de
2 correspondientes a los bits activos (1) segun su posicion:
6 = 1 ×22+ 1×21+ 0×20= 4 + 2 + 0 = 6
Cada digito binario representa una potencia de 2, comen-
zando desde la derecha con la potencia 0.
Los numeros enteros en sistemas digitales se representan
normalmente en complemento a 2, donde el bit mas significa-
tivo indica el signo: 0 para positivo y 1 para negativo. Esto
facilita realizar operaciones aritmeticas con numeros negativos
usando operaciones binarias estandar.
Para numeros en punto flotante, la representacion se divide
en tres partes: signo, exponente y mantisa (fraccion). El valor
decimal se calcula aproximadamente como:
valor = (−1)signo×(1 + mantisa )×2exponente −bias
Esta tecnica de representacion permite expresar un amplio
rango de numeros reales con precision limitada y eficiencia en
almacenamiento mediante manipulacion de bits.
Observe la figura 1 donde se puede observar las partes
del numero flotante de 32 bits Ahora bien , considerando las
partes del numero punto flotante, es importante detallar que
La precision dada por la mantiza se va a disminuir con
quantization .
V. P ROCEDIMIENTO DE QUANTIZATION EN MODELOS DE
REDES NEURONALES
Pasos generales del procedimiento:
•Transformacion de pesos: Los pesos de la red, original-
mente en formato de punto flotante (float), se convierten
a valores enteros mediante mapas de cuantizacion queasignan rangos de valores flotantes a niveles discretos
enteros.
•Cuantizacion de entradas: Las entradas a cada capa
tambien se convierten a enteros para mantener la coheren-
cia en la representacion y facilitar operaciones eficientes.
•Cuantizacion del sesgo (bias): Los terminos de sesgo,
que son sumados en cada neurona, se transforman de
float.
•Normalizacion del rango: Se definen valores maximos y
minimos para pesos, entradas y sesgos, que corresponden
a los valores limite de la representacion entera (por
ejemplo, el rango de int8). Esto asegura que los valores
cuantizados esten dentro de rangos representables.
•Calculo en espacio entero: Las operaciones de la capa
(multiplicacion y suma) se realizan en enteros, generando
un vector cuantizado.
•Des-cuantizacion:Despues de la capa, los valores en-
teros se convierten nuevamente a punto flotante para
continuar con el procesamiento de modo que las capas
siguientes no requieren conocer el esquema de cuanti-
zacion aplicado.
Durante la dequantization es donde puede ocurrir perdida de
precision, ya que la conversion entre representaciones intro-
duce aproximaciones. Sin embargo, el objetivo es que la salida
cuantizada sea lo suficientemente cercana a la original para no
afectar el rendimiento del modelo. Este proceso es beneficioso
ya que permite que modelos originalmente pesados funcionen
eficientemente con menor consumo de memoria y tiempo
de computo, esencial sistemas embebidos o con capacidad
limitada. Es importante aclara que las capas que siguen a una
capa cuantizada generalmente no requieren modificaciones ni
conocen directamente la cuantizacion aplicada, manteniendo
transparencia en la mayoria de frameworks.
VI. T IPOS DE QUANTIZATION
Los tipos de consonantizan son los siguientes:
•Quantization simetrica: Mapea valores positivos y nega-
tivos de un rango minimo a maximo que incluye el cero.
Aqui, el valor cero real se mapea exactamente a cero
entero. Esto simplifica el manejo de pesos y activaciones
con signo, aplicando la misma escala en ambos lados del
cero.
•Quantization asimetrica: Mapea valores entre 0 y un
valor maximo entero, pero el valor minimo real no se
mapea a cero, sino a un valor entero llamado zero point
(Z), que representa el valor neutro o ”offset” de la
cuantizacion. Esto permite representar valores con un de-
splazamiento, ´util cuando los valores no estan centrados
en cero.
Las formulas para la cuantizacion simetrica y asimetricas se
describen a continuacion : se toma un rango [b,a] y se mapea
a un rango de salida, se calcula el parametro de escalado y
por ultimo se calcula el numero neutro del mapeo.
•xq=clamp xf
s
+z; 0; 2n−1
–xf=Valor flotante

Fig. 2. Tipos de quantizaton : simetrica vs asimetrica
•Parametro de escalado s:
s=α−β
2n−1
2n−1 =El rango de salida
•Parametro neutro z:
z=
−1·β
s
•nes el numero de bits.
y su respectiva des-cuantizacion :
xf=s×(xq−z)
Para la cuantizacion simetrica :
•xq=clamp xf
s
;− 
2n−1−1
; 2n−1−1
•Parametro de escalado s:
s=abs(α)
2n−1−1
•nes el numero de bits.
y su respectiva descuantizacion se brinda por la siguiente
formula
Xf=S×Xq
VII. O TROS TIPOS DE CUANTIZACIONES
Cuantizacion dinamica: La cuantizacion dinamica se en-
foca en cuantizar las activaciones de las neuronas seleccio-
nando apropiadamente los valores minimos ( a) y maximos ( b)
para el mapeo de cuantizacion de cada tensor. La estrategia
de seleccion del intervalo a, bes la siguiente:
•Para cuantizacionasimetrica , se seleccionan los valores
extremos reales del tensor, es decir, byacorresponden
al maximo y minimo del tensor respectivamente.
•Para cuantizacionsimetrica , se toma el mayor valor en
terminos absolutos y se define el intervalo como [−a, a],
centrado en cero.
•Esta tecnica puede inducir un mayor error debido a la
sensibilidad a valores atipicos (outliers). Una solucion es
utilizar percentiles basados en la distribucion del tensor,
excluyendo los outliers y reduciendo el error cuadratico
medio (MSE).
Cuantizacion granulada en convoluciones:•Las convoluciones se realizan con multiples filtros que
aprenden valores y distribuciones distintas.
•Cada filtro detecta diferentes caracteristicas (features) de
la imagen.
•No es posible aplicar el mismo intervalo a, b para to-
dos los filtros, por lo que se calcula un intervalo a, b
especifico para cada filtro respetando su distribucion.
Quantization post-training:
•Se realiza despues del entrenamiento, utilizando datos
nunca antes vistos por el modelo.
•Introduce un componente llamado observer , que obtiene
estadisticas de cada capa para calibrar las salidas y
calcular los parametros de cuantizacion como la escala
(s) y punto cero ( z).
•Permite cuantizar el modelo sin necesidad de re-
entrenamiento completo.
Quantization aware training (QAT):
•Metodo avanzado donde la cuantizacion se simula durante
el entrenamiento.
•El modelo aprende a compensar la perdida de precision
por la cuantizacion al utilizar la funcion de perdida para
actualizar los pesos que constantemente sufren de este
efecto.
•Mejora el rendimiento en modelos cuantizados para en-
tornos de baja precision.
VIII. C ONCLUSION
La informacion presentada demostro la importancia para
la optimizacion del uso de recursos en modelos de redes
neuronales, especialmente si se desea implementar en sistemas
embebidos o con recursos limitados, en esta clase se aprendio
que : tex
•La cuantizacion consiste en transformar pesos, activa-
ciones y sesgos de punto flotante a representaciones de
menor precision, principalmente enteros, con el fin de
reducir tamaño y acelerar la inferencia.
•Existen distintos tipos de cuantizacion: simetrica ,
asimetrica , dinamica , granulada y post-
entrenamiento , cada una con estrategias especificas para
mapear y convertir datos.