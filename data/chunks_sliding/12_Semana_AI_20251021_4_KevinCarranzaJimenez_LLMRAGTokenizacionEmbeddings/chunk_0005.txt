como secuencias de numeros. Existen diversas estrategias de tokenizacion, como la basada en subpalabras (Byte Pair EncodingoWordPiece), que buscan equilibrar la eficiencia del vocabulario con la capacidad del modelo para manejar palabras desconocidas o de diferentes idiomas [5]. La Tabla II muestra un ejemplo simplificado del proceso de tokenizacion, en el cual cada palabra del texto es descompuesta en su correspondiente token y asociada a un identificador numerico dentro del vocabulario del modelo. Este procedi- miento permite representar de forma estructurada los elemen- tos linguisticos, facilitando que el modelo procese el texto como una secuencia de valores discretos que posteriormente seran transformados en vectores continuos mediante tecnicas deembedding. La Tabla III resume algunos de los tipos mas comunes de tokenizacion utilizados en modelos de lenguaje. Cada enfoque difiere en el nivel de granularidad con que divide el texto: desde unidades completas como palabras, hasta fragmentos mas pequenos como subpalabras, caracteres o incluso bytes individuales. Esta diversidad de metodos permite adaptar la representacion del texto segun las necesidades del modelo, equilibrando la complejidad del vocabulario con la capacidad para manejar palabras desconocidas o simbolos especiales. Cuadro III TIPOS COMUNES DE TOKENIZACION Tipo Ejemplo Ventaja principal Palabra "Los modelos" Simplicidad Caracter "L", .o", "s" Sin OOV* Subpalabra .aprend-iendo" Equilibrio vocabula- rio/contexto Byte-level bytes UTF-8 Soporta cualquier simbolo Espacio en blanco "Hola", "mundo" Rapido y simple Figura 2. 3D Semantic feature space III-C. Representacion de tokens en un espacio vectorial Una vez que