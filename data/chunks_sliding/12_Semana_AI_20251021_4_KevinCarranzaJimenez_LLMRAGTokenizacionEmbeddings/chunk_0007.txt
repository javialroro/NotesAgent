funcion de sus caracteristicas aprendidas por el modelo, lo que permite observar relaciones de similitud y diferencia entre concep- tos. Por ejemplo, terminos como “rey” y “reina” se ubican proximos entre sien la dimension de realeza, pero difieren en la dimension de genero, ilustrando como losembeddings capturan relaciones semanticas complejas dentro de un espacio continuo [6]. III-D. Similaridad entre vectores Una vez que las palabras han sido transformadas en vectores dentro de un espacio continuo, es posible cuantificar su grado de similitud midiendo la distancia o el angulo entre dichos vectores. En este contexto, dos vectores proximos representan palabras con significados semanticamente similares, mientras que aquellos que se encuentran alejados reflejan conceptos distintos o no relacionados. Esta propiedad permite a los mo- delos de lenguaje capturar relaciones latentes como analogias o asociaciones conceptuales, lo que ha sido fundamental para tareas como la busqueda semantica, la traduccion automatica y la inferencia contextual [7]. 3 III-E. Metricas mas comunes Las metricas mas comunes para calcular similitud entre vectores son: Distancia euclidiana: d(a, b) =sX i(ai−b i)2 (1) Mide que tan lejos estan los puntos. Similitud del coseno sim(a, b) =a·b ∥a∥∥b∥(2) Mide el angulo entre vectores: cuanto mas pequeno, mas similares. La mas usada en modelos de lenguaje es la similitud de coseno, ya que se enfoca en la direccion del vector mas que en su magnitud. IV. EMBEDDINGS Losembeddingsson representaciones numericas densas que asignan a cada token —ya sea una palabra,