entre vectores: cuanto mas pequeno, mas similares. La mas usada en modelos de lenguaje es la similitud de coseno, ya que se enfoca en la direccion del vector mas que en su magnitud. IV. EMBEDDINGS Losembeddingsson representaciones numericas densas que asignan a cada token —ya sea una palabra, subpalabra o inclu- so una frase— un vector en un espacio continuo de alta dimen- sion. Estas representaciones permiten capturar el significado semantico y las relaciones contextuales entre los terminos, de modo que palabras con sentidos similares se ubiquen proximas entre sidentro del espacio vectorial. Ademas, los modelos modernos son capaces de generarembeddingsa nivel de frase o enunciado (sentence embeddings), los cuales condensan el significado global de un texto. Este tipo de representacion posibilita comparar oraciones, ideas o documentos en funcion de su contenido semantico, en lugar de basarse unicamente en coincidencias literales de palabras [8]. La Figura 3 representa un ejemplo conceptual de embed- dings para frases similares en el que se muestran las diferentes fases de forma general y simplificada, pasando des de la palabra, documento u oracion, hasta el espacio del embedding. IV-A. Capacidad de los modelos de lenguaje Gracias a su entrenamiento a gran escala y al uso de arquitecturas basadas entransformers, los modelos de len- guaje de gran escala (LLM) han desarrollado un conjunto de capacidades emergentes que trascienden las funciones para las que fueron disenados explicitamente. Estas habilidades Figura 3. Ejemplo conceptual de embeddings de frases