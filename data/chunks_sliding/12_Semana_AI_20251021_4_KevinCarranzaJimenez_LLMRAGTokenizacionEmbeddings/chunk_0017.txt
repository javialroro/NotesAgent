E. Perez, A. Piktus, F. Petroni, V . Karpukhin, N. Goyal, H. Kuttler, M. Lewis, W. tau Yih, T. Rocktaschel, S. Riedel, and D. Kiela, “Retrieval-augmented generation for knowledge-intensive nlp tasks,”Advances in Neural Information Processing Systems (NeurIPS), 2020. [2] S. Wang, Y . Qin, W. Chen, Z. Wu, Z. Xi, Y . Xu, T. Gui, X. Qiu, and Z. Zhang, “A survey on large language model based autonomous agents,” arXiv preprint arXiv:2401.03428, 2024. [3] OpenAI, “Gpt-4 technical report,” arXiv preprint arXiv:2303.08774, 2023. [4] Y . LeCun, Y . Bengio, and G. Hinton, “Deep learning,”Nature, vol. 521, pp. 436–444, 2015. [5] R. Sennrich, B. Haddow, and A. Birch, “Neural machine translation of rare words with subword units,” inProceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL), 2016, pp. 1715–1725. [6] T. Mikolov, K. Chen, G. Corrado, and J. Dean, “Efficient estimation of word representations in vector space,” inProceedings of the Internatio- nal Conference on Learning Representations (ICLR), 2013. [7] J. Pennington, R. Socher, and C. D. Manning, “Glove: Global vectors for word representation,” inProceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2014, pp. 1532–1543. [8] N. Reimers and I. Gurevych, “Sentence-bert: Sentence embeddings using siamese bert-networks,” inProceedings of the 2019 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2019, pp. 3982–3992. [9] J. Wei, Y . Tay, R. Bommasani, C. Raffel, B. Zoph, S. Borgeaud, D. Yogatama, M. Bosma, D. Zhou,