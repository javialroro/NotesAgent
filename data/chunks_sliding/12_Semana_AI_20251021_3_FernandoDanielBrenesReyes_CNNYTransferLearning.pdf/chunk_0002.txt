estas barreras, se han desarrollado enfoques como Retrieval-Augmented Generation (RAG) y los Agentes Inteligentes. II. FUNDAMENTOS DELLMS YREPRESENTACION II-A. Tokenizacion: De la Palabra al Numero Para que los LLMs puedan computar con el lenguaje, el texto de entrada debe convertirse en una representacion numerica. El proceso deTokenizaciontransforma palabras, signos o simbolos en unidades minimas llamadastokens, asignando a cada una unID numericounico. Existen multiples estrategias de tokenizacion, cada una optimizada para un objetivo distinto: Por palabra: Ofrece simplicidad. Por caracter: Permite manejar simbolos o palabras fuera del vocabulario (OOV). Subpalabra (BPE, WordPiece): Logra un equilibrio optimo entre el tamano del vocabulario y la preservacion del contexto.II-B. Embeddings y Espacios Vectoriales Una vez tokenizados, los IDs numericos se convierten en embeddings, que son representaciones numericas densas en un espacio continuo de alta dimension. Captura semantica: Los embeddings capturan el sig- nificado y las relaciones contextuales entre palabras u oraciones completas. Proximidad: Las palabras con significados similares se ubican proximas en el espacio vectorial. Operaciones: Este espacio permite realizar operacio- nes semanticas, como analogias (por ejemplo,Rey− Hombre+Mujer≈Reina). Para medir lasimilitudentre dos vectoresaybenRn, la Similitud del Cosenoes la metrica mas utilizada: sim(a,b) =a·b ||a||||b||(1) Figura 1. Representacion tridimensional de tokens (Realeza). II-C. Capacidades Emergentes El entrenamiento masivo de los LLMs les confiere capa- cidades avanzadas queemergensin haber sido entrenados directamente para ellas: Razonamiento y planificacion. Aprendizaje en el prompt(In-context Learning): Adap- tan el comportamiento a partir de ejemplos dados en la entrada. Multitarea: Realizan