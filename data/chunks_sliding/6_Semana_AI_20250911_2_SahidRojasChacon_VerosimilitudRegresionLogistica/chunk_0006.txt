3.cap03:Flujo completo: imports, generacion del dataset con make_classification, division entrenamiento/prueba, entrenamiento del modelo propio y bloque analogo consklearn. VIII-C. Bloque principal: imports, dataset y flujo del entre- namiento (cap03) Como se muestra en la Figura 3, se realiza la im- portacion de librerias necesarias (numpyy modulos de sklearn). Luego, se crea undataset de clasificacioncon make_classification, que genera datos sinteticos con- trolados para problemas binarios. En este caso, se indica: n_samples= 1000,n_features= 2,n_informative= 2,n_redundant= 0,n_clusters_per_class= 1,random_state= 225. Esto produce dosfeaturesinformativas sin redundancia y un solo cluster por clase, coherente con los ejemplos del curso. A continuacion, se realiza ladivision entrenamien- to/pruebacontrain_test_split, manteniendo la proporcion de clases mediantestratify=yy fijando random_statepara reproducibilidad. Seguidamente, se instancia y entrenael modelo implementado desde cero: modelo_manual = LogisticRegressionAI(lr=0.001, epochs=6000), ejecutando el bucle explicado en la subseccion anterior (for- ward, gradientes y actualizacion). Finalmente, el bloque in- cluyeLogisticRegressiondesklearnparareplicar el mismo enfoquecon la libreria estandar, lo que sirve como referencia y valida que la implementacion manual respeta la teoria. VIII-D. Resumen codigo↔teoria Verosimilitud→log-loss.El codigo minimiza (16), que es −logL; asi, “minimizar la perdida” equivale a “maximizar la verosimilitud”. Regla de la cadena.La rutax→z→a→Lda∂L ∂z=a−y, base de los gradientes vectorizadosdwydb. Parametros actualizables.Solowybcambian; el resto (sig- moide, datos) son transformaciones/entradas fijas conforme a la formulacion del modelo. IX. CONCLUSIONES La clave para no perderse es mirar la composicionx→z→ a→Ly empujar las derivadas con la regla de la cadena. El uso de logaritmos cambia productos por sumas y, al negar el log-likelihood,