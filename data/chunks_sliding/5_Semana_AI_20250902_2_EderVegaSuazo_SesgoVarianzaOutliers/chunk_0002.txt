∂L ∂b=1 NNX i=12((wx i+b)−y i) I-B. Algoritmo de descenso de gradiente La actualizacion de parametros se realiza de forma iterativa mediante: w(t+1)=w(t)−α∂L ∂w(t) b(t+1)=b(t)−α∂L ∂b(t) dondeαrepresenta la tasa de aprendizaje que controla la magnitud de cada actualizacion. I-C. Terminologia fundamentalEpoca (Epoch): Ciclo completo de presentacion de todos los ejemplos de entrenamiento al modelo. Lote (Batch): Subconjunto de ejemplos utilizados para calcular una actualizacion de parametros. Tasa de aprendizaje: Hyperparametro que determina la velocidad de convergencia del algoritmo.II. DESAFIOS EN MODELADO PREDICTIVO II-A. Relaciones no lineales entre variables La regresion lineal presume una relacion lineal entre pre- dictores y variable respuesta. Cuando esta suposicion se viola, el modelo resulta inadecuado y muestra patrones sistematicos en los residuos: ei=yi−yi La solucion implica transformar las variables predictoras mediante expansion polinomial o otras transformaciones que permitan capturar relaciones no lineales manteniendo la linea- lidad en los parametros. Figura 1: Ejemplo de relacion no lineal y su ajuste mediante transformacion polinomial. II-B. Manejo de valores atipicos Las observaciones extremas pueden distorsionar significati- vamente los modelos de regresion. Existen multiples enfoques para su identificacion y tratamiento: II-B1. Identificacion de valores atipicos: Residuos estandarizados:z i=ei σedondeσ ees la desviacion estandar de los residuos. Rango intercuartilico: Valores fuera de[Q 1−1,5· IQR, Q 3+ 1,5·IQR]se consideran atipicos. II-B2. Tecnicas de tratamiento: Eliminacion: Remover observaciones identificadas como atipicas. Winsorizacion: Reemplazar valores extremos por per- centiles especificos (ej. percentil 5 y 95). Transformaciones: Aplicar funciones como logaritmo o raiz cuadrada