la salida estaen(0,1). 2.2 Ecuaciones Fundamentales de la Neurona Potencial de activacion: z=w⊤x+b. Funcion sigmoide: g(z) =1 1 +e−z. Salida del modelo: y=h(x) =g(w⊤x+b). Figura: diagrama esquematico de la neurona (Entradas→combinacion lineal→activacion→ salida) Figura 2: Diagrama esquematico que interpreta la regre- sion logistica como la neurona mas simple. 1 3Extension a la Clasificacion Multinomial y la Codificacion One-Hot 3.1Ejemplo de clase: 10 Regresiones Logisticas, una por alumno Para manejar las 10 clases se puede entrenar una regre- sion logistica por estudiante (una por clase); la capa de salida tendria 10 neuronas (una por clase). 3.2 Codificacion One-Hot de las Etiquetas (y) La etiqueta escalar se codifica como un vector one-hot enR10. Clase (digito) Vector One-Hot (y∈R10) Esperada 0[1,0,0,0,0,0,0,0,0,0]Neurona 0 2[0,0,1,0,0,0,0,0,0,0]Neurona 2 9[0,0,0,0,0,0,0,0,0,1]Neurona 9 Cuadro 1: Codificacion one-hot de etiquetas (ejemplos). 4 Compactacion por Algebra Lineal 4.1 Formulacion Matricial de Pesos y Sesgos Stackeando los vectores de pesos obtenemos la matriz de pesos y el vector de sesgos: W∈R10×784, b∈R10. La combinacion lineal de la capa de salida se escribe como: z=Wx+b, z∈R10. Elemento Simbolo Dimension Entradax784×1 Matriz de pesosW10×784 Sesgosb10×1 Potencial de activacionz10×1 Cuadro 2: Dimensiones en la formulacion matricial para MNIST. Figura 3: Matriz de pesos Wen la capa fully connected: cada fila corresponde a una neurona de salida y cada columna a un pixel de entrada.4.2Ejemplo Numerico de Clase: De Vector a Matriz V.B.1. Calculo de una sola regresion (vector de 4 features): w= 3 2 4 5