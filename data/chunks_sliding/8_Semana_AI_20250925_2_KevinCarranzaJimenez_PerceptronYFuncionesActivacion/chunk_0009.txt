entre 0 y 1 y la suma total es igual a 1. Se utiliza principalmente en la capa de salida de clasificadores multiclase [6]. Ïƒ(z)i=ezi PK j=1ezjparai= 1, . . . , K V. P ERCEPTRONMULTICAPA El Perceptron Multicapa (MLP, por sus siglas en ingles) es una arquitectura fundamental dentro de las redes neuronales artificiales. Esta compuesto por una capa de entrada, una o mas capas ocultas y una capa de salida. A diferencia del perceptron simple, que solo puede resolver problemas lineal- mente separables, el MLP utiliza funciones de activacion no lineales en sus neuronas ocultas, lo que le permite aproximar funciones complejas y resolver problemas no lineales. Su entrenamiento se realiza comunmente mediante el algoritmo de retropropagacion (backpropagation), el cual ajusta los pesos de las conexiones minimizando el error entre la salida predicha y la deseada. Esta arquitectura constituye la base de los modelos modernos de aprendizaje profundo [6], [9]. A. Maldicion de dimensionalidad A mayor cantidad de dimensiones, aumenta la complejidad, a su vez, aumentando la computabilidad y se vuelve mas complicado encontrar patrones. Para esto existen algoritmos de deduccion de dimensiones como el PCA.1) PCA: El Analisis de Componentes Principales (PCA, por sus siglas en ingles: Principal Component Analysis) es un metodo estadistico ampliamente utilizado para la reduccion de dimensionalidad, que transforma un conjunto de vari- ables posiblemente correlacionadas en un nuevo conjunto de variables no correlacionadas denominadas componentes principales. El procedimiento consiste en centrar los