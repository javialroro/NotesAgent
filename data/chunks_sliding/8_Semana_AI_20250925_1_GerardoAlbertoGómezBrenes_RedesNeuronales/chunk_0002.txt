de784 pixeles. La regresion logistica usa la transformacion lineal seguida de la sigmoide: z=wTx+b, σ(z) =1 1 +e−z. Para clasificacion binaria (ej.: “¿es 5 o no?”)σ(z)da una probabilidad entre 0 y 1. IV. DE BINARIO A MULTICLASE Y NOTACION MATRICIAL Para 10 clases se puede entrenar una regresion por clase o usar una salida vectorial. Notacion comun: Z=XWT+b, donde, por ejemplo,Wpuede tener forma10×784(10 neuronas de salida y 784 entradas). Con unbatchde tamano Bla entradaXesB×784y el resultadoZesB×10. Ejemplo numerico: con 10 salidas y 784 entradas hay10× 784 = 7 840parametros solo en esa capa.V. ONE-HOT,SOFTMAX Y DECISION Las etiquetas multi-clase se representan como vectoresone- hot. Para obtener una distribucion de probabilidad sobre clases se usasoftmax: softmax(z) i=exp(z i)P jexp(z j). La prediccion final corresponde al indice con mayor probabi- lidad. VI. ARQUITECTURA:CAPAS Y CONEXIONES Una red densa (fully connected / dense) conecta todas las salidas de una capa con todas las entradas de la siguiente. Anadir capas y activaciones no lineales permite resolver rela- ciones no lineales que un perceptron simple no puede (ejemplo clasico: XOR). VII. FUNCIONES DE ACTIVACION Y GRADIENTES Funciones mencionadas en clase: Sigmoide:σ(z) = 1/(1 +e−z). Derivada:σ′(z) = σ(z) (1−σ(z)). Tiene problemas devanishing gradient en extremos. ReLU:ReLU(z) = max(0, z). Es eficiente, pero puede generar neuronas “muertas” cuando la derivada es cero. Leaky ReLU: variante con pequena pendiente negativa para evitar neuronas muertas. tanh(z): acotada en(−1,1), util en algunos contextos. VIII. FORWARD,PERDIDA Y RETROPROPAGACION Elforwardcalcula salidas capa