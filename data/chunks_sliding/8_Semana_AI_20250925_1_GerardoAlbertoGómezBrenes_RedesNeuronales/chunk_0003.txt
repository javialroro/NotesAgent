(1−σ(z)). Tiene problemas devanishing gradient en extremos. ReLU:ReLU(z) = max(0, z). Es eficiente, pero puede generar neuronas “muertas” cuando la derivada es cero. Leaky ReLU: variante con pequena pendiente negativa para evitar neuronas muertas. tanh(z): acotada en(−1,1), util en algunos contextos. VIII. FORWARD,PERDIDA Y RETROPROPAGACION Elforwardcalcula salidas capa a capa. Con una funcion de perdidaLse aplica retropropagacion para obtener derivadas parciales∂L/∂wy actualizar parametros. Regla de actualiza- cion (descenso de gradiente): w←w−η∂L ∂w, dondeηes la tasa de aprendizaje. Para el perceptron se mencionoelhinge loss: Lhinge= max(0,1−y(wTx+b)). La retropropagacion usa la regla de la cadena para propagar sensibilidades hacia atras; por eso es necesario que las capas sean diferenciables. IX. COSTOS COMPUTACIONALES Y DIMENSIONALIDAD Aumentar neuronas y capas incrementa parametros y costo de optimizacion. Lamaldicion de la dimensionalidadcomplica la busqueda de soluciones optimas. Ejemplo: si una capa tiene 256 neuronas y la siguiente tiene 10, los pesos entre ellas son 10×256 = 2 560. Recomendacion practica: reducir dimensiones innecesarias (filtrado de features, PCA) cuando sea posible. X. NOTAS SOBRE REPRESENTACIONES Y CNN/EMBEDDINGS Para imagenes, las CNN aplican kernels que extraen pa- trones locales (bordes, texturas, formas). En lenguaje,embed- dingscondensan palabras o frases en vectores de dimension fija; la similitud semantica se mide por distancia en ese espacio vectorial.