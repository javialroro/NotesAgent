y esto nos asegura que vamos a tener un punto minimo. Y tambien es porque la funcion no MAE no es smooth, por lo que no nos va a permitir obtener las derivadas en todos los puntos, lo que induce a errores de calculo F . Derivadas Generales Regla Funcionf(x) Derivada f′(x) Constante k 0 Identidad x 1 Constante multiplicativa kx k Potencia xnnxn−1 Suma u(x) +v(x) u′(x) +v′(x) Producto u(x)v(x) u′(x)v(x) +u(x)v′(x) Constante sumada u(x) +z u′(x) Derivadas parciales f(x, y) = 2x+ 3y∂f ∂x= 2,∂f ∂y= 3 Cuadro I REPASO DE DERIVADAS BASICAS G. Descenso del gradiente El descenso del gradiente es un algoritmo iterativo de opti- mizacion para encontrar el minimo de una funcion. Funciona actualizando repetidamente los parametros en la direccion opuesta al gradiente de la funcion de costo. G1. Regla de actualizacion: xnuevo =xantiguo−α·(2x) G2. Importancia del α:Es el learning rate, debe ser pequeño para no pasarnos del punto minimo. Este es un hiperparametro Figura 2. Comparacion de distintos valores para alpha III. C ONTENIDO DE LA CLASE A. Funcion de Perdida y sus Derivadas Parciales Para optimizar los parametros wybde nuestro modelo, necesitamos actualizar sus valores de manera que la funcion de perdida se minimice. Para esto, evaluamos como cada parametro afecta la perdida utilizando derivadas parciales con respecto a wyb. Considerando la funcion de perdida basada en el error cuadratico medio (MSE) para nuestro modelo lineal fw,b(x) = wx+b, tenemos: L(w, b) =1