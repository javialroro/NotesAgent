1. I. Actualizacion de parametros Finalmente, los parametros se actualizan con descenso de gradiente: w=w−α∂L ∂w, b=b−α∂L ∂b. El valor deα(tasa de aprendizaje) es crucial. El flujo de calculo se muestra en la Figura 6. Fig. 6: Flujo de calculo y actualizacion de parametros J. Aspectos practicos •Epochs:numero de veces que el modelo recorre todo el dataset. Mas epochs permiten aprender mejor, pero tambien aumenta el riesgo de overfitting. •Batch size:cantidad de ejemplos procesados antes de actualizar parametros. Un batch pequeño hace el entre- namiento mas ruidoso pero puede mejorar la general- izacion. •Gradiente descendente estocastico (SGD):actualiza parametros con un ejemplo a la vez, lo que lo hace mas rapido pero inestable. V. CONCLUSIONES Durante esta semana se consolidaron los fundamentos de la regresion logistica. Se estudiaron sus bases matematicas, la funcion sigmoide y su derivada, la diferencia entre MSE y verosimilitud, el uso de logaritmos para simplificar expre- siones y la actualizacion de parametros mediante gradiente descendente. Los ejemplos practicos de la calabaza facilitaron la interpretacion de probabilidades, y la descomposicion paso a paso de derivadas mostro como se aplica la regla de la cadena en la practica. Con esto se sientan las bases para enfrentar algoritmos mas avanzados en aprendizaje supervisado.