una funcion de perdida Lapropiada para probabilidades. •Calculamos sus derivadas respecto a wyb. •Usamos esas derivadas en el algoritmo de descenso del gradiente , iterando sobre los datos de entrenamiento para ir actualizando los parametros y minimizar la perdida. C. Derivada de la funcion sigmoide σ(x) =1 1 +e−x σ′(x) =1′·(1 +e−x)−1·(1 +e−x)′ (1 +e−x)2 σ′(x) =e−x (1 +e−x)2 σ′(x) =e−x+ 1−1 (1 +e−x)2 σ′(x) =e−x+ 1 (1 +e−x)2−1 (1 +e−x)2 σ′(x) =1 1 +e−x−1 (1 +e−x)2 σ′(x) =1 1 +e−x· 1−1 1 +e−x σ′(x) =σ(x) (1−σ(x)) D. Hallar la funcion de perdida ¿MSE? Esto y mas en la siguiente clase. VII. C ONCLUSION En esta clase se reforzaron conceptos esenciales para comprender como los modelos de aprendizaje supervisado aprenden a partir de datos. Se revisaron las limitaciones de la regresion lineal y los problemas comunes asociados al sesgo y la varianza, asi como tecnicas para evaluar y mejorar la generalizacion de los modelos. Ademas, se introdujo la regresion logistica como un modelo de clasificacion, desta- cando el papel de la funcion sigmoide y su derivada en el proceso de optimizacion. Estos fundamentos sientan la base para profundizar en funciones de perdida especificas y en el entrenamiento de modelos mas complejos en futuras sesiones. REFERENCIAS [1] IEEE Costa Rica. “IngenIEEEria Costa Rica.” [En linea]. Disponible: https://r9.ieee.org/costarica/ingenieeeria [2] A. Shervine. “Hoja de referencia de Aprendizaje Au- tomatico.” Stanford University. [En linea]. Disponible: https://stanford.edu/ ∼shervine/l/es/teaching/cs-229/ hoja-referencia-aprendizaje-automatico-consejos-trucos