8, 9], [10, 11, 12, 13]], [[14, 15, 16, 17], [18, 19, 20, 21], [22, 23, 24, 25]]]) torch.Size([2, 3, 4]) 1) Reduccion:Se pueden sumar los elementos de un tensor usando sum() : x = torch.arange(3, dtype=torch.float32) x, x. sum() # Vector [0,1,2] y su suma tensor([0., 1., 2.]), tensor(3.) Para un tensor multidimensional, sum() por defecto reduce todos los ejes: A = torch.arange(6, dtype=torch.float32). reshape(2, 3) A.shape, A. sum() # Suma de todos los elementos torch.Size([2, 3]), tensor(15.) Se puede especificar un eje para sumar a lo largo de filas o columnas: A, A. sum(axis=0), A. sum(axis=1) tensor([[0., 1., 2.], [3., 4., 5.]]), tensor([3., 5., 7.]), tensor([3., 12.]) Reducir a multiples ejes simultaneamente es equivalente a sumar todos los elementos: A.sum(axis=[0,1]) == A. sum() True La media se calcula con mean() , equivalente a la suma dividida por el numero de elementos: A.mean(), A. sum()/A.numel() A.mean(axis=0), A. sum(axis=0)/A.shape[0] tensor(2.5000), tensor(2.5000) tensor([1.5, 2.5, 3.5]), tensor([1.5, 2.5, 3.5]) 2) Suma sin reduccion:Si se quiere conservar la forma del tensor tras sumar: sum_A = A. sum(axis=1, keepdims=True) A_normalized = A / sum_A # Broadcasting para normalizar filas A, sum_A, A_normalized tensor([[0., 1., 2.], [3., 4., 5.]]), tensor([[ 3.], [12.]]), tensor([[0.0000, 0.3333, 0.6667], [0.2500, 0.3333, 0.4167]]) 3) Suma acumulada: Se puede calcular la suma acumulada concumsum() : A.cumsum(axis=0) # Suma acumulada a lo largo de las filas tensor([[0., 1., 2.], [3., 5., 7.]]) REFERENCES [1] Steven Pachecho Portuguez, Clase sobre Algebra Lineal y