vistas. Ver Figura 5 para un ejemplo de visualizacion t-SNE. Figura 5: Ejemplo de visualizacion t-SNE de vectores latentes. III-G. Transicion a NLP: tokenizacion y embeddings III-G0a. Tokenizacion:Estrategias: palabra completa, subword (BPE), caracter, bytes. Subword reduce OOV y controla longitud de secuencia. Ver Figura 6 para un esquema de tokenizacion subword. III-G0b. Embeddings:Cada token se mapea a un vector e∈Rdmediante la capaEmbedding. Para representar frases se puede usar promedio de embeddings o agregadores mas complejos. III-G0c. Modelos de lenguaje:Evolucion: RNN/LSTM →Transformers con self-attention. La self-attention permite capturar dependencias largas y producir embeddings contex- tuales; esos embeddings sirven para recuperacion, clasificacion y agentes. Figura 6: Esquema ilustrativo de tokenizacion subword y mapeo a IDs. III-H. Recomendaciones operativas para la tarea Probar al menos dos configuraciones: (1) denoising au- toencoder, (2) V AE con latente de prueba (p. ej. 32, 64) dependiendo de la GPU. Guardar checkpoints y curvas de perdida. Evaluar MSE y SSIM. Para anomalias, definir umbral con validacion y reportar precision/recall. Para texto, experimentar tokenizacion subword y entrenar un embedding basico antes de usar modelos preentrena- dos. IV. CONCLUSIONES Los autoencoders representan una herramienta fundamental dentro del aprendizaje profundo no supervisado, al permitir que un modelo aprenda representaciones compactas de los datos sin depender de etiquetas externas. Durante la sesion se destaco como la arquitectura encoder–decoder constituye la base para multiples aplicaciones, desde la reduccion de dimen- sionalidad hasta la generacion y reconstruccion de imagenes. La comprension del espacio