sentacion intermedia estandar y eficiente. Esta representacion facilita la interoperabilidad y el despliegue de modelos en diferentes plataformas y hardware mediante optimizaciones en C++ u otros lenguajes, asegurando que el mismo modelo pueda ejecutarse con alto rendimiento en entornos variados. Considerando lo anterior, las plataformas poseen diversas lim- itaciones y rendimiento tanto en software como en hardware, si se entrenan modelos grandes, posiblemente un celular no este adaptado para soportar dicho modelo, para ello se observara el concepto de quantization. III. Q UANTIZATION Suponga que se tiene un modelo de deep learning con muchas capas, por ejemplo , llama 2 , con 70 mil millones de parametros aproximadamente, si cada parametro es de 32 bits, se obtiene un tamaño aproximado de 28 gb para solo almacenar el modelo, ¿Como podriamos cargarlo a memoria? Una alternativa es comprar una GPU con dicho tamaño para el procesamiento del modelo, pero GPUs que soporten esos tamaños son costosas , lo que se busca es reducir el tamaño del modelo, una de sus tecnicas es quantization A. definicion Quantization es una tecnica de compresion de modelos de aprendizaje automatico que reduce el numero de bits utilizados para representar los parametros del modelo, transformando los valores de punto flotante a representaciones de menor precision, generalmente enteros de 8, 5, 2 o incluso 1 bit. Lo que se busca es disminuir el tamaño y la complejidad computacional del modelo, manteniendo una precision cercana al original. No