se muestra un ejemplo donde se evalua la salida de una capa de activacion utilizando esta funcion de perdida. Fig. 30.Ejemplo Cambios a la Regla de la Cadena Como las funcionesğ¿ ğ‘–,ğ‘§(ğ‘™) ğ‘—yğ‘(ğ‘™) ğ‘—han sido mod- ificadas, es necesario plantear nuevas derivadas que permitan actualizar los parametros de cada neurona ğ‘—. En la ecuacion se observa que, para ajustar un peso especificoğ‘¤(ğ‘™) ğ‘—,ğ‘˜, debemos calcular sus derivadas parciales. Sin embargo, gracias al concepto decache, launica derivada que cambia al actualizar un peso diferente esğ›¿ğ‘§(ğ‘™) ğ‘— ğ›¿ğ‘¤(ğ‘™) ğ‘—,ğ‘˜, mientras que el resto permanece constante para toda la capa. Las derivadas se expresan de la siguiente forma: ğ›¿ğ¿ğ‘– ğ›¿ğ‘ğ‘™ ğ‘—=((ğ‘ğ‘™ 1âˆ’ğ‘¦ 1)2+(ğ‘ğ‘™ 2âˆ’ğ‘¦ 2)2+Â·Â·Â·+(ğ‘ğ‘™ ğ‘›âˆ’ğ‘¦ğ‘›)2) ğ›¿ğ¿ğ‘– ğ›¿ğ‘ğ‘™ ğ‘—=2(ğ‘ğ‘™ ğ‘—âˆ’ğ‘¦ğ‘—) ğ›¿ğ‘(ğ‘™) ğ‘— ğ›¿ğ‘§(ğ‘™) ğ‘—=ğ‘”(ğ‘§(ğ‘™) ğ‘—)(1âˆ’ğ‘”(ğ‘§(ğ‘™) ğ‘—)) ğ›¿ğ‘§(ğ‘™) ğ‘— ğ›¿ğ‘¤(ğ‘™) ğ‘—,ğ‘˜=ğ‘(ğ‘™âˆ’1) ğ‘˜ Con esto se logra actualizar los pesos de la capağ‘™, aunque las derivadas no cambian, si deben manejarse masindices a medida que la red crece en complejidad. Capağ‘™âˆ’1 Cuando el calculo debe extenderse hacia una capa anterior, como la capağ‘™âˆ’1, el procedimiento se vuelve mas complejo. Esto ocurre porque, segun el tamaÃ±o de la siguiente capa, el algoritmo requiere combinar mas conexiones y parametros, lo que incrementa la dificultad del calculo. ğ›¿ğ¿ğ‘– ğ›¿ğ‘(ğ‘™âˆ’1) ğ‘˜=ğ‘›ğ‘™âˆ‘ ğ‘—=1ğ›¿ğ‘§(ğ‘™) ğ‘— ğ›¿ğ‘(ğ‘™âˆ’1) ğ‘˜ğ›¿ğ‘(ğ‘™) ğ‘— ğ›¿ğ‘§(ğ‘™) ğ‘—ğ›¿ğ¿ğ‘– ğ›¿ğ‘(ğ‘™) ğ‘—