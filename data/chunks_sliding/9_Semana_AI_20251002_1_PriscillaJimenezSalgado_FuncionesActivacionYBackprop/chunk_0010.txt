como bordes o lineas. Luego, en las capas intermedias, ya aparecen formas un poco mas claras como partes de ojos o bocas. Finalmente, en lasultimas capas, la red es capaz de reconocer objetos completos, por ejemplo un rostro. Fig. 15.Extraccion progresiva de caracteristicas en una CNN ‚Äì Representaciones Vectoriales:En proce- samiento de lenguaje natural, las palabras se repre- sentan como vectores de alta dimension, esto permite que palabras con funciones similares se agrupen en el espacio vectorial. Fig. 16.VisualizacionIII.Continuacion de Funciones de activacion Las funciones de activacion son un elemento fundamental en las redes neuronales, ya que permiten introducir la no linealidad necesaria para representar relaciones complejas en los datos. A continuacion, se presentan las funciones mas importantes junto con sus principales caracteristicas matematicas. Fig. 17.Ejemplos de funciones de activacion: a la izquierda la funcion lineal y a la derecha la funcion tangente hiperbolica (tanh), usada en redes neuronales para introducir no linealidad. A. Funcion Lineal La funcion lineal se define comoùëì(ùë•)=ùë•. La derivada es constante, por lo que el modelo no puede usar el descenso del gradiente ni aprender de los datos. Fig. 18.Funcion lineal Fig. 19.Ejemplo B. Sigmoide Tiene una activacion que varia entre 0 y 1, siem- pre positiva, acotada y estrictamente creciente. Sin embargo, presenta el problema de que su derivada se aproxima a cero en los extremos de la funcion, lo que provoca gradientes muy peque√±os. Esto hace que el entrenamiento se vuelva lento o