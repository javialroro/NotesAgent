de activacion:En la regresion logistica esa transformacion se conoce como funcion no lineal, especificamente la sigmoide. Segun la seÃ±al recibida, la neurona se activa o no, permitiendo que la informacion continue, la transforme o la bloquee. Fig. 5.Funciones de activacion â€¢Funcion ReLU:La funcionğ‘”(ğ‘¥)=max(0,ğ‘¥) esta limitada por debajo de cero y es estricta- mente creciente. Es muy eficiente en modelos deDeep Learning, pero presenta el problema de las llamadasneuronas muertas, ya que no es derivable en todos los puntos y, en algunos casos, el gradiente puede llegar a ser cero, impidiendo la actualizacion de los pesos. Fig. 6.Ejemplo ReLu â€¢Leaky ReLU:Esta funcion asigna una pequeÃ±a constante al valor minimo permitido, lo que ayuda a evitar el problema de las neuronas muertas. Aunque representa una mejora respecto a la ReLU original, no se considera la solucion definitiva. ğ‘”(ğ‘¥)=( 0.01ğ‘¥, ğ‘¥<0 ğ‘¥, ğ‘¥â‰¥0 ğœ•ğ‘”(ğ‘¥) ğœ•ğ‘¥=( 0.01, ğ‘¥<0 1, ğ‘¥â‰¥0 Fig. 7.Ejemplo Leaky ReLu â€¢Parametric ReLU (PReLU):Esta funcion per- mite aprender un parametro que controla si la seÃ±al continua en la parte negativa. Dicho parametro se entrena junto con el resto de la red, lo que brinda mayor flexibilidad al modelo. ğ‘”(ğ‘¥)=( ğ‘¤ğ‘¥, ğ‘¥<0 ğ‘¥, ğ‘¥â‰¥0 ğœ•ğ‘”(ğ‘¥) ğœ•ğ‘¥=( ğ‘¤, ğ‘¥<0 1, ğ‘¥â‰¥0 â€¢Funcion Tanh:Tiene una forma parecida a la sigmoide, pero su salida esta acotada en el rango (âˆ’1,1), lo que permite manejar valores positivos y negativos. â€¢Binary Step Function:Devuelve 1 si la entrada es mayor que cero y 0 si es menor o