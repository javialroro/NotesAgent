de salida (Output Layer), que entrega el resultado final y cuyo tamaÃ±o depende del problema que se este resolviendo. La gran ventaja del MLP es que, gracias a sus multiples capas y funciones de activacion, introduce no linealidad, lo que le permite resolver problemas que el perceptron simple no podia. Ademas, se en- trena utilizando lapropagacion del error(backprop- agation), que consiste en calcular cuanto se equivoco la red y ajustar los pesos mediante descenso de gradiente, mejorando asi el rendimiento del modelo. Ahora nos preguntamos, Â¿como se calcula una pasada en la red? El proceso comienza con la expresionâ„(0)= ğ‘ ğ‘–ğ‘”ğ‘šğ‘œğ‘–ğ‘‘(ğ‘‹ğ‘Š0+ğ‘0), dondeâ„(0)corresponde a la primera capa oculta. Lo que se hace es calcular primero la regresion linealğ‘‹ğ‘Š0+ğ‘0, luego aplicar la funcion sigmoide al resultado, y con eso se obtiene el valor del primerHidden Layer. Despues, para la siguiente capa oculta, el pro- cedimiento es practicamente el mismo:â„(1)= ğ‘ ğ‘–ğ‘”ğ‘šğ‘œğ‘–ğ‘‘(â„(0)ğ‘Š1+ğ‘1). En este caso, el valor de â„(0)pasa a ser la entrada de la siguiente capa. Este mismo proceso se repite hasta llegar a laultima capa, que se expresa comoâ„(ğ‘›)=ğ‘ ğ‘–ğ‘”ğ‘šğ‘œğ‘–ğ‘‘(â„(ğ‘›âˆ’ 1)ğ‘Šğ‘›+ğ‘ğ‘›). En otras palabras, cada capa oculta toma como entrada el resultado de la capa anterior, y mediante una combinacion lineal mas la activacion, se van construyendo paso a paso los valores hasta la salida final de la red. â€“ Salida independiente y distribucion:Cada salida puede asociarse a una variable distinta. Segun el caso, la distribucion puede ser de tipo categorica (como en el