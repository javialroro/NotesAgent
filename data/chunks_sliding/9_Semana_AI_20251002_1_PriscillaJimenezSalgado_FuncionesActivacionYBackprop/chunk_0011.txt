activacion que varia entre 0 y 1, siem- pre positiva, acotada y estrictamente creciente. Sin embargo, presenta el problema de que su derivada se aproxima a cero en los extremos de la funcion, lo que provoca gradientes muy pequeÃ±os. Esto hace que el entrenamiento se vuelva lento o se detenga, fenomeno conocido comovanishing gradient. Fig. 20.Ejemplo C. Tangente Hiperbolica La funcionTanHtiene un rango de valores entre âˆ’1 y 1. Su comportamiento es similar al de la funcion sigmoide, con la diferencia de que esta centrada en el origen, lo que permite que los valores negativos tambien sean considerados. Sin embargo, al igual que la sigmoide, presenta el problema delgradiente desvanecidoen los extremos, lo que puede dificultar el entrenamiento de redes profundas. Fig. 21.Ejemplo D. Funcion Softmax La funcionSoftmaxconvierte la capa de salida (output layer) en una distribucion de probabilidad, ya que normaliza los valores mediante una sumatoria. Su definicion es la siguiente: ğœ(ğ‘¥)ğ‘—=ğ‘’ğ‘¥ğ‘— Iğ¾ ğ‘˜=1ğ‘’ğ‘¥ğ‘˜ Es comunmente utilizada en problemas de clasifi- cacion, donde el vector de entrada se conoce como logits. Ademas, se emplea junto con la funcion de perdidaCross-Entropy Loss. Fig. 22.Ejemplo softmax â€¢Â¿Por que usarğ‘’ğ‘¥?Porque es una funcion estrictamente creciente y evita valores negativos en la salida. â€¢Cross-Entropy Loss:Tambien llamadaLog- LossoLogistic Loss, se utiliza como funcion de perdida en Softmax. Representa probabilidades en un espacio logaritmico dentro del rango[0,1] y es numericamente estable. La perdida se define como: ğ¿=log(ğ‘ƒ(ğ‘Œ=ğ‘¦ ğ‘–|ğ‘‹=ğ‘¥ğ‘–)) y en el caso de clasificacion multiclase: