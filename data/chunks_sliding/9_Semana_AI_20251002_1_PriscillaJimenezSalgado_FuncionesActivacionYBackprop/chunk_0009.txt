cuando tra- bajamos con datos que tienen muchisimas variables o dimensiones. Al ir aumentando esas dimensiones, los datos empiezan a dispersarse y quedan muy separados entre si, lo que hace mas dificil encontrar patrones claros. En otras palabras, el modelo tiene que calcular en un espacio cada vez mas grande y con menos densidad de informacion, lo que complica el aprendizaje. Fig. 13.Maldicion de la dimensionalidad – Comportamiento jerarquico:Se utiliza este enfoque porque imita la forma en que los humanos aprenden: comienzan con conceptos simples y luego los combinan para formar ideas mas complejas. Esto permite generar mejoras exponenciales en las funciones y aprovechar mejor el aprendizaje. •Permite construir funciones polinomicas. •Utiliza la composicion de funciones, reutilizando funciones simples para crear otras de mayor nivel. •Ofrece una representacion compacta, donde con pocos pesos se pueden modelar funciones com- plejas. •Ejemplo:una red neuronal puede aproximar otra funcion. Fig. 14.Comportamiento jerarquico – Mapas de caracteristicas en CNN:En una red neuronal convolucional (CNN), las capas no trabajan solo con los pixeles, sino que van aprendiendo rep- resentaciones cada vez mas complejas de la imagen. Al inicio, en las primeras capas, se detectan cosas muy basicas como bordes o lineas. Luego, en las capas intermedias, ya aparecen formas un poco mas claras como partes de ojos o bocas. Finalmente, en lasultimas capas, la red es capaz de reconocer objetos completos, por ejemplo un rostro. Fig. 15.Extraccion progresiva de caracteristicas en una CNN – Representaciones