que se aprovecha en el algoritmo de backpropagation para optimizar los calculos. Fig. 27.Vector Gradiente D. Multiples neuronas Fig. 28.Grafo con mayor dimensionaldad â€¢Superindice:SeÃ±ala la capa a la que pertenece una variable. Ejemplo:ğ‘(ğ‘™)corresponde a la capa ğ‘™. â€¢Subindice:Identifica el numero de neurona dentro de una capa especifica. Ejemplo:ğ‘(ğ‘™) ğ‘—se refiere a lağ‘—- Â´esima neurona en la capağ‘™. â€¢Pesos:Se representan con dos subindices: el primero indica la neurona destino y el segundo la neurona de origen. Ejemplo:ğ‘¤(ğ‘™) ğ‘—,ğ‘˜representa el peso que conecta la neuronağ‘(ğ‘™âˆ’1) ğ‘˜con la neurona ğ‘(ğ‘™) ğ‘—. A continuacion, en la siguiente figura se ilustra como una neurona de la capağ‘™recibe entradas desde varias neuronas de la capa anterior(ğ‘™âˆ’1). Este proceso se puede dividir en dos pasos: â€¢Preactivacion: ğ‘§(ğ‘™) ğ‘—=ğ‘(ğ‘™) ğ‘—+ğ‘›ğ‘™âˆ’1âˆ‘ ğ‘˜=1ğ‘¤(ğ‘™) ğ‘—,ğ‘˜ğ‘(ğ‘™âˆ’1) ğ‘˜ dondeğ‘(ğ‘™) ğ‘—es el sesgo de la neurona yğ‘¤(ğ‘™) ğ‘—,ğ‘˜los pesos de conexion. â€¢Activacion: ğ‘(ğ‘™) ğ‘—=ğ‘”(ğ‘§(ğ‘™) ğ‘—) dondeğ‘”representa la funcion de activacion aplicada. Fig. 29.EjemploE. Calculo de funcion de perdida Para esta seccion, ellossglobal se obtiene sumando las diferencias entre la salida de cada neurona en la capa de activacionğ‘—y su valor esperadoğ‘¦ ğ‘—, recorriendo todas las neuronas de la capağ‘™. ğ¿ğ‘–=ğ‘›ğ‘™âˆ‘ ğ‘—=1(ğ‘(ğ‘™) ğ‘—âˆ’ğ‘¦ğ‘—)2 En la siguiente figura se muestra un ejemplo donde se evalua la salida de una capa de activacion utilizando esta funcion de perdida. Fig. 30.Ejemplo Cambios a la Regla de la Cadena Como las funcionesğ¿ ğ‘–,ğ‘§(ğ‘™) ğ‘—yğ‘(ğ‘™) ğ‘—han sido mod- ificadas, es necesario plantear nuevas derivadas que permitan actualizar los parametros de