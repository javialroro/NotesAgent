creciente y evita valores negativos en la salida. â€¢Cross-Entropy Loss:Tambien llamadaLog- LossoLogistic Loss, se utiliza como funcion de perdida en Softmax. Representa probabilidades en un espacio logaritmico dentro del rango[0,1] y es numericamente estable. La perdida se define como: ğ¿=log(ğ‘ƒ(ğ‘Œ=ğ‘¦ ğ‘–|ğ‘‹=ğ‘¥ğ‘–)) y en el caso de clasificacion multiclase: ğ¿=âˆ’log ğ‘’ğ‘ ğ‘˜ Iğ¶ ğ‘—=1ğ‘’ğ‘ ğ‘—! Fig. 23.Ejemplo E. Â¿Cual funcion de activacion utilizar? La eleccion de la funcion de activacion depende del tipo de problema que se este resolviendo. Las funcionesSigmoidyTanhsuelen presentar el in- conveniente delvanishing gradient, lo que dificulta el entrenamiento en redes profundas. Por ello, se recomienda iniciar con la funcionReLU, ya que es rapida de calcular y ampliamente utilizada enDeep Learning. En caso de que no funcione adecuada- mente, se pueden emplear variantes comoLeaky ReLUoParametric ReLU, que buscan superar estas limitaciones. IV. Backpropagation Permite calcular cuanto contribuye cada peso al error final de la red, actualizando los parametros en direccion opuesta a la propagacion hacia adelante. Este proceso es esencial para que la red aprenda y mejore su desempeÃ±o durante el entrenamiento. A. Procesos del Entrenamiento â€¢Forward Propagation:Consiste en calcular la salida de la red enviando los datos desde la capa de entrada hacia las capas siguientes, hasta obtener el resultado final. â€¢Backpropagation:Implica propagar el error desde la capa de salida hacia las capas anteriores, calculando las derivadas parciales con respecto a los pesos y sesgos para ajustar los parametros del modelo. Fig. 24.Forward y Back Propagation B. Optimizacion del