Descenso del gradiente Se propone una analogia sobre que se esta en una montana muy elevada, se tiene los ojos vendados y la meta es bajar con la menor cantidad de esfuerzo y los mas rapido posible. Y la solucion es desde el punto inicial, calcular el lado que tiene mas pendiente, dar un paso y repetir ese mismo proceso hasta llegar a un punto minimo de altura. Matematicamente, esto se formaliza mediante la regla de actualizacion: xnuevo =xantiguo −α∇f(xt), donde αes la tasa de aprendizaje ( learning rate ) y∇f(xt) representa el gradiente de la funcion en el punto xantiguo . El valor de αes critico: un valor demasiado grande puede provocar que el algoritmo oscile y no converja, mientras que un valor demasiado pequeno ocasiona una convergencia muy lenta. Para mitigar estos problemas, se suelen emplear estrate- gias como la busqueda de una tasa de aprendizaje optima o el early stopping , que detiene el entrenamiento cuando la funcion de perdida deja de mejorar significativamente o alcanza un valor aceptable. Nota: Los terminos derivada, pendiente y gradiente son equivalentes. Derivada de una constante:d dx[c] = 0 Derivada de una variable:d dx[x] = 1 Derivada de constante por variable:d dx[c·x] =c Regla de la potencia:d dx[xn] =nxn−1 Derivada de una suma:d dx[f(x) +g(x)] =f′(x) +g′(x) Regla del producto:d dx[f(x)g(x)] =f′(x)g(x) +f(x)g′(x) Derivadas parciales:∂f ∂xi=derivada de frespecto a xi Ejemplo de parciales: f(x, y) =x2y+ 3xy2,∂f ∂x= 2xy+ 3y2,∂f ∂y=x2+