experimento para visualizar activaciones intermedias y optimizar arquitecturas. 4) GoogleNet (Inception):Reduciendo los mas de 60 mil- lones de parametros de AlexNet a unos 4 millones, GoogleNet introdujo los modulosInception[3]. Cada modulo combina convoluciones de diferentes tamaños (1×1,3×3,5×5) junto conmax pooling, concatenando sus resultados. Al final, la salida (7×7×1024) se aplana y se pasa a unaverage poolingde1×1×1024. 5) VGG16:Caracterizada por su simplicidad, utilizaunicamente filtros de3×3y aumenta la profundidad hasta 16 capas. Esta arquitectura demostro que aumentar la profundidad mejora el rendimiento si se mantienen filtros pequeños y consistentes. 6) ResNet:Introduce lasconexiones residuales, que per- miten el paso de informacion entre capas no adyacentes. Esto evita la degradacion del gradiente en redes muy profundas y mejora la capacidad de entrenamiento. 7) DenseNet:Conecta cada capa con todas las anteriores, favoreciendo la reutilizacion de caracteristicas y reduciendo la cantidad de parametros necesarios. Este enfoque mejora la eficiencia y el flujo de informacion a lo largo de la red. III. MATERIA DE CLASE A. Problemas en las Redes Neuronales Convolucionales 1) Explicabilidad del Modelo:Uno de los principales de- safios actuales es la falta de interpretabilidad en las redes Fig. 1. Representacion de embeddings mediante t-SNE. profundas. Losfeaturesaprendidos por las capas internas suelen ser dificiles de entender por los humanos, lo que complica saber que esta “viendo” realmente el modelo. 2) Visualizacion y Analisis de Activaciones:Una forma de entender mejor el funcionamiento interno es observar: •Visualizacion de activaciones:muestra que regiones de la imagen activan ciertas neuronas.