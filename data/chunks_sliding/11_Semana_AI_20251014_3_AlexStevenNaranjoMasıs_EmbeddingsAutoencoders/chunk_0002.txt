y habilitan tareas de aprendizaje no supervisado/semisupervisado. II. FUNDAMENTOS DEREDESNEURONALES CONVOLUCIONALES(CNNS) A. Filtros (Kernels) y Campos Receptivos Un filtro 2D de tamañok×kse desliza sobre la imagen (o mapa de activacion) para producir unfeature map. Para una entrada RGBH×W×C inyC outfiltros, cada filtro tiene tamañok×k×C iny produce un canal en la salida. Filtro Gaussiano:suaviza la imagen (blur) y reduce ruido; resalta contornos al combinarse con operadores de gradiente. Campo Receptivo (RF):region de la entrada que “ve” una neurona de una capa dada. Aumenta con la profun- didad. Si encadenamos capas con kernelk iystrides i, el RF efectivo crece de forma acumulativa. Parametros y costo:el numero de parametros en una capa conv esk2·Cin·Cout+C out(sesgo). La complejidad computacional se aproxima porH out·Wout·k2·Cin·Cout.B. Parametros de la Convolucion: Stride, Padding y Tamaño de Salida Para una entrada 1D de longitudm, kernelk,paddingpy strides, la salida es: out=m+ 2p−k s + 1. En 2D se aplica por dimension (alto y ancho). Elpadding simetrico tipico para “conservacion de tamaño” cons= 1es p=k−1 2(sikes impar). Elstride>1reduce la resolucion espacial. C. Pesos Compartidos y Eficiencia Lacomparticion de pesosaplica el mismo kernel en todas las posiciones espaciales, reduciendo parametros frente a capas densas. En primeras capas, la red aprende bordes y texturas; en capas profundas, patrones semanticos mas abstractos. D. Capa de Pooling Reduce la resolucion espacial conservando canales: Max Pooling:retiene el valor maximo de cada ventana. Average Pooling:promedia los valores. Regla practica:pooling2×2constride2 para reduccion a la mitad. MantieneD=C iny reduceH,