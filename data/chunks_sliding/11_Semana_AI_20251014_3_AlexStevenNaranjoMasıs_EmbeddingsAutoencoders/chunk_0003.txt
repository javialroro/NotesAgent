En primeras capas, la red aprende bordes y texturas; en capas profundas, patrones semanticos mas abstractos. D. Capa de Pooling Reduce la resolucion espacial conservando canales: Max Pooling:retiene el valor maximo de cada ventana. Average Pooling:promedia los valores. Regla practica:pooling2×2constride2 para reduccion a la mitad. MantieneD=C iny reduceH, W. E. Activaciones, Normalizacion y Regularizacion Activacion:ReLU es estandar en CNN modernas (evita saturacion y acelera entrenamiento). Tanh/sigmoid pue- den usarse en salidas especificas. Batch Normalization (BN):estabiliza la distribucion de activaciones, permite mayores tasas de aprendizaje y acelera la convergencia. Regularizacion:Dropout (tipico en capas densas), L2 (weight decay) ydata augmentationreducen sobreajuste. F . Capa Fully-Connected (MLP) y Clasificacion Tras extraer mapas de activacion, se aplicaflatten(oglobal average pooling) y capas densas para clasificacion. En proble- mas multi-clase se usasoftmaxy perdida de entropia cruzada. III. ARQUITECTURASCONVOLUCIONALES A. LeNet-5 Pionera (LeCun, 1998) para digitos manuscritos (MNIST). Dos bloques conv+pooling y capas densas. Introdujo la viabi- lidad practica de CNNs. B. AlexNet (2012) Krizhevsky et al. popularizan ReLU,dropout, entrenamiento en multiples GPUs y kernels grandes (11×11,5×5,3×3) en entradas224×224. Disparo la adopcion dedeep learning a gran escala. C. ZFNet y Visualizacion Intermedia Ajusta tamaños de kernel/stride y estudiafeature maps internos para entender que aprende cada capa, motivando practicas de diseño y depuracion. D. GoogLeNet / Inception Modulos con ramas paralelas (1×1,3×3,5×5+max pooling); reduce parametros (de∼60M a∼4M) usando cuellos 1×1yglobal average poolingal final. E. VGG-16 Filosofia de simplicidad: solo3×3+ profundidad (16/19 capas). A pesar de muchos parametros, es