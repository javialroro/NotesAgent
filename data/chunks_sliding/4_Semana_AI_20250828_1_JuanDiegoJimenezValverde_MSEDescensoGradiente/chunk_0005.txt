el gradiente esdy dx= 2x= 2. •Para acercarnos al minimo, nos movemos en la direccion del gradiente negativo con un paso de tamano α. Regla de actualizacion xnuevo =xantiguo−α·(2x) •Donde 2xes el gradiente. •El proceso se repite hasta que el gradiente sea 0 (punto de minimo).Importancia del α(learning rate) •El tamano del paso αdebe ser pequeno (ejemplo: 0.1) para no sobrepasar el minimo. •Al acercarnos al minimo, los saltos se reducen porque el gradiente disminuye. •Unαmuy grande puede provocar oscilaciones o incluso alejarse del minimo. •Unαdemasiado pequeno ralentiza la convergencia. Nota •Ellearning rate (α) es un hiperparametro que debe seleccionarse cuidadosamente. Fig. 1. Impacto del learning rate en gradient descent F . ¿Por queusar MSE y no MAE? ElMean Squared Error (MSE) es mas utilizado que el Mean Absolute Error (MAE) en optimizacion con descenso del gradiente porque: •La funcion MSE es suave (diferenciable en todos sus puntos), lo que permite calcular derivadas de forma sencilla y aplicar metodos basados en gradiente. •En contraste, la funcion MAE no es diferenciable en 0(presenta una esquina), lo que complica el uso de derivadas directas y hace mas dificil la optimizacion con gradiente puro. •Gracias a su naturaleza cuadratica, el MSE penaliza mas fuertemente los errores grandes, favoreciendo un ajuste mas preciso en esos casos. Error Cuadratico Medio (MSE): MSE =1 NNX i=1(fw,b(xi)−yi)2 Fig. 2. MSE Error Absoluto Medio (MAE): MAE =1 NNX i=1|fw,b(xi)−yi| Fig. 3. MAE IV. M ATERIAL DE CLASE A.