conjunto de entre- namiento usado para calcular la gradiente y actualizar parametros. •Ejemplo: 10 000 samples y batch size = 1 000 ⇒se necesitan 10 batches para completar 1 epoch. •No se espera a procesar todo el dataset: cada particion (batch) sirve para calcular la gradiente y actualizar los parametros. •Cada vez que procesamos un batch actualizamos los parametros (o acumulamos gradientes segun la estrate- gia). Tipos de descenso por gradiente: a) Batch Gradient Descent (Vanilla): •Calcula la gradiente usando todo el dataset: ∇L= 1 NPN i=1. . .. •Actualizacion cuando se ha procesado el conjunto com- pleto. •Ventajas: gradiente estable, pasos consistentes. •Desventajas: –Requiere tener todo el dataset en memoria. –En datasets grandes, las actualizaciones son lentas (cada paso es costoso). –Gradiente muy estable puede ocultar senales utiles y hacer que el proceso converja a parametros no deseados segun la topologia (segun el problema). Fig. 4. Batch gradient descent b) Stochastic Gradient Descent (SGD): •Actualiza los parametros por cada sample del training set (o mezcla aleatoria de samples). •Ventajas: detecta rapidamente si el algoritmo puede con- verger; util para datasets muy grandes. •Desventajas: –Senales de gradiente ruidosas (alto ruido en las actualizaciones). –Muchas actualizaciones (computacionalmente cos- toso si no se optimiza). –La trayectoria del parametro es muy oscilatoria: w←w−α∂L ∂w ejecutado por muestra puede producir movimientos muy erraticos. Fig. 5. Stochastic gradient descent c) Mini-batch Gradient Descent: •Combina ambas estrategias: se calcula la gradiente sobre batches de tamano intermedio.