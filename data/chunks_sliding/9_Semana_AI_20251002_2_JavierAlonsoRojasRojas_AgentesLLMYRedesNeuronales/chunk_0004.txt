El perceptron puede entenderse de forma similar a una regresion logistica, aunque se diferencia en la funcion de perdida que utiliza. Durante la historia de la inteligencia artificial surgio el llamado “invierno de la IA” , en parte debido al problema del XOR, ya que este no podia ser representado adecuadamente por un modelo lineal ni por un perceptron simple. Fig. 2. Funciones de activacion B. El problema del XOR El principal inconveniente del perceptron simple es que el problema XOR no es linealmente separable, por lo que este modelo no puede ofrecer una solucion adecuada. Esto dio origen a las redes neuronales multicapa (MLP), capaces de resolver problemas no lineales y ampliar significativamente el rango de aplicaciones posibles. C. Inspiracion biologica Las redes neuronales artificiales se inspiran en el fun- cionamiento del cerebro humano. Cada neurona recibe señales a traves de sus dendritas (entradas), las procesa en el nucleo mediante una combinacion lineal, y decide si transmite o no la señal segun una funcion de activacion. D. Funciones de activacion Las funciones de activacion introducen no linealidad en el modelo, permitiendo que la red aprenda relaciones complejas: •ReLU: g(x) = max(0 , x); eficiente, pero puede generar “neuronas muertas” cuando el gradiente es cero. •Leaky ReLU: introduce una pequeña pendiente en la parte negativa para evitar neuronas inactivas. •Tanh: produce salidas en el rango (−1,1),´util para manejar valores positivos y negativos. •Sigmoide: transforma la entrada en valores entre 0