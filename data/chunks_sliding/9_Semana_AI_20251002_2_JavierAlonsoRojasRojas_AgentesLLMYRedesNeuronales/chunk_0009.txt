k=1exk donde cada valor xjse denomina logit. Esta funcion se utiliza principalmente en problemas de clasificacion multiclase, ya que garantiza que todas las salidas sean positivas y sumen 1. •El uso de exasegura una funcion estrictamente creciente y evita valores negativos. •Se emplea junto con la funcion de perdida Cross- Entropy Loss , tambien llamada Log-Loss . La perdida se define como: L=−logP(Y=yi|X=xi) y, en el caso multiclase: L=−logesk PC j=1esj F . Seleccion de la funcion de activacion La eleccion de la funcion de activacion depende del tipo de problema y la arquitectura de la red. Las funciones Sigmoid y Tanh tienden a sufrir el problema del gradiente desvanecido, por lo que no son recomendadas para redes profundas. En la practica, se suele comenzar con la funcionReLU por su eficiencia computacional y buen rendimiento en modelos deDeep Learning . Si esta presenta problemas (por ejemplo, neuronas muertas), se pueden utilizar variantes como Leaky ReLU oParametric ReLU , que permiten mantener un flujo de gradiente estable incluso en valores negativos. Fig. 6. Uso de Softmax Fig. 7. Forward Propagation y Back Propagation VI. B ACKPROPAGATION El algoritmo de backpropagation permite calcular cuanto contribuye cada peso al error final de la red, actualizando los parametros en la direccion opuesta a la propagacion hacia adelante. Este proceso es esencial para que la red neuronal aprenda y mejore su desempeño durante el entrenamiento. Esto representado en la figura 7. A. Forward Propagation Consiste