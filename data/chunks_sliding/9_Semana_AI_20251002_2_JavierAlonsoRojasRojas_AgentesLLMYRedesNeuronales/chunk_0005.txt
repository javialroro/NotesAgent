, x); eficiente, pero puede generar “neuronas muertas” cuando el gradiente es cero. •Leaky ReLU: introduce una pequeña pendiente en la parte negativa para evitar neuronas inactivas. •Tanh: produce salidas en el rango (−1,1),´util para manejar valores positivos y negativos. •Sigmoide: transforma la entrada en valores entre 0 y 1, comun en tareas de clasificacion binaria. E. Perceptron Multicapa (MLP) ElMultilayer Perceptron (MLP) extiende el perceptron sim- ple añadiendo capas ocultas que permiten resolver problemas no lineales. Su estructura general incluye: •Capa de entrada: recibe los datos originales Xi. •Capas ocultas: realizan transformaciones y calculos in- ternos. •Capa de salida: entrega el resultado final, cuyo tamaño depende del tipo de problema. El entrenamiento se realiza mediante backpropagation , que calcula el error del modelo y ajusta los pesos utilizando descenso de gradiente. Fig. 3. Comportamiento Jerarquico Fig. 4. Funcionamiento de las CNN Cada capa se calcula de la siguiente forma: h(0)=σ(XW 0+b0) (1) h(1)=σ(h(0)W1+b1) (2) h(n)=g(h(n−1)Wn+bn) (3) F . Capas de salida y distribucion Las salidas pueden ser categoricas o continuas: •En clasificacion, se usa softmax como funcion de salida. •En regresion, se emplea una funcion lineal. En todos los casos, la activacion final g(x)debe ser no lineal para permitir un aprendizaje mas expresivo. G. Maldicion de la dimensionalidad Cuando se trabaja con datos de muchas variables, los puntos se dispersan en un espacio de alta dimension, reduciendo su densidad y dificultando el hallazgo de patrones significativos. H. Comportamiento jerarquico