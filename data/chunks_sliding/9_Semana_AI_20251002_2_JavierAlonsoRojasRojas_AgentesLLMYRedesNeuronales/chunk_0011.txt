error con respecto a cada parametro y aprovechando la salida de la capa anterior. D. Vector gradiente Elvector gradiente esta formado por todas las derivadas parciales de los parametros (pesos y sesgos) de la red. Durante el calculo del gradiente se identifican operaciones repetidas, lo que permite optimizar los calculos en el algoritmo de back- propagation mediante reutilizacion de resultados intermedios (cache). E. Redes con multiples neuronas En redes con mayor dimensionalidad como la de la figura 9, se introducen notaciones adicionales: •Superindice: indica la capa. Ejemplo: a(l)representa la activacion en la capa l. •Subindice: indica la neurona dentro de una capa. Ejem- plo:a(l) jes la j-´esima neurona en la capa l. •Pesos: se representan como w(l) j,k, que conecta la neurona a(l−1) kcona(l) j, aca j seria el destino y k el origen. Cada neurona de la capa lrecibe entradas desde todas las neuronas de la capa anterior (l−1), siguiendo los pasos: •Preactivacion: z(l) j=b(l) j+nl−1X k=1w(l) j,ka(l−1) k •Activacion: a(l) j=g(z(l) j) Para obtener la activacion de una neurona destino, se calculan las contribuciones de todas las neuronas de la capa anterior, multiplicando los pesos de conexion correspondientes por la activacion de cada neurona origen. Posteriormente, se suman estos productos junto con el sesgo asociado, repitiendo el proceso para cada neurona de la capa. F . Funcion de perdida global La funcion de perdida global se obtiene sumando las difer- encias entre la salida de cada neurona en