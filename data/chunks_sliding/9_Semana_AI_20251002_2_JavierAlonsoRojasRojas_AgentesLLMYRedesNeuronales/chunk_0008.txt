embargo, al igual que la sigmoide, tambien sufre del problema del gradiente desvanecido en los extremos.D. Parametric ReLU (PReLU) La funcionParametric ReLU (PReLU) es una variante de la funcion ReLU tradicional, como se muestra en la figura ??. A diferencia de la ReLU estandar, esta introduce un parametro αque se aprende durante el entrenamiento y controla la pendiente en la region negativa. De esta manera, el modelo puede ajustar automaticamente el grado de “fuga” en los valores menores que cero, evitando el problema de las neuronas muertas . Su definicion matematica es la siguiente: g(x) =( αx, six <0 x, six≥0 La derivada correspondiente es: dg(x) dx=( α, six <0 1,six≥0 El parametro αse entrena junto con el resto de los pesos de la red, lo que otorga al modelo mayor flexibilidad y capacidad de adaptacion frente a distintas distribuciones de datos. Por esta razon, la PReLU suele ofrecer un mejor desempeño en arquitecturas profundas donde la ReLU estandar podria perder gradiente. E. Softmax La funcion Softmax transforma las salidas de la capa final en una distribucion de probabilidad, como vemos en la figura 6. Su expresion se define como: σ(x)j=exj PK k=1exk donde cada valor xjse denomina logit. Esta funcion se utiliza principalmente en problemas de clasificacion multiclase, ya que garantiza que todas las salidas sean positivas y sumen 1. •El uso de exasegura una funcion estrictamente creciente y evita valores negativos. •Se emplea junto con la funcion de