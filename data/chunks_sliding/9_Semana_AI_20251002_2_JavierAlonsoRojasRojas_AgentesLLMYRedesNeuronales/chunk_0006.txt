activacion final g(x)debe ser no lineal para permitir un aprendizaje mas expresivo. G. Maldicion de la dimensionalidad Cuando se trabaja con datos de muchas variables, los puntos se dispersan en un espacio de alta dimension, reduciendo su densidad y dificultando el hallazgo de patrones significativos. H. Comportamiento jerarquico Como vemos en la Figura 3, las redes neuronales apren- den de forma jerarquica, combinando funciones simples para formar otras mas complejas. Esto permite construir representa- ciones compactas y eficientes, en las que un numero reducido de pesos puede modelar funciones avanzadas. I. CNN En las redes convolucionales (CNN), las primeras capas detectan bordes o patrones basicos, las intermedias aprenden estructuras mas definidas y lasultimas capas reconocen objetos completos, como rostros o figuras, esto representado en la Figura 4. J. Representaciones vectoriales En el procesamiento de lenguaje natural (NLP), las palabras se representan como vectores en un espacio de alta dimension, de modo que las palabras con significados o funciones simi- lares se ubican proximas entre si en dicho espacio. Fig. 5. Tangente hiperbolica V. F UNCIONES DE ACTIVACION Las funciones de activacion son un componente esencial en las redes neuronales, ya que permiten introducir la no linealidad necesaria para modelar relaciones complejas entre los datos. A continuacion, se describen las funciones mas rel- evantes junto con sus principales caracteristicas matematicas. A. Lineal La funcion lineal se define como: f(x) =ax Su derivada es constante ( fâ€²(x) =a), por lo que el