factores estadisticos derivados del conjunto de datos de prueba (“calibration set”). VII-B. Post-Training Quantization (PTQ) Despues del entrenamiento, se insertan observadores (ob- servers) en el modelo para analizar las salidas de cada capa y determinar los mejores parametros de escala y punto cero. Este proceso no requiere reentrenamiento y es rapido, aunque puede perder algo de precision. VII-C. Quantization-Aware Training (QAT) Simula la quantization durante el entrenamiento. El modelo aprende a compensar los errores introducidos por la reduccion de precision, por lo que mantiene un rendimiento superior tras el proceso. VIII. VENTAJAS DELQUANTIZATION Menor consumo de memoria: los modelos comprimidos se cargan mas rapido. Menor tiempo de inferencia: calculos mas simples. Menor consumo energetico: ideal para dispositivos em- bebidos o moviles. Portabilidad: permite ejecutar modelos complejos en hardware limitado.IX. CONCLUSIONES El estudio del quantization permite comprender como los modelos de inteligencia artificial pueden adaptarse a las limi- taciones del hardware sin comprometer significativamente su desempeno. Esta tecnica representa un punto de conexion entre el desarrollo teorico de los algoritmos y su aplicacion real en sistemas de produccion, donde los recursos computacionales, la energia y el tiempo de inferencia son factores determinantes.