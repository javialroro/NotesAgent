{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10918002",
   "metadata": {},
   "source": [
    "# Fase 1. Recolecci√≥n y organizaci√≥n de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be5f26d",
   "metadata": {},
   "source": [
    "En docs \"lineamientos_nomenclatura_y_citacion.md\" est√° la informaci√≥n por si quieren guiarse y revisar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c92d8c2",
   "metadata": {},
   "source": [
    "# Fase 2. Procesamiento de texto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9be9e59",
   "metadata": {},
   "source": [
    "## Extracci√≥n del texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "073fa793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 46 archivos extra√≠dos correctamente a data/apuntes_clean/raw\n"
     ]
    }
   ],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "import os, re\n",
    "\n",
    "# Directorio base del proyecto\n",
    "BASE_DIR = \"data\"\n",
    "RAW_PDF_DIR = os.path.join(BASE_DIR, \"apuntes_raw\")\n",
    "OUT_RAW_DIR = os.path.join(BASE_DIR, \"apuntes_clean\", \"raw\")\n",
    "\n",
    "# Crear la carpeta de salida si no existe\n",
    "os.makedirs(OUT_RAW_DIR, exist_ok=True)\n",
    "\n",
    "# Buscar todos los PDFs en la carpeta apuntes_raw\n",
    "pdf_files = [f for f in os.listdir(RAW_PDF_DIR) if f.lower().endswith(\".pdf\")]\n",
    "\n",
    "for pdf_file in pdf_files:\n",
    "    pdf_path = os.path.join(RAW_PDF_DIR, pdf_file)\n",
    "    txt_name = os.path.splitext(pdf_file)[0] + \".txt\"\n",
    "    out_txt_path = os.path.join(OUT_RAW_DIR, txt_name)\n",
    "\n",
    "    reader = PdfReader(pdf_path)\n",
    "    text_pages = []\n",
    "    for page in reader.pages:\n",
    "        text_pages.append(page.extract_text() or \"\")\n",
    "    \n",
    "    # Unir todas las p√°ginas con doble salto de l√≠nea\n",
    "    full_text = \"\\n\\n\".join(text_pages)\n",
    "    with open(out_txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(full_text)\n",
    "\n",
    "print(f\"‚úÖ {len(pdf_files)} archivos extra√≠dos correctamente a {OUT_RAW_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ac2e1a",
   "metadata": {},
   "source": [
    "## Normalizaci√≥n de txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9619af32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 46 archivos corregidos y guardados en data/apuntes_clean/normalized\n"
     ]
    }
   ],
   "source": [
    "import os, re, unicodedata\n",
    "\n",
    "BASE_DIR = \"data\"\n",
    "CLEAN_IN_DIR  = os.path.join(BASE_DIR, \"apuntes_clean\", \"raw\")        # carpeta de entrada\n",
    "CLEAN_OUT_DIR = os.path.join(BASE_DIR, \"apuntes_clean\", \"normalized\") # carpeta de salida\n",
    "os.makedirs(CLEAN_OUT_DIR, exist_ok=True)\n",
    "\n",
    "def quitar_tildes_y_reparar_espacios(texto: str) -> str:\n",
    "    # 1) Normalizaci√≥n Unicode para exponer diacr√≠ticos combinantes\n",
    "    t = unicodedata.normalize(\"NFD\", texto)\n",
    "\n",
    "    # 2) Reemplazos t√≠picos de PDF \n",
    "    t = (t.replace(\"\\u00A0\", \" \")      # NBSP -> espacio normal\n",
    "           .replace(\"\\u00AD\", \"\")      # soft hyphen -> nada\n",
    "           .replace(\"Ô¨Å\", \"fi\").replace(\"Ô¨Ç\", \"fl\")  # ligaduras\n",
    "           .replace(\"\\u0131\", \"i\")     # ƒ± (i sin punto) -> i\n",
    "           .replace(\"Àô\", \"\").replace(\"`\", \"\").replace(\"¬®\", \"\").replace(\"ÀÜ\", \"\"))\n",
    "\n",
    "    # 3) Unir SOLO cuando hay acento suelto entre letras: \"implementaci ¬¥on\" -> \"implementacion\"\n",
    "    t = re.sub(r\"([A-Za-z√±√ë])\\s*[\\u00B4\\u0301]\\s*([A-Za-z√±√ë])\", r\"\\1\\2\", t)\n",
    "\n",
    "    # 4) Convertir virgulilla suelta (~ o \\u02DC) en √ë/√± cuando corresponde (p.ej. \"tama Àúno\" -> \"tama√±o\")\n",
    "    #    a) letra + ~ + n/N\n",
    "    t = re.sub(r\"([A-Za-z√±√ë])\\s*[\\u02DC~]\\s*([Nn])\",\n",
    "               lambda m: m.group(1) + (\"√ë\" if m.group(2).isupper() else \"√±\"),\n",
    "               t)\n",
    "    #    b) ~ al inicio o tras espacio antes de n/N + vocal (p.ej. \" Àúno \" -> \" √±o \")\n",
    "    t = re.sub(r\"(?<!\\S)[\\u02DC~]\\s*([Nn])(?=[aeiou√°√©√≠√≥√∫AEIOU√Å√â√ç√ì√ö])\",\n",
    "               lambda m: (\"√ë\" if m.group(1).isupper() else \"√±\"),\n",
    "               t)\n",
    "\n",
    "    # 5) Eliminar diacr√≠ticos (tildes) PERO conservar √±/√ë\n",
    "    t = ''.join(c for c in t if unicodedata.category(c) != 'Mn' or c.lower() == '√±')\n",
    "\n",
    "    # 6) Limpieza suave: colapsar espacios repetidos y limitar saltos\n",
    "    t = re.sub(r\"[ \\t]+\", \" \", t)\n",
    "    t = re.sub(r\"\\n{3,}\", \"\\n\\n\", t)\n",
    "\n",
    "    return t.strip()\n",
    "\n",
    "# Aplicar a todos los .txt de entrada\n",
    "count = 0\n",
    "for fname in os.listdir(CLEAN_IN_DIR):\n",
    "    if not fname.lower().endswith(\".txt\"):\n",
    "        continue\n",
    "\n",
    "    with open(os.path.join(CLEAN_IN_DIR, fname), \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        raw = f.read()\n",
    "\n",
    "    norm = quitar_tildes_y_reparar_espacios(raw)\n",
    "\n",
    "    with open(os.path.join(CLEAN_OUT_DIR, fname), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(norm)\n",
    "    count += 1\n",
    "\n",
    "print(f\"‚úÖ {count} archivos corregidos y guardados en {CLEAN_OUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31180d1",
   "metadata": {},
   "source": [
    "## Segmentaci√≥n (P√°rrafos y Ventanas Deslizantes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f353734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Segmentaci√≥n regenerada con par√°metros ajustados.\n",
      " - √çndice p√°rrafos:  data/chunks_paragraphs/index_paragraphs.csv\n",
      " - √çndice ventanas:  data/chunks_sliding/index_sliding.csv\n",
      " - Resumen:          data/chunks_summary.csv\n",
      " - Carpeta chunks A: data/chunks_paragraphs\n",
      " - Carpeta chunks B: data/chunks_sliding\n"
     ]
    }
   ],
   "source": [
    "import os, re, csv, shutil\n",
    "from statistics import mean\n",
    "\n",
    "BASE_DIR = \"data\"\n",
    "INPUT_DIR = os.path.join(BASE_DIR, \"apuntes_clean\", \"normalized\")\n",
    "\n",
    "# --- P√°rrafos ---\n",
    "PAR_MIN_CHARS = 480\n",
    "PAR_MAX_CHARS = 2000\n",
    "MERGE_TITLES = True\n",
    "TITLE_MAX_CHARS = 140\n",
    "\n",
    "# --- Ventanas deslizantes ---\n",
    "WIN_WORDS = 240\n",
    "WIN_OVERLAP = 0.20\n",
    "WIN_STRIDE = max(1, int(WIN_WORDS * (1 - WIN_OVERLAP)))\n",
    "\n",
    "# --- Salidas ---\n",
    "OUT_PAR_DIR = os.path.join(BASE_DIR, \"chunks_paragraphs\")\n",
    "OUT_WIN_DIR = os.path.join(BASE_DIR, \"chunks_sliding\")\n",
    "\n",
    "# Limpiar salidas anteriores para √≠ndices consistentes\n",
    "for d in (OUT_PAR_DIR, OUT_WIN_DIR):\n",
    "    if os.path.exists(d):\n",
    "        shutil.rmtree(d)\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "IDX_PAR_CSV = os.path.join(OUT_PAR_DIR, \"index_paragraphs.csv\")\n",
    "IDX_WIN_CSV = os.path.join(OUT_WIN_DIR, \"index_sliding.csv\")\n",
    "SUMMARY_CSV = os.path.join(BASE_DIR, \"chunks_summary.csv\")\n",
    "\n",
    "# ===================== UTILIDADES =====================\n",
    "def read_txt(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        return f.read()\n",
    "\n",
    "def safe_filename_stem(fname):\n",
    "    return os.path.splitext(os.path.basename(fname))[0]\n",
    "\n",
    "def split_paragraphs(text):\n",
    "    raw_pars = [p.strip() for p in re.split(r\"\\n\\s*\\n\", text) if p.strip()]\n",
    "    pars = []\n",
    "    for p in raw_pars:\n",
    "        p = re.sub(r\"[ \\t]+\", \" \", p).strip()\n",
    "        pars.append(p)\n",
    "    return pars\n",
    "\n",
    "def split_sentences(p):\n",
    "    parts = re.split(r\"(?<=[\\.\\?\\!])\\s+\", p)\n",
    "    return [s.strip() for s in parts if s.strip()]\n",
    "\n",
    "def repartition_long_paragraph(p, max_chars):\n",
    "    if len(p) <= max_chars:\n",
    "        return [p]\n",
    "    sent = split_sentences(p)\n",
    "    chunks, buf = [], \"\"\n",
    "    for s in sent:\n",
    "        if not buf:\n",
    "            buf = s\n",
    "        elif len(buf) + 1 + len(s) <= max_chars:\n",
    "            buf = buf + \" \" + s\n",
    "        else:\n",
    "            chunks.append(buf.strip())\n",
    "            buf = s\n",
    "    if buf:\n",
    "        chunks.append(buf.strip())\n",
    "    final = []\n",
    "    for c in chunks:\n",
    "        if len(c) <= max_chars:\n",
    "            final.append(c)\n",
    "        else:\n",
    "            for i in range(0, len(c), max_chars):\n",
    "                final.append(c[i:i+max_chars].strip())\n",
    "    return final\n",
    "\n",
    "def fuse_short_paragraphs(pars, min_chars, merge_titles, title_max):\n",
    "    out = []\n",
    "    i = 0\n",
    "    while i < len(pars):\n",
    "        cur = pars[i]\n",
    "        is_title_like = merge_titles and (len(cur) <= title_max and \"\\n\" not in cur and len(cur.split()) <= 16)\n",
    "        if is_title_like and i + 1 < len(pars):\n",
    "            merged = (cur + \" ‚Äî \" + pars[i+1]).strip()\n",
    "            out.append(merged)\n",
    "            i += 2\n",
    "            continue\n",
    "        if len(cur) < min_chars and i + 1 < len(pars):\n",
    "            merged = (cur + \" \" + pars[i+1]).strip()\n",
    "            out.append(merged)\n",
    "            i += 2\n",
    "        else:\n",
    "            out.append(cur)\n",
    "            i += 1\n",
    "    return out\n",
    "\n",
    "def ensure_dir(d):\n",
    "    if not os.path.exists(d):\n",
    "        os.makedirs(d, exist_ok=True)\n",
    "\n",
    "def write_chunk(path, text):\n",
    "    ensure_dir(os.path.dirname(path))\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text.strip())\n",
    "\n",
    "def word_tokenize(text):\n",
    "    return re.findall(r\"\\S+\", text)\n",
    "\n",
    "# ===================== M√âTODO A: P√ÅRRAFOS =====================\n",
    "par_rows = []\n",
    "summary_rows = []\n",
    "\n",
    "files = [f for f in os.listdir(INPUT_DIR) if f.lower().endswith(\".txt\")]\n",
    "for fname in sorted(files):\n",
    "    path_in = os.path.join(INPUT_DIR, fname)\n",
    "    base = safe_filename_stem(fname)\n",
    "    out_dir_doc = os.path.join(OUT_PAR_DIR, base)\n",
    "    ensure_dir(out_dir_doc)\n",
    "\n",
    "    txt = read_txt(path_in)\n",
    "    pars = split_paragraphs(txt)\n",
    "    pars = fuse_short_paragraphs(pars, PAR_MIN_CHARS, MERGE_TITLES, TITLE_MAX_CHARS)\n",
    "\n",
    "    final_pars = []\n",
    "    for p in pars:\n",
    "        final_pars.extend(repartition_long_paragraph(p, PAR_MAX_CHARS))\n",
    "\n",
    "    lengths = []\n",
    "    for idx, chunk in enumerate(final_pars, start=1):\n",
    "        chunk_name = f\"chunk_{idx:04d}.txt\"\n",
    "        out_path = os.path.join(out_dir_doc, chunk_name)\n",
    "        write_chunk(out_path, chunk)\n",
    "        lengths.append(len(chunk))\n",
    "        par_rows.append({\n",
    "            \"filename_base\": base,\n",
    "            \"method\": \"paragraphs\",\n",
    "            \"chunk_id\": f\"{base}-p-{idx:04d}\",\n",
    "            \"chunk_path\": os.path.relpath(out_path, BASE_DIR).replace(\"\\\\\",\"/\"),\n",
    "            \"char_len\": len(chunk),\n",
    "            \"word_len\": len(chunk.split()),\n",
    "            \"paragraph_idx\": idx\n",
    "        })\n",
    "\n",
    "    summary_rows.append({\n",
    "        \"filename_base\": base,\n",
    "        \"method\": \"paragraphs\",\n",
    "        \"n_chunks\": len(lengths),\n",
    "        \"char_mean\": round(mean(lengths), 1) if lengths else 0,\n",
    "        \"pct_short_<300\": round(100*sum(l<300 for l in lengths)/len(lengths), 1) if lengths else 0\n",
    "    })\n",
    "\n",
    "if par_rows:\n",
    "    with open(IDX_PAR_CSV, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=list(par_rows[0].keys()))\n",
    "        writer.writeheader()\n",
    "        writer.writerows(par_rows)\n",
    "\n",
    "# ===================== M√âTODO B: VENTANAS DESLIZANTES =====================\n",
    "win_rows = []\n",
    "for fname in sorted(files):\n",
    "    path_in = os.path.join(INPUT_DIR, fname)\n",
    "    base = safe_filename_stem(fname)\n",
    "    out_dir_doc = os.path.join(OUT_WIN_DIR, base)\n",
    "    ensure_dir(out_dir_doc)\n",
    "\n",
    "    txt = read_txt(path_in)\n",
    "    words = word_tokenize(txt)\n",
    "    n = len(words)\n",
    "    lengths = []\n",
    "\n",
    "    if n == 0:\n",
    "        summary_rows.append({\n",
    "            \"filename_base\": base,\n",
    "            \"method\": \"sliding\",\n",
    "            \"n_chunks\": 0, \"char_mean\": 0, \"pct_short_<300\": 0\n",
    "            })\n",
    "        continue\n",
    "\n",
    "    idx = 0\n",
    "    win_id = 1\n",
    "    while idx < n:\n",
    "        end = min(n, idx + WIN_WORDS)\n",
    "        w_chunk = words[idx:end]\n",
    "        chunk = \" \".join(w_chunk).strip()\n",
    "        if not chunk:\n",
    "            break\n",
    "\n",
    "        chunk_name = f\"chunk_{win_id:04d}.txt\"\n",
    "        out_path = os.path.join(out_dir_doc, chunk_name)\n",
    "        write_chunk(out_path, chunk)\n",
    "\n",
    "        lengths.append(len(chunk))\n",
    "        win_rows.append({\n",
    "            \"filename_base\": base,\n",
    "            \"method\": \"sliding\",\n",
    "            \"chunk_id\": f\"{base}-w-{win_id:04d}\",\n",
    "            \"chunk_path\": os.path.relpath(out_path, BASE_DIR).replace(\"\\\\\",\"/\"),\n",
    "            \"char_len\": len(chunk),\n",
    "            \"word_len\": len(w_chunk),\n",
    "            \"start_word\": idx,\n",
    "            \"end_word\": end\n",
    "        })\n",
    "\n",
    "        win_id += 1\n",
    "        if end == n:\n",
    "            break\n",
    "        idx += WIN_STRIDE\n",
    "\n",
    "    summary_rows.append({\n",
    "        \"filename_base\": base,\n",
    "        \"method\": \"sliding\",\n",
    "        \"n_chunks\": len(lengths),\n",
    "        \"char_mean\": round(mean(lengths), 1) if lengths else 0,\n",
    "        \"pct_short_<300\": round(100*sum(l<300 for l in lengths)/len(lengths), 1) if lengths else 0\n",
    "    })\n",
    "\n",
    "if win_rows:\n",
    "    with open(IDX_WIN_CSV, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=list(win_rows[0].keys()))\n",
    "        writer.writeheader()\n",
    "        writer.writerows(win_rows)\n",
    "\n",
    "# ===================== RESUMEN =====================\n",
    "if summary_rows:\n",
    "    with open(SUMMARY_CSV, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=list(summary_rows[0].keys()))\n",
    "        writer.writeheader()\n",
    "        writer.writerows(summary_rows)\n",
    "\n",
    "print(\"‚úÖ Segmentaci√≥n regenerada con par√°metros ajustados.\")\n",
    "print(f\" - √çndice p√°rrafos:  {IDX_PAR_CSV}\")\n",
    "print(f\" - √çndice ventanas:  {IDX_WIN_CSV}\")\n",
    "print(f\" - Resumen:          {SUMMARY_CSV}\")\n",
    "print(f\" - Carpeta chunks A: {OUT_PAR_DIR}\")\n",
    "print(f\" - Carpeta chunks B: {OUT_WIN_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b23516",
   "metadata": {},
   "source": [
    "# Fase 3. Tokenizaci√≥n y Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a786f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documentos (unique filename_base): 46\n",
      "Total de chunks en √≠ndice: 386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ingestando chunks en Chroma: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 386/386 [00:09<00:00, 39.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Embeddings generados e indexados.\n",
      "üìö Collection: ai_apuntes_sliding_openai_v1 | count = 386\n",
      "üíæ Persist dir: data/vectorstores/chroma_sliding_openai_v1\n"
     ]
    }
   ],
   "source": [
    "import os, csv, time, math\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# (A) ‚Äî‚Äî CONFIGURACI√ìN GENERAL \n",
    "BASE_DIR       = \"data\"\n",
    "INDEX_CSV      = os.path.join(BASE_DIR, \"chunks_sliding\", \"index_sliding.csv\")   # puedes cambiar a p√°rrafos si quieres\n",
    "PERSIST_DIR    = os.path.join(BASE_DIR, \"vectorstores\", \"chroma_sliding_openai_v1\")\n",
    "COLLECTION_NAME= \"ai_apuntes_sliding_openai_v1\"\n",
    "\n",
    "# Proveedor de embeddings: \"openai\" o \"local\"\n",
    "PROVIDER       = \"openai\"       \n",
    "OPENAI_MODEL   = \"text-embedding-3-small\"      \n",
    "LOCAL_MODEL    = \"all-MiniLM-L6-v2\"           \n",
    "\n",
    "BATCH_SIZE     = 128            # tama√±o de lote para ingesti√≥n\n",
    "MAX_RETRIES    = 5              # reintentos por rate-limit/errores transitorios\n",
    "RETRY_BASE_SEC = 2              # backoff exponencial\n",
    "\n",
    "# (B) ‚Äî‚Äî SETUP DE EMBEDDINGS \n",
    "embed_dims = None\n",
    "\n",
    "if PROVIDER == \"openai\":\n",
    "    from openai import OpenAI\n",
    "    OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "    if not OPENAI_API_KEY:\n",
    "        raise ValueError(\"La variable de entorno OPENAI_API_KEY no est√° configurada.\")\n",
    "    \n",
    "    oai_client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "    def embed_texts(texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"Embeddings con OpenAI + reintentos.\"\"\"\n",
    "        for attempt in range(MAX_RETRIES):\n",
    "            try:\n",
    "                resp = oai_client.embeddings.create(\n",
    "                    model=OPENAI_MODEL,\n",
    "                    input=texts\n",
    "                )\n",
    "                vecs = [d.embedding for d in resp.data]\n",
    "                return vecs\n",
    "            except Exception as e:\n",
    "                wait = RETRY_BASE_SEC * (2 ** attempt)\n",
    "                print(f\"[WARN] Error {e}. Reintentando en {wait}s...\")\n",
    "                if attempt == MAX_RETRIES - 1:\n",
    "                    raise\n",
    "                time.sleep(wait)\n",
    "\n",
    "elif PROVIDER == \"local\":\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    st_model = SentenceTransformer(LOCAL_MODEL)\n",
    "\n",
    "    def embed_texts(texts: List[str]) -> List[List[float]]:\n",
    "        return st_model.encode(texts, convert_to_numpy=False, normalize_embeddings=False).tolist()\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"PROVIDER debe ser 'openai' o 'local'.\")\n",
    "\n",
    "# (C) ‚Äî‚Äî INICIALIZAR CHROMA PERSISTENTE \n",
    "os.makedirs(PERSIST_DIR, exist_ok=True)\n",
    "\n",
    "client = chromadb.PersistentClient(\n",
    "    path=PERSIST_DIR,\n",
    "    settings=Settings(is_persistent=True)\n",
    ")\n",
    "\n",
    "# Crear o recuperar la colecci√≥n\n",
    "try:\n",
    "    collection = client.get_collection(COLLECTION_NAME)\n",
    "except:\n",
    "    collection = client.create_collection(\n",
    "        name=COLLECTION_NAME,\n",
    "        metadata={\"hnsw:space\": \"cosine\"}  # distancia coseno\n",
    "    )\n",
    "\n",
    "# (D) ‚Äî‚Äî UTILIDADES \n",
    "def read_index_rows(csv_path: str):\n",
    "    rows = []\n",
    "    with open(csv_path, \"r\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for r in reader:\n",
    "            rows.append(r)\n",
    "    return rows\n",
    "\n",
    "def load_chunk_text(chunk_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Abre el archivo de texto del chunk, corrigiendo rutas relativas.\n",
    "    \"\"\"\n",
    "    # Si el path ya incluye \"data/\", se queda tal cual\n",
    "    if not os.path.isabs(chunk_path):\n",
    "        # Si empieza por \"data/\", lo consideramos relativo al proyecto\n",
    "        if chunk_path.startswith(\"data/\") or chunk_path.startswith(\".\\\\data\\\\\") or chunk_path.startswith(\".\\\\chunks_\"):\n",
    "            path = os.path.normpath(chunk_path)\n",
    "        else:\n",
    "            # Si viene solo 'chunks_sliding/...', le anteponemos 'data/'\n",
    "            path = os.path.join(\"data\", chunk_path)\n",
    "    else:\n",
    "        path = chunk_path\n",
    "\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"No se encontr√≥ el archivo: {path}\")\n",
    "\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        return f.read().strip()\n",
    "\n",
    "\n",
    "# (E) ‚Äî‚Äî CARGAR √çNDICE Y PREPARAR INGESTA \n",
    "rows = read_index_rows(INDEX_CSV)\n",
    "print(f\"Documentos (unique filename_base): {len(set(r['filename_base'] for r in rows))}\")\n",
    "print(f\"Total de chunks en √≠ndice: {len(rows)}\")\n",
    "\n",
    "# (F) ‚Äî‚Äî INGESTA EN LOTES CON EMBEDDINGS \n",
    "ids, docs, metas = [], [], []\n",
    "\n",
    "def flush_batch():\n",
    "    if not ids:\n",
    "        return\n",
    "    # Calcula embeddings del batch actual\n",
    "    vecs = embed_texts(docs)\n",
    "    # upsert = idempotente: si ya existe el id, lo actualiza\n",
    "    collection.upsert(ids=ids, documents=docs, metadatas=metas, embeddings=vecs)\n",
    "    ids.clear(); docs.clear(); metas.clear()\n",
    "\n",
    "for r in tqdm(rows, desc=\"Ingestando chunks en Chroma\"):\n",
    "    chunk_id   = r[\"chunk_id\"]              # ej: <base>-w-0001\n",
    "    chunk_path = r[\"chunk_path\"]            # ej: data/chunks_sliding/<base>/chunk_0001.txt\n",
    "    text       = load_chunk_text(chunk_path)\n",
    "    if not text:\n",
    "        continue\n",
    "\n",
    "    ids.append(chunk_id)\n",
    "    docs.append(text)\n",
    "    metas.append({\n",
    "        \"filename_base\": r.get(\"filename_base\", \"\"),\n",
    "        \"method\":       r.get(\"method\", \"sliding\"),\n",
    "        \"chunk_path\":   r.get(\"chunk_path\", \"\"),\n",
    "        \"char_len\":     int(r.get(\"char_len\", 0)),\n",
    "        \"word_len\":     int(r.get(\"word_len\", 0)),\n",
    "        \"start_word\":   int(r.get(\"start_word\", 0)),\n",
    "        \"end_word\":     int(r.get(\"end_word\", 0)),\n",
    "    })\n",
    "\n",
    "    if len(ids) >= BATCH_SIZE:\n",
    "        flush_batch()\n",
    "\n",
    "# √∫ltimo lote\n",
    "flush_batch()\n",
    "\n",
    "print(\"‚úÖ Embeddings generados e indexados.\")\n",
    "print(\"üìö Collection:\", COLLECTION_NAME, \"| count =\", collection.count())\n",
    "print(\"üíæ Persist dir:\", PERSIST_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647db0fa",
   "metadata": {},
   "source": [
    "## Prueba de Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12b5eda5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#1  dist=0.5520  base=3_Semana_AI_20250821_1_JulioVarelaVenegas_AlgebraLinealYProgramacionVectorial  palabras=240\n",
      "chunks_sliding/3_Semana_AI_20250821_1_JulioVarelaVenegas_AlgebraLinealYProgramacionVectorial/chunk_0004.txt\n",
      "los vectores son la base para representar conceptos en IA. III-C. Magnitud y Distancias La magnitud indica la distancia entre el punto inicial y final del vector, calculada con la norma ‚à•v‚à•. Distancia Manhattan (L1): suma de los valores abso- lutos de las diferencias en cada eje: d(x,y) =nX i=1|xi‚àíyi| Distancia Euclidiana (L2): hipotenusa del triangulo formado por los vectores: d(x,y) =vuutnX i=1(xi‚àíyi)2 Figura 2. Distancia Euclidiana entre dos puntos. III-D. Propiedades de la Norma La norma cum...\n",
      "\n",
      "#2  dist=0.5734  base=3_Semana_AI_20250819_2_MarianaQuesadaSanchez_AlgebraLinealYAprendizajeSupervisado  palabras=240\n",
      "chunks_sliding/3_Semana_AI_20250819_2_MarianaQuesadaSanchez_AlgebraLinealYAprendizajeSupervisado/chunk_0003.txt\n",
      "Representacion grafica del vector (4,3)en el plano cartesiano. B. Norma o magnitud La norma mide longitud de un vector y se denota como ‚à•x‚à•. Geometricamente, puede interpretarse como la distancia desde el punto de origen hasta el punto final definido porx. De esta manera, la norma proporciona una medida cuantitativa de la magnitud del vector, independientemente de su direccion. Para un vector x= (x1, x2, . . . , x n), las normas mas comunes son: ‚Ä¢Norma L1 o Manhattan: La distancia Manhattan entr...\n",
      "\n",
      "#3  dist=0.5866  base=3_Semana_AI_20250819_1_JavierRojasRojas_VectoresYAprendizajeSupervisado  palabras=240\n",
      "chunks_sliding/3_Semana_AI_20250819_1_JavierRojasRojas_VectoresYAprendizajeSupervisado/chunk_0005.txt\n",
      "las frases en vectores y midiendo eseangulo . 1)Ejemplo de calculo deangulo entre dos vectores :Se consideran los vectores u= (1,2)yv= (3,4). Paso 1: Calcular el producto punto u¬∑v= 1¬∑3 + 2¬∑4 = 3 + 8 = 11 Paso 2: Calcular las normas ‚à•u‚à•=p 12+ 22=‚àö 5,‚à•v‚à•=p 32+ 42=‚àö 25 = 5 Paso 3: Aplicar la identidad del coseno cos(Œ∏) =u¬∑v ‚à•u‚à• ¬∑ ‚à•v‚à•=11‚àö 5¬∑5=11 5‚àö 5‚âà0.9839 Paso 4: Calcular elangulo Œ∏= arccos(0 .9839) ‚âà10.3‚ó¶ G.Vector co-direccional Son dos vectores que siguen la misma direccion, pero con magnitudes...\n",
      "\n",
      "#4  dist=0.5922  base=3_Semana_AI_20250819_2_MarianaQuesadaSanchez_AlgebraLinealYAprendizajeSupervisado  palabras=240\n",
      "chunks_sliding/3_Semana_AI_20250819_2_MarianaQuesadaSanchez_AlgebraLinealYAprendizajeSupervisado/chunk_0004.txt\n",
      "3)Desigualdad triangular: ‚à•x+y‚à• ‚â§ ‚à• x‚à•+‚à•y‚à•para todos los vectores xey. C. Vectores unitarios Un vector unitario es aquel cuya norma es igual a uno. Se obtiene normalizando un vector vmediante su magnitud: u=v ‚à•v‚à• De esta manera, uconserva la direccion de v, pero con longitud unitaria. Fig. 3. Vector unitario. [2] D. Producto punto El producto punto entre dos vectores es la suma de las multiplicaciones de sus componentes, lo que produce un valor real. Esta operacion es fundamental en inteligencia...\n",
      "\n",
      "#5  dist=0.5964  base=3_Semana_AI_20250819_1_JavierRojasRojas_VectoresYAprendizajeSupervisado  palabras=240\n",
      "chunks_sliding/3_Semana_AI_20250819_1_JavierRojasRojas_VectoresYAprendizajeSupervisado/chunk_0003.txt\n",
      "Lanorma de un vector representa su magnitud, es decir, que tan grande es o cuanto mide desde el punto de inicio hasta el punto final. Se denota como ‚à•x‚à•. Existen diferentes formas de medir la norma de un vector: 1)Norma L1: Distancia Manhattan :Ladistancia Man- hattan entre dos puntos se calcula sumando las distancias absolutas de cada componente, desplazandoseunicamente a lo largo de los ejes del espacio desde el punto de inicio hasta el punto final. Esta metrica se utiliza especialmente cuando...\n"
     ]
    }
   ],
   "source": [
    "# Tooo este c√≥digo era una prueba para revisar si los embeddings se hab√≠an hecho bien\n",
    "\n",
    "from openai import OpenAI\n",
    "api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "oai_client = OpenAI(api_key=api_key)\n",
    "OPENAI_MODEL = \"text-embedding-3-small\"\n",
    "\n",
    "QUESTION = \"¬øCual es la formula de la distancia euclidiana?\"\n",
    "TOP_K = 5\n",
    "\n",
    "# 1) Embeber la pregunta con OpenAI\n",
    "qvec = oai_client.embeddings.create(\n",
    "    model=OPENAI_MODEL,\n",
    "    input=QUESTION\n",
    ").data[0].embedding\n",
    "\n",
    "# 2) Consultar \n",
    "res = collection.query(\n",
    "    query_embeddings=[qvec],\n",
    "    n_results=TOP_K,\n",
    "    include=[\"metadatas\", \"distances\", \"documents\"]\n",
    ")\n",
    "\n",
    "for rank, (doc, meta, dist) in enumerate(zip(res[\"documents\"][0],\n",
    "                                             res[\"metadatas\"][0],\n",
    "                                             res[\"distances\"][0]), start=1):\n",
    "    print(f\"\\n#{rank}  dist={dist:.4f}  base={meta.get('filename_base')}  palabras={meta.get('word_len')}\")\n",
    "    print(meta.get(\"chunk_path\"))\n",
    "    print(doc[:500].replace(\"\\n\",\" \") + (\"...\" if len(doc)>500 else \"\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49b0e44",
   "metadata": {},
   "source": [
    "# Fase 4. Herramientas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94564b0",
   "metadata": {},
   "source": [
    "# RAG Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d604e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import BaseTool\n",
    "from typing import Any, Type\n",
    "from pydantic import BaseModel, Field, PrivateAttr\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "# ======== MODELO DE INPUT ========\n",
    "class RAGInput(BaseModel):\n",
    "    query: str = Field(..., description=\"Pregunta o frase a buscar en los apuntes\")\n",
    "\n",
    "# ======== TOOL ========\n",
    "class RAGTool(BaseTool):\n",
    "    name: str = \"RAG_Tool\"\n",
    "    description: str = \"Busca informaci√≥n en la base vectorial local de apuntes de Inteligencia Artificial\"\n",
    "    args_schema: Type[BaseModel] = RAGInput\n",
    "\n",
    "    _client: Any = PrivateAttr()\n",
    "    _collection: Any = PrivateAttr()\n",
    "    _oai_client: Any = PrivateAttr()\n",
    "\n",
    "    def __init__(self, collection_name: str, persist_dir: str, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self._client = chromadb.PersistentClient(path=persist_dir, settings=Settings(is_persistent=True))\n",
    "        self._collection = self._client.get_collection(collection_name)\n",
    "        self._oai_client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "    def _run(self, query: str):\n",
    "        \"\"\"Consulta la base vectorial usando embeddings OpenAI\"\"\"\n",
    "        embedding = self._oai_client.embeddings.create(\n",
    "            model=\"text-embedding-3-small\",\n",
    "            input=query\n",
    "        ).data[0].embedding\n",
    "\n",
    "        results = self._collection.query(\n",
    "            query_embeddings=[embedding],\n",
    "            n_results=5\n",
    "        )\n",
    "        docs = results[\"documents\"][0]\n",
    "        metas = results[\"metadatas\"][0]\n",
    "\n",
    "        combined = [\n",
    "            f\"Documento: {m.get('filename_base','')} | Fragmento: {d[:300]}...\"\n",
    "            for d, m in zip(docs, metas)\n",
    "        ]\n",
    "        return \"\\n\".join(combined)\n",
    "\n",
    "    async def _arun(self, query: str):\n",
    "        raise NotImplementedError(\"RAGTool no soporta ejecuci√≥n as√≠ncrona.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba91464",
   "metadata": {},
   "source": [
    "## Mini-Prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7302470b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Consulta: ¬øQu√© es una red neuronal convolucional?\n",
      "------------------------------------------------------\n",
      "üìò Resultados:\n",
      "Documento: 10_Semana_AI_20251007_1_GianmarcoOportaPerez_RedesNeuronalesConvolucionales | Fragmento: avanzan a traves de las capas convolucionales y de agrupamiento, se reduce su tama√±o espacial, pero aumenta la abstraccion de las caracteristicas aprendidas. Fig. 1. redes convolucionales C. Aplicaciones Comunes Las redes convolucionales se aplican ampliamente en diver- sas tareas de vision artifici...\n",
      "Documento: 11_Semana_AI_20251014_2_ LuisFernandoBenavidesVillegas_ConvolucionesPoolingAutoencoders | Fragmento: de un bloque, promoviendo la reutilizacion de carac- teristicas y reduciendo la redundancia. Esta estructura densa mejora la propagacion del gradiente, optimiza la eficiencia del modelo y mantiene un numero reducido de parametros. II. PROBLEMAS CON LASREDESNEURONALES CONVOLUCIONALES A pesar de su al...\n",
      "Documento: 8_Semana_AI_20250923_2_FabianDiazBarboza_RedesNeuronalesFullyConnected | Fragmento: ReLU, etc.) permiten que la red modelice relaciones no lineales entre entradas y salidas. 2.Capasyprofundidad:amayorprofundidad,mayor capacidad para representar abstracciones jerarquicas. 3.Diferenciabilidad:la diferenciabilidad de las fun- ciones internas es requisito para aplicar retropropaga- cio...\n",
      "Documento: 11_Semana_AI_20251014_1_JuanDiegoJimenezValverde_CNNYAutoencoders | Fragmento: final del modelo. G. Arquitecturas Convolucionales Una red convolucional combina secuencias deconvolucion ‚Üíactivacion (ReLU)‚Üípooling. Este patron se repite varias veces para extraer informacion progresivamente mas abstracta de la imagen. Generalmente, se prefieren filtros peque√±os (como3√ó3) para cap...\n",
      "Documento: 8_Semana_AI_20250923_1_BrandonEmmanuelSanchezAraya_RedesNeuronalesMNIST | Fragmento: neurona/clase tiene su propio sesgo:b= (b0, . . . , b 9)‚ä§.Cantidad de neuronas=tama√±o deb. IV. EJERCICIO:DE VECTOR A MATRIZ 1) Una sola regresion binaria (vectorx) Sea x=Ô£Æ Ô£ØÔ£ØÔ£∞3 4 5 6Ô£π Ô£∫Ô£∫Ô£ª, w=Ô£Æ Ô£ØÔ£ØÔ£∞3 2 4 5Ô£π Ô£∫Ô£∫Ô£ª, b= 2. Entonces z=w‚ä§x+b= [ 3 2 4 5 ]Ô£Æ Ô£ØÔ£ØÔ£∞3 4 5 6Ô£π Ô£∫Ô£∫Ô£ª+2 = 67+2 = 69,y=œÉ(z). 2) Varias regre...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "collection_name = \"ai_apuntes_sliding_openai_v1\"\n",
    "persist_dir = \"data/vectorstores/chroma_sliding_openai_v1\"\n",
    "\n",
    "# ======== INICIALIZAR TOOL ========\n",
    "rag_tool = RAGTool(collection_name=collection_name, persist_dir=persist_dir)\n",
    "\n",
    "# ======== PRUEBAS ========\n",
    "query = \"¬øQu√© es una red neuronal convolucional?\"\n",
    "print(\"üîç Consulta:\", query)\n",
    "print(\"------------------------------------------------------\")\n",
    "\n",
    "response = rag_tool._run(query)\n",
    "\n",
    "print(\"üìò Resultados:\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6221620",
   "metadata": {},
   "source": [
    "# Web Search Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ab69b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import Tool\n",
    "from langchain_tavily import TavilySearch\n",
    "\n",
    "def get_websearch_tool():\n",
    "    search = TavilySearch(max_results=5)\n",
    "    return Tool(\n",
    "        name=\"WebSearch_Tool\",\n",
    "        description=\"Busca informaci√≥n en Internet usando Tavily\",\n",
    "        func=search.run\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de27a66a",
   "metadata": {},
   "source": [
    "## Mini-Prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfdf15c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Consulta: √öltimos avances en inteligencia artificial en 2025\n",
      "------------------------------------------------------\n",
      "üåê Resultados:\n",
      "√öltimos avances en inteligencia artificial en 2025\n",
      "{'query': '√öltimos avances en inteligencia artificial en 2025', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://www.bbc.com/mundo/articles/c4gxzx0kpp6o', 'title': 'IA: Qu√© se espera en 2025 de la inteligencia artificial, el ...', 'content': '**La inteligencia artificial (IA) est√° marcando un antes y un despu√©s en la historia de la tecnolog√≠a, y 2025 traer√° m√°s sorpresas.** Un \"centauro doctor + un sistema de IA\" mejora las decisiones que toman los humanos por su cuenta y los sistemas de IA por la suya. Los agentes de IA aut√≥nomos basados en modelos de lenguaje son el objetivo para 2025 de las grandes empresas tecnol√≥gicas como OpenAI (ChatGPT), Meta (LLaMA), Google (Gemini) o Anthropic (Claude). Nos enfrentamos a un dilema crucial: saber cu√°ndo es mejor ser \"autom√°ticos\" en el uso de agentes de IA aut√≥nomos y cu√°ndo necesitamos tomar la decisi√≥n, es decir, recurrir \"al control humano\" o la \"interacci√≥n humano e IA\".', 'score': 0.98598, 'raw_content': None}, {'url': 'https://www.next-step.es/tendencias-y-retos-de-la-ia-en-2025/', 'title': 'Tendencias y Retos de la IA en 2025', 'content': 'Las tendencias emergentes apuntan a una nueva realidad: Asistentes de IA que ser√°n capaces de anticiparse a las necesidades y ofrecer la soluci√≥n a un problema, antes incluso de formular la pregunta; que realizar√°n operaciones empresariales complejas, de forma totalmente automatizada; sistemas inteligentes que toman decisiones de forma aut√≥noma y en tiempo real y hasta algoritmos que gestionar√°n los recursos de forma totalmente eficiente, para impulsar la sostenibilidad. Por ello, el principal reto de las empresas en 2025 va a ser implementar la automatizaci√≥n con los agentes de IA, para ser m√°s competitivas, pero aplicando criterios √©ticos para no desplazar a las personas, sino empoderarlas. **La gobernanza de la inteligencia artificial (IA)** es el pilar fundamental para asegurar que esta tecnolog√≠a transformadora se desarrolle, implemente y utilice de manera responsable, √©tica y efectiva.', 'score': 0.9845, 'raw_content': None}, {'url': 'https://mexico.unir.net/noticias/ingenieria/avances-inteligencia-artificial/', 'title': '√öltimos avances de la inteligencia artificial, lo m√°s ...', 'content': 'Descubre los avances m√°s innovadores en inteligencia artificial y c√≥mo est√°n transformando el mundo en la actualidad y lo que viene. Ahora mismo, los **avances de la inteligencia artificial** en el mundo acad√©mico hacen una diferencia para estudiantes, profesores y poblaci√≥n en general por igual. Los generadores con IA que mencionamos al inicio son **avances actuales de la inteligencia artificial** que sirven para elaborar material audiovisual y composiciones musicales. Adem√°s, en el contexto de mejoras a futuro, se incluyen los **avances de la inteligencia artificial en M√©xico**. Teniendo en cuenta las innovaciones de la IA, parece que apenas estamos viendo la punta del iceberg y que la inteligencia artificial invadir√° todo lo que conocemos.', 'score': 0.97616, 'raw_content': None}, {'url': 'https://www.infobae.com/tecno/2025/08/28/inteligencia-artificial-en-2025-cinco-cosas-que-todos-deberiamos-conocer-ya-mismo-para-salvarnos/', 'title': 'Inteligencia artificial en 2025: cinco cosas que todos ...', 'content': '1. La IA generativa es cada vez m√°s realista ¬∑ 2. Las ‚Äúalucinaciones‚Äù no desaparecer√°n ¬∑ 3. El costo energ√©tico es un desaf√≠o creciente ¬∑ 4. Nadie', 'score': 0.97425, 'raw_content': None}, {'url': 'https://www.hostinger.com/es/tutoriales/estadisticas-y-tendencias-de-ia', 'title': 'IA: 28 estad√≠sticas y tendencias en 2025', 'content': 'Las 10 estad√≠sticas de la inteligencia artificial que debes conocer en 2025 ¬∑ Se espera que el mercado mundial de la IA crezca un 37% anual. ¬∑ De', 'score': 0.97048, 'raw_content': None}], 'response_time': 0.0, 'request_id': 'c0d68a03-cf9b-4afd-9dbb-cec8732184de'}\n",
      "\n",
      "--- Resultado 1 ---\n",
      "T√≠tulo: IA: Qu√© se espera en 2025 de la inteligencia artificial, el ...\n",
      "URL: https://www.bbc.com/mundo/articles/c4gxzx0kpp6o\n",
      "Contenido: **La inteligencia artificial (IA) est√° marcando un antes y un despu√©s en la historia de la tecnolog√≠a, y 2025 traer√° m√°s sorpresas.** Un \"centauro doctor + un sistema de IA\" mejora las decisiones que toman los humanos por su cuenta y los sistemas de IA por la suya. Los agentes de IA aut√≥nomos basados en modelos de lenguaje son el objetivo para 2025 de las grandes empresas tecnol√≥gicas como OpenAI (ChatGPT), Meta (LLaMA), Google (Gemini) o Anthropic (Claude). Nos enfrentamos a un dilema crucial: saber cu√°ndo es mejor ser \"autom√°ticos\" en el uso de agentes de IA aut√≥nomos y cu√°ndo necesitamos tomar la decisi√≥n, es decir, recurrir \"al control humano\" o la \"interacci√≥n humano e IA\".\n",
      "\n",
      "--- Resultado 2 ---\n",
      "T√≠tulo: Tendencias y Retos de la IA en 2025\n",
      "URL: https://www.next-step.es/tendencias-y-retos-de-la-ia-en-2025/\n",
      "Contenido: Las tendencias emergentes apuntan a una nueva realidad: Asistentes de IA que ser√°n capaces de anticiparse a las necesidades y ofrecer la soluci√≥n a un problema, antes incluso de formular la pregunta; que realizar√°n operaciones empresariales complejas, de forma totalmente automatizada; sistemas inteligentes que toman decisiones de forma aut√≥noma y en tiempo real y hasta algoritmos que gestionar√°n los recursos de forma totalmente eficiente, para impulsar la sostenibilidad. Por ello, el principal reto de las empresas en 2025 va a ser implementar la automatizaci√≥n con los agentes de IA, para ser m√°s competitivas, pero aplicando criterios √©ticos para no desplazar a las personas, sino empoderarlas. **La gobernanza de la inteligencia artificial (IA)** es el pilar fundamental para asegurar que esta tecnolog√≠a transformadora se desarrolle, implemente y utilice de manera responsable, √©tica y efectiva.\n",
      "\n",
      "--- Resultado 3 ---\n",
      "T√≠tulo: √öltimos avances de la inteligencia artificial, lo m√°s ...\n",
      "URL: https://mexico.unir.net/noticias/ingenieria/avances-inteligencia-artificial/\n",
      "Contenido: Descubre los avances m√°s innovadores en inteligencia artificial y c√≥mo est√°n transformando el mundo en la actualidad y lo que viene. Ahora mismo, los **avances de la inteligencia artificial** en el mundo acad√©mico hacen una diferencia para estudiantes, profesores y poblaci√≥n en general por igual. Los generadores con IA que mencionamos al inicio son **avances actuales de la inteligencia artificial** que sirven para elaborar material audiovisual y composiciones musicales. Adem√°s, en el contexto de mejoras a futuro, se incluyen los **avances de la inteligencia artificial en M√©xico**. Teniendo en cuenta las innovaciones de la IA, parece que apenas estamos viendo la punta del iceberg y que la inteligencia artificial invadir√° todo lo que conocemos.\n",
      "\n",
      "--- Resultado 4 ---\n",
      "T√≠tulo: Inteligencia artificial en 2025: cinco cosas que todos ...\n",
      "URL: https://www.infobae.com/tecno/2025/08/28/inteligencia-artificial-en-2025-cinco-cosas-que-todos-deberiamos-conocer-ya-mismo-para-salvarnos/\n",
      "Contenido: 1. La IA generativa es cada vez m√°s realista ¬∑ 2. Las ‚Äúalucinaciones‚Äù no desaparecer√°n ¬∑ 3. El costo energ√©tico es un desaf√≠o creciente ¬∑ 4. Nadie\n",
      "\n",
      "--- Resultado 5 ---\n",
      "T√≠tulo: IA: 28 estad√≠sticas y tendencias en 2025\n",
      "URL: https://www.hostinger.com/es/tutoriales/estadisticas-y-tendencias-de-ia\n",
      "Contenido: Las 10 estad√≠sticas de la inteligencia artificial que debes conocer en 2025 ¬∑ Se espera que el mercado mundial de la IA crezca un 37% anual. ¬∑ De\n"
     ]
    }
   ],
   "source": [
    "# ======== Inicializar Tool ========\n",
    "web_tool = get_websearch_tool()\n",
    "\n",
    "# ======== Consulta ========\n",
    "query = \"√öltimos avances en inteligencia artificial en 2025\"\n",
    "print(\"üîç Consulta:\", query)\n",
    "print(\"------------------------------------------------------\")\n",
    "\n",
    "# Ejecutar b√∫squeda\n",
    "response = web_tool.run(query)\n",
    "\n",
    "print(\"üåê Resultados:\")\n",
    "print(response['query'])\n",
    "print(response)\n",
    "for i in range(len(response['results'])):\n",
    "    print(f\"\\n--- Resultado {i+1} ---\")\n",
    "    print(\"T√≠tulo:\", response['results'][i]['title'])\n",
    "    print(\"URL:\", response['results'][i]['url'])\n",
    "    print(\"Contenido:\", response['results'][i]['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63b96bb",
   "metadata": {},
   "source": [
    "# Fase 5.  Perfil, orquestacion y memoria del agente LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83fc97e",
   "metadata": {},
   "source": [
    "# Fase 6. Aplicaci√≥n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
