{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10918002",
   "metadata": {},
   "source": [
    "# Fase 1. Recolecci√≥n y organizaci√≥n de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be5f26d",
   "metadata": {},
   "source": [
    "En docs \"lineamientos_nomenclatura_y_citacion.md\" est√° la informaci√≥n por si quieren guiarse y revisar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c92d8c2",
   "metadata": {},
   "source": [
    "# Fase 2. Procesamiento de texto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9be9e59",
   "metadata": {},
   "source": [
    "## Extracci√≥n del texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "073fa793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 46 archivos extra√≠dos correctamente a data/apuntes_clean/raw\n"
     ]
    }
   ],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "import os, re\n",
    "\n",
    "# Directorio base del proyecto\n",
    "BASE_DIR = \"data\"\n",
    "RAW_PDF_DIR = os.path.join(BASE_DIR, \"apuntes_raw\")\n",
    "OUT_RAW_DIR = os.path.join(BASE_DIR, \"apuntes_clean\", \"raw\")\n",
    "\n",
    "# Crear la carpeta de salida si no existe\n",
    "os.makedirs(OUT_RAW_DIR, exist_ok=True)\n",
    "\n",
    "# Buscar todos los PDFs en la carpeta apuntes_raw\n",
    "pdf_files = [f for f in os.listdir(RAW_PDF_DIR) if f.lower().endswith(\".pdf\")]\n",
    "\n",
    "for pdf_file in pdf_files:\n",
    "    pdf_path = os.path.join(RAW_PDF_DIR, pdf_file)\n",
    "    txt_name = os.path.splitext(pdf_file)[0] + \".txt\"\n",
    "    out_txt_path = os.path.join(OUT_RAW_DIR, txt_name)\n",
    "\n",
    "    reader = PdfReader(pdf_path)\n",
    "    text_pages = []\n",
    "    for page in reader.pages:\n",
    "        text_pages.append(page.extract_text() or \"\")\n",
    "    \n",
    "    # Unir todas las p√°ginas con doble salto de l√≠nea\n",
    "    full_text = \"\\n\\n\".join(text_pages)\n",
    "    with open(out_txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(full_text)\n",
    "\n",
    "print(f\"‚úÖ {len(pdf_files)} archivos extra√≠dos correctamente a {OUT_RAW_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ac2e1a",
   "metadata": {},
   "source": [
    "## Normalizaci√≥n de txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9619af32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 46 archivos corregidos y guardados en data/apuntes_clean/normalized\n"
     ]
    }
   ],
   "source": [
    "import os, re, unicodedata\n",
    "\n",
    "BASE_DIR = \"data\"\n",
    "CLEAN_IN_DIR  = os.path.join(BASE_DIR, \"apuntes_clean\", \"raw\")        # carpeta de entrada\n",
    "CLEAN_OUT_DIR = os.path.join(BASE_DIR, \"apuntes_clean\", \"normalized\") # carpeta de salida\n",
    "os.makedirs(CLEAN_OUT_DIR, exist_ok=True)\n",
    "\n",
    "def quitar_tildes_y_reparar_espacios(texto: str) -> str:\n",
    "    # 1) Normalizaci√≥n Unicode para exponer diacr√≠ticos combinantes\n",
    "    t = unicodedata.normalize(\"NFD\", texto)\n",
    "\n",
    "    # 2) Reemplazos t√≠picos de PDF \n",
    "    t = (t.replace(\"\\u00A0\", \" \")      # NBSP -> espacio normal\n",
    "           .replace(\"\\u00AD\", \"\")      # soft hyphen -> nada\n",
    "           .replace(\"Ô¨Å\", \"fi\").replace(\"Ô¨Ç\", \"fl\")  # ligaduras\n",
    "           .replace(\"\\u0131\", \"i\")     # ƒ± (i sin punto) -> i\n",
    "           .replace(\"Àô\", \"\").replace(\"`\", \"\").replace(\"¬®\", \"\").replace(\"ÀÜ\", \"\"))\n",
    "\n",
    "    # 3) Unir SOLO cuando hay acento suelto entre letras: \"implementaci ¬¥on\" -> \"implementacion\"\n",
    "    t = re.sub(r\"([A-Za-z√±√ë])\\s*[\\u00B4\\u0301]\\s*([A-Za-z√±√ë])\", r\"\\1\\2\", t)\n",
    "\n",
    "    # 4) Convertir virgulilla suelta (~ o \\u02DC) en √ë/√± cuando corresponde (p.ej. \"tama Àúno\" -> \"tama√±o\")\n",
    "    #    a) letra + ~ + n/N\n",
    "    t = re.sub(r\"([A-Za-z√±√ë])\\s*[\\u02DC~]\\s*([Nn])\",\n",
    "               lambda m: m.group(1) + (\"√ë\" if m.group(2).isupper() else \"√±\"),\n",
    "               t)\n",
    "    #    b) ~ al inicio o tras espacio antes de n/N + vocal (p.ej. \" Àúno \" -> \" √±o \")\n",
    "    t = re.sub(r\"(?<!\\S)[\\u02DC~]\\s*([Nn])(?=[aeiou√°√©√≠√≥√∫AEIOU√Å√â√ç√ì√ö])\",\n",
    "               lambda m: (\"√ë\" if m.group(1).isupper() else \"√±\"),\n",
    "               t)\n",
    "\n",
    "    # 5) Eliminar diacr√≠ticos (tildes) PERO conservar √±/√ë\n",
    "    t = ''.join(c for c in t if unicodedata.category(c) != 'Mn' or c.lower() == '√±')\n",
    "\n",
    "    # 6) Limpieza suave: colapsar espacios repetidos y limitar saltos\n",
    "    t = re.sub(r\"[ \\t]+\", \" \", t)\n",
    "    t = re.sub(r\"\\n{3,}\", \"\\n\\n\", t)\n",
    "\n",
    "    return t.strip()\n",
    "\n",
    "# Aplicar a todos los .txt de entrada\n",
    "count = 0\n",
    "for fname in os.listdir(CLEAN_IN_DIR):\n",
    "    if not fname.lower().endswith(\".txt\"):\n",
    "        continue\n",
    "\n",
    "    with open(os.path.join(CLEAN_IN_DIR, fname), \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        raw = f.read()\n",
    "\n",
    "    norm = quitar_tildes_y_reparar_espacios(raw)\n",
    "\n",
    "    with open(os.path.join(CLEAN_OUT_DIR, fname), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(norm)\n",
    "    count += 1\n",
    "\n",
    "print(f\"‚úÖ {count} archivos corregidos y guardados en {CLEAN_OUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31180d1",
   "metadata": {},
   "source": [
    "## Segmentaci√≥n (P√°rrafos y Ventanas Deslizantes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f353734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Segmentaci√≥n regenerada con par√°metros ajustados.\n",
      " - √çndice p√°rrafos:  data/chunks_paragraphs/index_paragraphs.csv\n",
      " - √çndice ventanas:  data/chunks_sliding/index_sliding.csv\n",
      " - Resumen:          data/chunks_summary.csv\n",
      " - Carpeta chunks A: data/chunks_paragraphs\n",
      " - Carpeta chunks B: data/chunks_sliding\n"
     ]
    }
   ],
   "source": [
    "import os, re, csv, shutil\n",
    "from statistics import mean\n",
    "\n",
    "BASE_DIR = \"data\"\n",
    "INPUT_DIR = os.path.join(BASE_DIR, \"apuntes_clean\", \"normalized\")\n",
    "\n",
    "# --- P√°rrafos ---\n",
    "PAR_MIN_CHARS = 480\n",
    "PAR_MAX_CHARS = 2000\n",
    "MERGE_TITLES = True\n",
    "TITLE_MAX_CHARS = 140\n",
    "\n",
    "# --- Ventanas deslizantes ---\n",
    "WIN_WORDS = 240\n",
    "WIN_OVERLAP = 0.20\n",
    "WIN_STRIDE = max(1, int(WIN_WORDS * (1 - WIN_OVERLAP)))\n",
    "\n",
    "# --- Salidas ---\n",
    "OUT_PAR_DIR = os.path.join(BASE_DIR, \"chunks_paragraphs\")\n",
    "OUT_WIN_DIR = os.path.join(BASE_DIR, \"chunks_sliding\")\n",
    "\n",
    "# Limpiar salidas anteriores para √≠ndices consistentes\n",
    "for d in (OUT_PAR_DIR, OUT_WIN_DIR):\n",
    "    if os.path.exists(d):\n",
    "        shutil.rmtree(d)\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "IDX_PAR_CSV = os.path.join(OUT_PAR_DIR, \"index_paragraphs.csv\")\n",
    "IDX_WIN_CSV = os.path.join(OUT_WIN_DIR, \"index_sliding.csv\")\n",
    "SUMMARY_CSV = os.path.join(BASE_DIR, \"chunks_summary.csv\")\n",
    "\n",
    "# ===================== UTILIDADES =====================\n",
    "def read_txt(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        return f.read()\n",
    "\n",
    "def safe_filename_stem(fname):\n",
    "    return os.path.splitext(os.path.basename(fname))[0]\n",
    "\n",
    "def split_paragraphs(text):\n",
    "    raw_pars = [p.strip() for p in re.split(r\"\\n\\s*\\n\", text) if p.strip()]\n",
    "    pars = []\n",
    "    for p in raw_pars:\n",
    "        p = re.sub(r\"[ \\t]+\", \" \", p).strip()\n",
    "        pars.append(p)\n",
    "    return pars\n",
    "\n",
    "def split_sentences(p):\n",
    "    parts = re.split(r\"(?<=[\\.\\?\\!])\\s+\", p)\n",
    "    return [s.strip() for s in parts if s.strip()]\n",
    "\n",
    "def repartition_long_paragraph(p, max_chars):\n",
    "    if len(p) <= max_chars:\n",
    "        return [p]\n",
    "    sent = split_sentences(p)\n",
    "    chunks, buf = [], \"\"\n",
    "    for s in sent:\n",
    "        if not buf:\n",
    "            buf = s\n",
    "        elif len(buf) + 1 + len(s) <= max_chars:\n",
    "            buf = buf + \" \" + s\n",
    "        else:\n",
    "            chunks.append(buf.strip())\n",
    "            buf = s\n",
    "    if buf:\n",
    "        chunks.append(buf.strip())\n",
    "    final = []\n",
    "    for c in chunks:\n",
    "        if len(c) <= max_chars:\n",
    "            final.append(c)\n",
    "        else:\n",
    "            for i in range(0, len(c), max_chars):\n",
    "                final.append(c[i:i+max_chars].strip())\n",
    "    return final\n",
    "\n",
    "def fuse_short_paragraphs(pars, min_chars, merge_titles, title_max):\n",
    "    out = []\n",
    "    i = 0\n",
    "    while i < len(pars):\n",
    "        cur = pars[i]\n",
    "        is_title_like = merge_titles and (len(cur) <= title_max and \"\\n\" not in cur and len(cur.split()) <= 16)\n",
    "        if is_title_like and i + 1 < len(pars):\n",
    "            merged = (cur + \" ‚Äî \" + pars[i+1]).strip()\n",
    "            out.append(merged)\n",
    "            i += 2\n",
    "            continue\n",
    "        if len(cur) < min_chars and i + 1 < len(pars):\n",
    "            merged = (cur + \" \" + pars[i+1]).strip()\n",
    "            out.append(merged)\n",
    "            i += 2\n",
    "        else:\n",
    "            out.append(cur)\n",
    "            i += 1\n",
    "    return out\n",
    "\n",
    "def ensure_dir(d):\n",
    "    if not os.path.exists(d):\n",
    "        os.makedirs(d, exist_ok=True)\n",
    "\n",
    "def write_chunk(path, text):\n",
    "    ensure_dir(os.path.dirname(path))\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text.strip())\n",
    "\n",
    "def word_tokenize(text):\n",
    "    return re.findall(r\"\\S+\", text)\n",
    "\n",
    "# ===================== M√âTODO A: P√ÅRRAFOS =====================\n",
    "par_rows = []\n",
    "summary_rows = []\n",
    "\n",
    "files = [f for f in os.listdir(INPUT_DIR) if f.lower().endswith(\".txt\")]\n",
    "for fname in sorted(files):\n",
    "    path_in = os.path.join(INPUT_DIR, fname)\n",
    "    base = safe_filename_stem(fname)\n",
    "    out_dir_doc = os.path.join(OUT_PAR_DIR, base)\n",
    "    ensure_dir(out_dir_doc)\n",
    "\n",
    "    txt = read_txt(path_in)\n",
    "    pars = split_paragraphs(txt)\n",
    "    pars = fuse_short_paragraphs(pars, PAR_MIN_CHARS, MERGE_TITLES, TITLE_MAX_CHARS)\n",
    "\n",
    "    final_pars = []\n",
    "    for p in pars:\n",
    "        final_pars.extend(repartition_long_paragraph(p, PAR_MAX_CHARS))\n",
    "\n",
    "    lengths = []\n",
    "    for idx, chunk in enumerate(final_pars, start=1):\n",
    "        chunk_name = f\"chunk_{idx:04d}.txt\"\n",
    "        out_path = os.path.join(out_dir_doc, chunk_name)\n",
    "        write_chunk(out_path, chunk)\n",
    "        lengths.append(len(chunk))\n",
    "        par_rows.append({\n",
    "            \"filename_base\": base,\n",
    "            \"method\": \"paragraphs\",\n",
    "            \"chunk_id\": f\"{base}-p-{idx:04d}\",\n",
    "            \"chunk_path\": os.path.relpath(out_path, BASE_DIR).replace(\"\\\\\",\"/\"),\n",
    "            \"char_len\": len(chunk),\n",
    "            \"word_len\": len(chunk.split()),\n",
    "            \"paragraph_idx\": idx\n",
    "        })\n",
    "\n",
    "    summary_rows.append({\n",
    "        \"filename_base\": base,\n",
    "        \"method\": \"paragraphs\",\n",
    "        \"n_chunks\": len(lengths),\n",
    "        \"char_mean\": round(mean(lengths), 1) if lengths else 0,\n",
    "        \"pct_short_<300\": round(100*sum(l<300 for l in lengths)/len(lengths), 1) if lengths else 0\n",
    "    })\n",
    "\n",
    "if par_rows:\n",
    "    with open(IDX_PAR_CSV, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=list(par_rows[0].keys()))\n",
    "        writer.writeheader()\n",
    "        writer.writerows(par_rows)\n",
    "\n",
    "# ===================== M√âTODO B: VENTANAS DESLIZANTES =====================\n",
    "win_rows = []\n",
    "for fname in sorted(files):\n",
    "    path_in = os.path.join(INPUT_DIR, fname)\n",
    "    base = safe_filename_stem(fname)\n",
    "    out_dir_doc = os.path.join(OUT_WIN_DIR, base)\n",
    "    ensure_dir(out_dir_doc)\n",
    "\n",
    "    txt = read_txt(path_in)\n",
    "    words = word_tokenize(txt)\n",
    "    n = len(words)\n",
    "    lengths = []\n",
    "\n",
    "    if n == 0:\n",
    "        summary_rows.append({\n",
    "            \"filename_base\": base,\n",
    "            \"method\": \"sliding\",\n",
    "            \"n_chunks\": 0, \"char_mean\": 0, \"pct_short_<300\": 0\n",
    "            })\n",
    "        continue\n",
    "\n",
    "    idx = 0\n",
    "    win_id = 1\n",
    "    while idx < n:\n",
    "        end = min(n, idx + WIN_WORDS)\n",
    "        w_chunk = words[idx:end]\n",
    "        chunk = \" \".join(w_chunk).strip()\n",
    "        if not chunk:\n",
    "            break\n",
    "\n",
    "        chunk_name = f\"chunk_{win_id:04d}.txt\"\n",
    "        out_path = os.path.join(out_dir_doc, chunk_name)\n",
    "        write_chunk(out_path, chunk)\n",
    "\n",
    "        lengths.append(len(chunk))\n",
    "        win_rows.append({\n",
    "            \"filename_base\": base,\n",
    "            \"method\": \"sliding\",\n",
    "            \"chunk_id\": f\"{base}-w-{win_id:04d}\",\n",
    "            \"chunk_path\": os.path.relpath(out_path, BASE_DIR).replace(\"\\\\\",\"/\"),\n",
    "            \"char_len\": len(chunk),\n",
    "            \"word_len\": len(w_chunk),\n",
    "            \"start_word\": idx,\n",
    "            \"end_word\": end\n",
    "        })\n",
    "\n",
    "        win_id += 1\n",
    "        if end == n:\n",
    "            break\n",
    "        idx += WIN_STRIDE\n",
    "\n",
    "    summary_rows.append({\n",
    "        \"filename_base\": base,\n",
    "        \"method\": \"sliding\",\n",
    "        \"n_chunks\": len(lengths),\n",
    "        \"char_mean\": round(mean(lengths), 1) if lengths else 0,\n",
    "        \"pct_short_<300\": round(100*sum(l<300 for l in lengths)/len(lengths), 1) if lengths else 0\n",
    "    })\n",
    "\n",
    "if win_rows:\n",
    "    with open(IDX_WIN_CSV, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=list(win_rows[0].keys()))\n",
    "        writer.writeheader()\n",
    "        writer.writerows(win_rows)\n",
    "\n",
    "# ===================== RESUMEN =====================\n",
    "if summary_rows:\n",
    "    with open(SUMMARY_CSV, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=list(summary_rows[0].keys()))\n",
    "        writer.writeheader()\n",
    "        writer.writerows(summary_rows)\n",
    "\n",
    "print(\"‚úÖ Segmentaci√≥n regenerada con par√°metros ajustados.\")\n",
    "print(f\" - √çndice p√°rrafos:  {IDX_PAR_CSV}\")\n",
    "print(f\" - √çndice ventanas:  {IDX_WIN_CSV}\")\n",
    "print(f\" - Resumen:          {SUMMARY_CSV}\")\n",
    "print(f\" - Carpeta chunks A: {OUT_PAR_DIR}\")\n",
    "print(f\" - Carpeta chunks B: {OUT_WIN_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b23516",
   "metadata": {},
   "source": [
    "# Fase 3. Tokenizaci√≥n y Embeddings con metodo sliding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a786f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documentos (unique filename_base): 46\n",
      "Total de chunks en √≠ndice: 386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ingestando chunks en Chroma: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 386/386 [00:08<00:00, 43.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Embeddings generados e indexados.\n",
      "üìö Collection: ai_apuntes_sliding_openai_v1 | count = 386\n",
      "üíæ Persist dir: data/vectorstores/chroma_sliding_openai_v1\n"
     ]
    }
   ],
   "source": [
    "import os, csv, time, math\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# (A) ‚Äî‚Äî CONFIGURACI√ìN GENERAL \n",
    "BASE_DIR       = \"data\"\n",
    "INDEX_CSV      = os.path.join(BASE_DIR, \"chunks_sliding\", \"index_sliding.csv\")   # puedes cambiar a p√°rrafos si quieres\n",
    "PERSIST_DIR    = os.path.join(BASE_DIR, \"vectorstores\", \"chroma_sliding_openai_v1\")\n",
    "COLLECTION_NAME= \"ai_apuntes_sliding_openai_v1\"\n",
    "\n",
    "# Proveedor de embeddings: \"openai\" o \"local\"\n",
    "PROVIDER       = \"openai\"       \n",
    "OPENAI_MODEL   = \"text-embedding-3-small\"      \n",
    "LOCAL_MODEL    = \"all-MiniLM-L6-v2\"           \n",
    "\n",
    "BATCH_SIZE     = 128            # tama√±o de lote para ingesti√≥n\n",
    "MAX_RETRIES    = 5              # reintentos por rate-limit/errores transitorios\n",
    "RETRY_BASE_SEC = 2              # backoff exponencial\n",
    "\n",
    "# (B) ‚Äî‚Äî SETUP DE EMBEDDINGS \n",
    "embed_dims = None\n",
    "\n",
    "if PROVIDER == \"openai\":\n",
    "    from openai import OpenAI\n",
    "    OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "    if not OPENAI_API_KEY:\n",
    "        raise ValueError(\"La variable de entorno OPENAI_API_KEY no est√° configurada.\")\n",
    "    \n",
    "    oai_client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "    def embed_texts(texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"Embeddings con OpenAI + reintentos.\"\"\"\n",
    "        for attempt in range(MAX_RETRIES):\n",
    "            try:\n",
    "                resp = oai_client.embeddings.create(\n",
    "                    model=OPENAI_MODEL,\n",
    "                    input=texts\n",
    "                )\n",
    "                vecs = [d.embedding for d in resp.data]\n",
    "                return vecs\n",
    "            except Exception as e:\n",
    "                wait = RETRY_BASE_SEC * (2 ** attempt)\n",
    "                print(f\"[WARN] Error {e}. Reintentando en {wait}s...\")\n",
    "                if attempt == MAX_RETRIES - 1:\n",
    "                    raise\n",
    "                time.sleep(wait)\n",
    "else:\n",
    "    raise ValueError(\"PROVIDER debe ser 'openai' o 'local'.\")\n",
    "\n",
    "# (C) ‚Äî‚Äî INICIALIZAR CHROMA PERSISTENTE \n",
    "os.makedirs(PERSIST_DIR, exist_ok=True)\n",
    "\n",
    "client = chromadb.PersistentClient(\n",
    "    path=PERSIST_DIR,\n",
    "    settings=Settings(is_persistent=True)\n",
    ")\n",
    "\n",
    "# Crear o recuperar la colecci√≥n\n",
    "try:\n",
    "    collection_sliding = client.get_collection(COLLECTION_NAME)\n",
    "except:\n",
    "    collection_sliding = client.create_collection(\n",
    "        name=COLLECTION_NAME,\n",
    "        metadata={\"hnsw:space\": \"cosine\"}  # distancia coseno\n",
    "    )\n",
    "\n",
    "# (D) ‚Äî‚Äî UTILIDADES \n",
    "def read_index_rows(csv_path: str):\n",
    "    rows = []\n",
    "    with open(csv_path, \"r\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for r in reader:\n",
    "            rows.append(r)\n",
    "    return rows\n",
    "\n",
    "def load_chunk_text(chunk_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Abre el archivo de texto del chunk, corrigiendo rutas relativas.\n",
    "    \"\"\"\n",
    "    # Si el path ya incluye \"data/\", se queda tal cual\n",
    "    if not os.path.isabs(chunk_path):\n",
    "        # Si empieza por \"data/\", lo consideramos relativo al proyecto\n",
    "        if chunk_path.startswith(\"data/\") or chunk_path.startswith(\".\\\\data\\\\\") or chunk_path.startswith(\".\\\\chunks_\"):\n",
    "            path = os.path.normpath(chunk_path)\n",
    "        else:\n",
    "            # Si viene solo 'chunks_sliding/...', le anteponemos 'data/'\n",
    "            path = os.path.join(\"data\", chunk_path)\n",
    "    else:\n",
    "        path = chunk_path\n",
    "\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"No se encontr√≥ el archivo: {path}\")\n",
    "\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        return f.read().strip()\n",
    "\n",
    "\n",
    "# (E) ‚Äî‚Äî CARGAR √çNDICE Y PREPARAR INGESTA \n",
    "rows = read_index_rows(INDEX_CSV)\n",
    "print(f\"Documentos (unique filename_base): {len(set(r['filename_base'] for r in rows))}\")\n",
    "print(f\"Total de chunks en √≠ndice: {len(rows)}\")\n",
    "\n",
    "# (F) ‚Äî‚Äî INGESTA EN LOTES CON EMBEDDINGS \n",
    "ids, docs, metas = [], [], []\n",
    "\n",
    "def flush_batch():\n",
    "    if not ids:\n",
    "        return\n",
    "    # Calcula embeddings del batch actual\n",
    "    vecs = embed_texts(docs)\n",
    "    # upsert = idempotente: si ya existe el id, lo actualiza\n",
    "    collection_sliding.upsert(ids=ids, documents=docs, metadatas=metas, embeddings=vecs)\n",
    "    ids.clear(); docs.clear(); metas.clear()\n",
    "\n",
    "for r in tqdm(rows, desc=\"Ingestando chunks en Chroma\"):\n",
    "    chunk_id   = r[\"chunk_id\"]              # ej: <base>-w-0001\n",
    "    chunk_path = r[\"chunk_path\"]            # ej: data/chunks_sliding/<base>/chunk_0001.txt\n",
    "    text       = load_chunk_text(chunk_path)\n",
    "    if not text:\n",
    "        continue\n",
    "\n",
    "    ids.append(chunk_id)\n",
    "    docs.append(text)\n",
    "    metas.append({\n",
    "        \"filename_base\": r.get(\"filename_base\", \"\"),\n",
    "        \"method\":       r.get(\"method\", \"sliding\"),\n",
    "        \"chunk_path\":   r.get(\"chunk_path\", \"\"),\n",
    "        \"char_len\":     int(r.get(\"char_len\", 0)),\n",
    "        \"word_len\":     int(r.get(\"word_len\", 0)),\n",
    "        \"start_word\":   int(r.get(\"start_word\", 0)),\n",
    "        \"end_word\":     int(r.get(\"end_word\", 0)),\n",
    "    })\n",
    "\n",
    "    if len(ids) >= BATCH_SIZE:\n",
    "        flush_batch()\n",
    "\n",
    "# √∫ltimo lote\n",
    "flush_batch()\n",
    "\n",
    "print(\"‚úÖ Embeddings generados e indexados.\")\n",
    "print(\"üìö Collection:\", COLLECTION_NAME, \"| count =\", collection_sliding.count())\n",
    "print(\"üíæ Persist dir:\", PERSIST_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90482d39",
   "metadata": {},
   "source": [
    "# Tokenizacion y Embeddings con metodo de parrafos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "55934ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documentos (unique filename_base): 46\n",
      "Total de chunks en √≠ndice: 334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ingestando chunks en Chroma: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 334/334 [00:04<00:00, 73.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Embeddings generados e indexados.\n",
      "üìö Collection: ai_apuntes_paragraphs_openai_v1 | count = 334\n",
      "üíæ Persist dir: data/vectorstores/chroma_paragraphs_openai_v1\n"
     ]
    }
   ],
   "source": [
    "import os, csv, time, math\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# (A) ‚Äî‚Äî CONFIGURACI√ìN GENERAL \n",
    "BASE_DIR       = \"data\"\n",
    "INDEX_CSV      = os.path.join(BASE_DIR, \"chunks_paragraphs\", \"index_paragraphs.csv\")   # puedes cambiar a p√°rrafos si quieres\n",
    "PERSIST_DIR    = os.path.join(BASE_DIR, \"vectorstores\", \"chroma_paragraphs_openai_v1\")\n",
    "COLLECTION_NAME= \"ai_apuntes_paragraphs_openai_v1\"\n",
    "\n",
    "# Proveedor de embeddings: \"openai\" o \"local\"\n",
    "PROVIDER       = \"openai\"       \n",
    "OPENAI_MODEL   = \"text-embedding-3-small\"      \n",
    "LOCAL_MODEL    = \"all-MiniLM-L6-v2\"           \n",
    "\n",
    "BATCH_SIZE     = 128            # tama√±o de lote para ingesti√≥n\n",
    "MAX_RETRIES    = 5              # reintentos por rate-limit/errores transitorios\n",
    "RETRY_BASE_SEC = 2              # backoff exponencial\n",
    "\n",
    "# (B) ‚Äî‚Äî SETUP DE EMBEDDINGS \n",
    "embed_dims = None\n",
    "\n",
    "if PROVIDER == \"openai\":\n",
    "    from openai import OpenAI\n",
    "    OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "    if not OPENAI_API_KEY:\n",
    "        raise ValueError(\"La variable de entorno OPENAI_API_KEY no est√° configurada.\")\n",
    "    \n",
    "    oai_client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "    def embed_texts(texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"Embeddings con OpenAI + reintentos.\"\"\"\n",
    "        for attempt in range(MAX_RETRIES):\n",
    "            try:\n",
    "                resp = oai_client.embeddings.create(\n",
    "                    model=OPENAI_MODEL,\n",
    "                    input=texts\n",
    "                )\n",
    "                vecs = [d.embedding for d in resp.data]\n",
    "                return vecs\n",
    "            except Exception as e:\n",
    "                wait = RETRY_BASE_SEC * (2 ** attempt)\n",
    "                print(f\"[WARN] Error {e}. Reintentando en {wait}s...\")\n",
    "                if attempt == MAX_RETRIES - 1:\n",
    "                    raise\n",
    "                time.sleep(wait)\n",
    "else:\n",
    "    raise ValueError(\"PROVIDER debe ser 'openai' o 'local'.\")\n",
    "\n",
    "# (C) ‚Äî‚Äî INICIALIZAR CHROMA PERSISTENTE \n",
    "os.makedirs(PERSIST_DIR, exist_ok=True)\n",
    "\n",
    "client = chromadb.PersistentClient(\n",
    "    path=PERSIST_DIR,\n",
    "    settings=Settings(is_persistent=True)\n",
    ")\n",
    "\n",
    "# Crear o recuperar la colecci√≥n\n",
    "try:\n",
    "    collection_paragraphs = client.get_collection(COLLECTION_NAME)\n",
    "except:\n",
    "    collection_paragraphs = client.create_collection(\n",
    "        name=COLLECTION_NAME,\n",
    "        metadata={\"hnsw:space\": \"cosine\"}  # distancia coseno\n",
    "    )\n",
    "\n",
    "# (D) ‚Äî‚Äî UTILIDADES \n",
    "def read_index_rows(csv_path: str):\n",
    "    rows = []\n",
    "    with open(csv_path, \"r\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for r in reader:\n",
    "            rows.append(r)\n",
    "    return rows\n",
    "\n",
    "def load_chunk_text(chunk_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Abre el archivo de texto del chunk, corrigiendo rutas relativas.\n",
    "    \"\"\"\n",
    "    # Si el path ya incluye \"data/\", se queda tal cual\n",
    "    if not os.path.isabs(chunk_path):\n",
    "        # Si empieza por \"data/\", lo consideramos relativo al proyecto\n",
    "        if chunk_path.startswith(\"data/\") or chunk_path.startswith(\".\\\\data\\\\\") or chunk_path.startswith(\".\\\\chunks_\"):\n",
    "            path = os.path.normpath(chunk_path)\n",
    "        else:\n",
    "            # Si viene solo 'chunks_sliding/...', le anteponemos 'data/'\n",
    "            path = os.path.join(\"data\", chunk_path)\n",
    "    else:\n",
    "        path = chunk_path\n",
    "\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"No se encontr√≥ el archivo: {path}\")\n",
    "\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        return f.read().strip()\n",
    "\n",
    "\n",
    "# (E) ‚Äî‚Äî CARGAR √çNDICE Y PREPARAR INGESTA \n",
    "rows = read_index_rows(INDEX_CSV)\n",
    "print(f\"Documentos (unique filename_base): {len(set(r['filename_base'] for r in rows))}\")\n",
    "print(f\"Total de chunks en √≠ndice: {len(rows)}\")\n",
    "\n",
    "# (F) ‚Äî‚Äî INGESTA EN LOTES CON EMBEDDINGS \n",
    "ids, docs, metas = [], [], []\n",
    "\n",
    "def flush_batch():\n",
    "    if not ids:\n",
    "        return\n",
    "    # Calcula embeddings del batch actual\n",
    "    vecs = embed_texts(docs)\n",
    "    # upsert = idempotente: si ya existe el id, lo actualiza\n",
    "    collection_paragraphs.upsert(ids=ids, documents=docs, metadatas=metas, embeddings=vecs)\n",
    "    ids.clear(); docs.clear(); metas.clear()\n",
    "\n",
    "for r in tqdm(rows, desc=\"Ingestando chunks en Chroma\"):\n",
    "    chunk_id   = r[\"chunk_id\"]              # ej: <base>-w-0001\n",
    "    chunk_path = r[\"chunk_path\"]            # ej: data/chunks_sliding/<base>/chunk_0001.txt\n",
    "    text       = load_chunk_text(chunk_path)\n",
    "    if not text:\n",
    "        continue\n",
    "\n",
    "    ids.append(chunk_id)\n",
    "    docs.append(text)\n",
    "    metas.append({\n",
    "        \"filename_base\": r.get(\"filename_base\", \"\"),\n",
    "        \"method\":       r.get(\"method\", \"sliding\"),\n",
    "        \"chunk_path\":   r.get(\"chunk_path\", \"\"),\n",
    "        \"char_len\":     int(r.get(\"char_len\", 0)),\n",
    "        \"word_len\":     int(r.get(\"word_len\", 0)),\n",
    "        \"start_word\":   int(r.get(\"start_word\", 0)),\n",
    "        \"end_word\":     int(r.get(\"end_word\", 0)),\n",
    "    })\n",
    "\n",
    "    if len(ids) >= BATCH_SIZE:\n",
    "        flush_batch()\n",
    "\n",
    "# √∫ltimo lote\n",
    "flush_batch()\n",
    "\n",
    "print(\"‚úÖ Embeddings generados e indexados.\")\n",
    "print(\"üìö Collection:\", COLLECTION_NAME, \"| count =\", collection_paragraphs.count())\n",
    "print(\"üíæ Persist dir:\", PERSIST_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647db0fa",
   "metadata": {},
   "source": [
    "## Prueba de Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "12b5eda5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#1  dist=0.3058  base=11_Semana_AI_20251014_3_AlexStevenNaranjoMasƒ±s_EmbeddingsAutoencoders  palabras=240\n",
      "chunks_sliding/11_Semana_AI_20251014_3_AlexStevenNaranjoMasƒ±s_EmbeddingsAutoencoders/chunk_0005.txt\n",
      "2. Estructura basica de un Autoencoder. V. AUTOENCODERS(CODIFICADORESAUTOMATICOS) A. Estructura General y Objetivo Encoder‚ÜíEspacio Latente‚ÜíDecoder Aprenden a reconstruir la entrada. Aunque la se√±al de entre- namiento es auto-supervisada (salida = entrada), se consideran tipicamente metodos no supervisados por no requerir etiquetas externas. B. Componentes y Variantes Encoder:reduce espacialidad y comprime informacion (conv +downsampling). Latente:vector/tensor compacto; su tama√±o controla capaci...\n",
      "\n",
      "#2  dist=0.3154  base=11_Semana_AI_20251014_1_JuanDiegoJimenezValverde_CNNYAutoencoders  palabras=240\n",
      "chunks_sliding/11_Semana_AI_20251014_1_JuanDiegoJimenezValverde_CNNYAutoencoders/chunk_0007.txt\n",
      "sonutiles, por ejemplo, en aplicaciones medicas para resaltar fracturas o anomalias en radiografias. B. Autoencoders Aunque utilizan arquitecturas similares a las redes convolu- cionales, losautoencoderstrabajan sin etiquetas explicitas, por lo que se consideran metodos no supervisados. Su objetivo es reconstruir la entrada original, aprendiendo una repre- sentacion interna comprimida. Fig. 2. Estructura basica de un Autoencoder. El proceso consta de tres partes, como se muestra en la Fig. 2: 1)...\n",
      "\n",
      "#3  dist=0.3202  base=11_Semana_AI_20251014_2_ LuisFernandoBenavidesVillegas_ConvolucionesPoolingAutoencoders  palabras=240\n",
      "chunks_sliding/11_Semana_AI_20251014_2_ LuisFernandoBenavidesVillegas_ConvolucionesPoolingAutoencoders/chunk_0009.txt\n",
      "la separacion entre clases es clara en el espacio reducido, se considera que el modelo ha aprendido una rep- resentacion adecuada. Por el contrario, si las clases aparecen mezcladas, indica que la red no ha logrado distinguir correc- tamente las caracteristicas de cada una. C. Mapas de Activacion Se pueden generarheatmapso mapas de activacion que destacan las zonas especificas de una imagen que influyen mas en la decision del modelo. Estos mapas sonutiles para verificar si la red esta enfocandos...\n",
      "\n",
      "#4  dist=0.3418  base=11_Semana_AI_20251016_2_AndresSanchezRojas_AutoencodersUNetRAGs  palabras=240\n",
      "chunks_sliding/11_Semana_AI_20251016_2_AndresSanchezRojas_AutoencodersUNetRAGs/chunk_0002.txt\n",
      "presenta la conexion con tareas de vision por computador (p. ej., segmentacion y arquitecturas tipo U -Net) y la extension a representaciones para texto mediante tokenizacion y embeddings, asicomo su papel en sistemas mas amplios como RAGs y agentes basados en LLM. El texto ofrece una guia practica con definiciones, formulas y recomendaciones operativas para implementar experimentos en imagenes y texto. II. AUTOENCODERS II-A. Definicion y proposito Los autoencoders son una arquitectura novedosa ...\n",
      "\n",
      "#5  dist=0.3516  base=11_Semana_AI_20251014_2_ LuisFernandoBenavidesVillegas_ConvolucionesPoolingAutoencoders  palabras=240\n",
      "chunks_sliding/11_Semana_AI_20251014_2_ LuisFernandoBenavidesVillegas_ConvolucionesPoolingAutoencoders/chunk_0010.txt\n",
      "Latente‚ÜíDecoder El aprendizaje del autoencoder consiste en minimizar el error de reconstruccionentre la entrada original y la salida reconstruida. Aunque no haya etiquetas externas, el entre- namiento es parcialmente supervisado, ya que la salida se compara directamente con la entrada. A. Encoder Consiste en una serie de bloques convolucionales seguidos de operaciones depooling, con el objetivo de extraer las caracteristicas mas relevantes de la entrada y comprimir la informacion a traves de un ...\n"
     ]
    }
   ],
   "source": [
    "# Todo este c√≥digo era una prueba para revisar si los embeddings se hab√≠an hecho bien\n",
    "import os\n",
    "from openai import OpenAI\n",
    "api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "oai_client = OpenAI(api_key=api_key)\n",
    "OPENAI_MODEL = \"text-embedding-3-small\"\n",
    "\n",
    "QUESTION = \"¬øQu√© es un autoencoder y c√≥mo se entrena?\"\n",
    "TOP_K = 5\n",
    "\n",
    "# 1) Embeber la pregunta con OpenAI\n",
    "qvec = oai_client.embeddings.create(\n",
    "    model=OPENAI_MODEL,\n",
    "    input=QUESTION\n",
    ").data[0].embedding\n",
    "\n",
    "# 2) Consultar \n",
    "res_sliding = collection_sliding.query(\n",
    "    query_embeddings=[qvec],\n",
    "    n_results=TOP_K,\n",
    "    include=[\"metadatas\", \"distances\", \"documents\"]\n",
    ")\n",
    "\n",
    "res_paragraphs = collection_paragraphs.query(\n",
    "    query_embeddings=[qvec],\n",
    "    n_results=TOP_K,\n",
    "    include=[\"metadatas\", \"distances\", \"documents\"]\n",
    ")\n",
    "method = \"sliding\"  \n",
    "\n",
    "res = res_sliding if method == \"sliding\" else res_paragraphs\n",
    "\n",
    "for rank, (doc, meta, dist) in enumerate(zip(res[\"documents\"][0],\n",
    "                                             res[\"metadatas\"][0],\n",
    "                                             res[\"distances\"][0]), start=1):\n",
    "    print(f\"\\n#{rank}  dist={dist:.4f}  base={meta.get('filename_base')}  palabras={meta.get('word_len')}\")\n",
    "    print(meta.get(\"chunk_path\"))\n",
    "    print(doc[:500].replace(\"\\n\",\" \") + (\"...\" if len(doc)>500 else \"\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49b0e44",
   "metadata": {},
   "source": [
    "# Fase 4. Herramientas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94564b0",
   "metadata": {},
   "source": [
    "# RAG Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d604e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import BaseTool\n",
    "from typing import Any, Type\n",
    "from pydantic import BaseModel, Field, PrivateAttr\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "# ======== MODELO DE INPUT ========\n",
    "class RAGInput(BaseModel):\n",
    "    query: str = Field(..., description=\"Pregunta o frase a buscar en los apuntes\")\n",
    "\n",
    "# ======== TOOL ========\n",
    "class RAGTool(BaseTool):\n",
    "    name: str = \"RAG_Tool\"\n",
    "    description: str = \"Busca informaci√≥n en la base vectorial local de apuntes de Inteligencia Artificial\"\n",
    "    args_schema: Type[BaseModel] = RAGInput\n",
    "\n",
    "    _client: Any = PrivateAttr()\n",
    "    _collection: Any = PrivateAttr()\n",
    "    _oai_client: Any = PrivateAttr()\n",
    "\n",
    "    def __init__(self, collection_name: str, persist_dir: str, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self._client = chromadb.PersistentClient(path=persist_dir, settings=Settings(is_persistent=True))\n",
    "        self._collection = self._client.get_collection(collection_name)\n",
    "        self._oai_client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "    def _run(self, query: str):\n",
    "        \"\"\"Consulta la base vectorial usando embeddings OpenAI\"\"\"\n",
    "        embedding = self._oai_client.embeddings.create(\n",
    "            model=\"text-embedding-3-small\",\n",
    "            input=query\n",
    "        ).data[0].embedding\n",
    "\n",
    "        results = self._collection.query(\n",
    "            query_embeddings=[embedding],\n",
    "            n_results=5,\n",
    "            include=[\"documents\",  \"distances\", \"metadatas\"]\n",
    "        )\n",
    "        docs = results[\"documents\"][0]\n",
    "        metas = results[\"metadatas\"][0]\n",
    "\n",
    "        combined = [\n",
    "            f\"Documento: {m.get('filename_base','')} | Fragmento: {d[:600]}...\"\n",
    "            for d, m in zip(docs, metas)\n",
    "        ]\n",
    "        return \"\\n\".join(combined)\n",
    "\n",
    "    async def _arun(self, query: str):\n",
    "        raise NotImplementedError(\"RAGTool no soporta ejecuci√≥n as√≠ncrona.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba91464",
   "metadata": {},
   "source": [
    "## Mini-Prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7302470b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Consulta: ¬øQu√© es una red neuronal convolucional?\n",
      "------------------------------------------------------\n",
      "üìò Resultados:\n",
      "Documento: 10_Semana_AI_20251007_1_GianmarcoOportaPerez_RedesNeuronalesConvolucionales | Fragmento: avanzan a traves de las capas convolucionales y de agrupamiento, se reduce su tama√±o espacial, pero aumenta la abstraccion de las caracteristicas aprendidas. Fig. 1. redes convolucionales C. Aplicaciones Comunes Las redes convolucionales se aplican ampliamente en diver- sas tareas de vision artificial, entre las que destacan: ‚Ä¢Clasificacion de imagenes. ‚Ä¢Segmentacion de objetos. ‚Ä¢Segmentacion de instancias. ‚Ä¢Procesamiento general de imagenes. Estas arquitecturas han demostrado una gran eficacia en problemas de reconocimiento visual, deteccion de patrones y procesamiento de se√±ales en el domini...\n",
      "Documento: 11_Semana_AI_20251014_2_ LuisFernandoBenavidesVillegas_ConvolucionesPoolingAutoencoders | Fragmento: de un bloque, promoviendo la reutilizacion de carac- teristicas y reduciendo la redundancia. Esta estructura densa mejora la propagacion del gradiente, optimiza la eficiencia del modelo y mantiene un numero reducido de parametros. II. PROBLEMAS CON LASREDESNEURONALES CONVOLUCIONALES A pesar de su alto desempe√±o, las redes convolucionales se comportan como una ‚Äúcaja negra‚Äù, ya que resulta dificil com- prender que tipo de informacion estan utilizando para tomar sus decisiones. Las representaciones internas que generan son altamente abstractas, lo que plantea un reto importante de interpretabilid...\n",
      "Documento: 8_Semana_AI_20250923_2_FabianDiazBarboza_RedesNeuronalesFullyConnected | Fragmento: ReLU, etc.) permiten que la red modelice relaciones no lineales entre entradas y salidas. 2.Capasyprofundidad:amayorprofundidad,mayor capacidad para representar abstracciones jerarquicas. 3.Diferenciabilidad:la diferenciabilidad de las fun- ciones internas es requisito para aplicar retropropaga- cion y optimizar los parametros mediante gradiente descendente. 6 Conclusiones En conclusion de la clase, las redes neuronales son como una evolucion natural de la regresion logistica: partien- do de la clasificacion binaria, pasando por la extension multinomial y compactando parametros mediante alge- ...\n",
      "Documento: 11_Semana_AI_20251014_1_JuanDiegoJimenezValverde_CNNYAutoencoders | Fragmento: final del modelo. G. Arquitecturas Convolucionales Una red convolucional combina secuencias deconvolucion ‚Üíactivacion (ReLU)‚Üípooling. Este patron se repite varias veces para extraer informacion progresivamente mas abstracta de la imagen. Generalmente, se prefieren filtros peque√±os (como3√ó3) para capturar detalles locales de forma mas eficiente. Elconvolutional stackse forma al aplicar multiples capas de convolucion consecutivas. Por ejemplo, en una imagen de 5√ó5, un filtro3√ó3puede desplazarse para generar una salida de3√ó3. Regla practica:las dimensiones de las imagenes deben ser divisibles ent...\n",
      "Documento: 8_Semana_AI_20250923_1_BrandonEmmanuelSanchezAraya_RedesNeuronalesMNIST | Fragmento: neurona/clase tiene su propio sesgo:b= (b0, . . . , b 9)‚ä§.Cantidad de neuronas=tama√±o deb. IV. EJERCICIO:DE VECTOR A MATRIZ 1) Una sola regresion binaria (vectorx) Sea x=Ô£Æ Ô£ØÔ£ØÔ£∞3 4 5 6Ô£π Ô£∫Ô£∫Ô£ª, w=Ô£Æ Ô£ØÔ£ØÔ£∞3 2 4 5Ô£π Ô£∫Ô£∫Ô£ª, b= 2. Entonces z=w‚ä§x+b= [ 3 2 4 5 ]Ô£Æ Ô£ØÔ£ØÔ£∞3 4 5 6Ô£π Ô£∫Ô£∫Ô£ª+2 = 67+2 = 69,y=œÉ(z). 2) Varias regresiones a la vez Ahora dos regresiones (piensa ‚Äúdos neuronas de salida‚Äù). Apilamos sus pesos en una matrizWy sus sesgos en un vectorb: W=\u00143 2 4 5 4 3 2 1\u0015 ‚ààR2√ó4, b=\u00142 3\u0015 ‚ààR2. Con el mismoxde arriba: z=Wx+b=\u00143 2 4 5 4 3 2 1\u0015Ô£Æ Ô£ØÔ£ØÔ£∞3 4 5 6Ô£π Ô£∫Ô£∫Ô£ª+\u00142 3\u0015 =\u001469 43\u0015 . V. REDNEURONAL Una red neuronal es un model...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "collection_sliding = \"ai_apuntes_sliding_openai_v1\"\n",
    "collection_paragraphs = \"ai_apuntes_paragraphs_openai_v1\"\n",
    "persist_dir = \"data/vectorstores/chroma_sliding_openai_v1\"\n",
    "\n",
    "method = \"sliding\"\n",
    "collection = collection_sliding if method == \"sliding\" else collection_paragraphs\n",
    "\n",
    "# ======== INICIALIZAR TOOL ========\n",
    "rag_tool = RAGTool(collection_name=collection, persist_dir=persist_dir)\n",
    "\n",
    "# ======== PRUEBAS ========\n",
    "query = \"¬øQu√© es una red neuronal convolucional?\"\n",
    "print(\"üîç Consulta:\", query)\n",
    "print(\"------------------------------------------------------\")\n",
    "\n",
    "response = rag_tool._run(query)\n",
    "\n",
    "print(\"üìò Resultados:\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6221620",
   "metadata": {},
   "source": [
    "# Web Search Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ab69b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import Tool\n",
    "from langchain_tavily import TavilySearch\n",
    "\n",
    "def get_websearch_tool():\n",
    "    search = TavilySearch(max_results=5)\n",
    "    return Tool(\n",
    "        name=\"WebSearch_Tool\",\n",
    "        description=\"Busca informaci√≥n en Internet usando Tavily\",\n",
    "        func=search.run\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de27a66a",
   "metadata": {},
   "source": [
    "## Mini-Prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfdf15c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Consulta: √öltimos avances en inteligencia artificial en 2025\n",
      "------------------------------------------------------\n",
      "üåê Resultados:\n",
      "√öltimos avances en inteligencia artificial en 2025\n",
      "{'query': '√öltimos avances en inteligencia artificial en 2025', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://www.bbc.com/mundo/articles/c4gxzx0kpp6o', 'title': 'IA: Qu√© se espera en 2025 de la inteligencia artificial, el avance que ...', 'content': '**La inteligencia artificial (IA) est√° marcando un antes y un despu√©s en la historia de la tecnolog√≠a, y 2025 traer√° m√°s sorpresas.** Un \"centauro doctor + un sistema de IA\" mejora las decisiones que toman los humanos por su cuenta y los sistemas de IA por la suya. Los agentes de IA aut√≥nomos basados en modelos de lenguaje son el objetivo para 2025 de las grandes empresas tecnol√≥gicas como OpenAI (ChatGPT), Meta (LLaMA), Google (Gemini) o Anthropic (Claude). Nos enfrentamos a un dilema crucial: saber cu√°ndo es mejor ser \"autom√°ticos\" en el uso de agentes de IA aut√≥nomos y cu√°ndo necesitamos tomar la decisi√≥n, es decir, recurrir \"al control humano\" o la \"interacci√≥n humano e IA\".', 'score': 0.8893851, 'raw_content': None}, {'url': 'https://www.greentecher.com/inteligencia-artificial-2025/?srsltid=AfmBOoooJyPCWPod-fOqaQf8FjZhFI_Cc1fFP-hAoOQ-S4o2vOrqqoCm', 'title': 'Las 10 mejores Inteligencias Artificiales (IA) de 2025 - Greentech', 'content': '+ Diplomado Internacional en Direcci√≥n y Gesti√≥n de Empresas IT (tecnol√≥gicas) + Diplomado Internacional de Consultor√≠a en Inteligencia Artificial, Automatizaciones y Agentes IA En 2025, la **inteligencia artificial (IA)** sigue transformando el panorama tecnol√≥gico con herramientas avanzadas que est√°n revolucionando c√≥mo las personas trabajan, crean y resuelven problemas. üöÄ Puedes estudiar en Greentech, y aprender de estas tecnolog√≠as, con el Diplomado en Innovaci√≥n, Inteligencia Artificial y Metaverso AQU√ç  **Inteligencia Artificial, IA, automatizaci√≥n, marketing digital, SEO, ChatGPT, MidJourney, DALL¬∑E, Deep Learning, Hugging Face, creaci√≥n de contenido, videos generados por IA, procesamiento de lenguaje natural, productividad, innovaci√≥n.** ai, chatgpt, computadora, futuro, gemini, ia, inteligencia, inteligencia artificial, midjourney, perplexiti, robotica, Tecnolog√≠a + Diplomado Internacional en Direcci√≥n y Gesti√≥n de Empresas IT', 'score': 0.8649622, 'raw_content': None}, {'url': 'https://www.digital-robots.com/noticias/la-revolucion-de-la-inteligencia-artificial-en-2025-un-futuro-transformador', 'title': 'La Revoluci√≥n de la Inteligencia Artificial en 2025 - Digital Robots', 'content': 'From the formation of mixed work teams to the local regulation of technology, we see a scenario in which Artificial Intelligence) will be incorporated in a significant way in our lives and businesses. Artificial Intelligence) is at the height of a dramatic transformation by 2025, with a significant effect on the labor, technology and regulatory sectors. The incorporation of Artificial Intelligence) with **human workforces**, the creation of **specialized models** and local regulations constitute a new episode in the technological progression. **Robotic Process Automation (RPA)** has become a key tool for maximizing **sales conversions** by automating repetitive tasks and optimizing the use of time. RPA technology has revolutionized data processing by automating repetitive tasks, improving efficiency and reducing errors.', 'score': 0.81347597, 'raw_content': None}, {'url': 'https://edgecore.com/es/la-inteligencia-artificial-el-cambiador-de-2025/', 'title': 'La Inteligencia Artificial: El Cambiador de 2025', 'content': 'En 2025, los desarrolladores conceptualizar√°n y adelantar√°n dise√±os que satisfagan los requisitos del futuro. Los centros de datos dise√±ados', 'score': 0.8066246, 'raw_content': None}, {'url': 'https://www.youtube.com/watch?v=njgs9r-Z5iA', 'title': 'El estado del arte de AI en 2025 - YouTube', 'content': 'El estado del arte de AI en 2025\\nPlatzi\\n1900000 subscribers\\n8191 likes\\n275147 views\\n14 Jun 2025\\nEste video te lleva por los retos, oportunidades y dilemas √©ticos de la inteligencia artificial en 2025, mostrando c√≥mo est√° transformando industrias, empleos y hasta la educaci√≥n. Si quieres entender el impacto real de AI y por qu√© este es el mejor momento para ser parte de la revoluci√≥n tecnol√≥gica, no te lo pierdas.\\n------------------------------------\\nüëâ Platzi es una plataforma de educaci√≥n Online, con tu suscripci√≥n tienes acceso a  cursos en diferentes √°reas de aprendizaje:\\n\\n- Desarrollo e ingenier√≠a\\n- Inteligencia artificial\\n- Data Science\\n- Dise√±o y UX\\n- Marketing\\n- Negocios y emprendimiento\\n- Producci√≥n audiovisual\\n- Liderazgo y management\\n\\nCada √°rea est√° compuesta por Rutas de aprendizaje y Escuelas que har√°n de tu perfil uno de los m√°s demandados de la industria. Adem√°s, cuentas con ADA -Asistente De Aprendizaje- creada con inteligencia artificial, grupos de estudio, meetups digitales y tutoriales en nuestro sistema de discusiones.\\n\\nTodo esto y m√°s, en https://platzi.com\\n------------------------------------\\nS√≠guenos\\nFacebook: https://platzi.com/l/fHl6pows/\\nTwitter: https://platzi.com/l/0DJ5PONB/\\nInstagram: https://platzi.com/l/jt260ue0/\\n@recent-platzi\\n463 comments\\n', 'score': 0.7823471, 'raw_content': None}], 'response_time': 1.15, 'request_id': 'cddce2ee-040d-4d13-a735-eecf28e5843a'}\n",
      "\n",
      "--- Resultado 1 ---\n",
      "T√≠tulo: IA: Qu√© se espera en 2025 de la inteligencia artificial, el avance que ...\n",
      "URL: https://www.bbc.com/mundo/articles/c4gxzx0kpp6o\n",
      "Contenido: **La inteligencia artificial (IA) est√° marcando un antes y un despu√©s en la historia de la tecnolog√≠a, y 2025 traer√° m√°s sorpresas.** Un \"centauro doctor + un sistema de IA\" mejora las decisiones que toman los humanos por su cuenta y los sistemas de IA por la suya. Los agentes de IA aut√≥nomos basados en modelos de lenguaje son el objetivo para 2025 de las grandes empresas tecnol√≥gicas como OpenAI (ChatGPT), Meta (LLaMA), Google (Gemini) o Anthropic (Claude). Nos enfrentamos a un dilema crucial: saber cu√°ndo es mejor ser \"autom√°ticos\" en el uso de agentes de IA aut√≥nomos y cu√°ndo necesitamos tomar la decisi√≥n, es decir, recurrir \"al control humano\" o la \"interacci√≥n humano e IA\".\n",
      "\n",
      "--- Resultado 2 ---\n",
      "T√≠tulo: Las 10 mejores Inteligencias Artificiales (IA) de 2025 - Greentech\n",
      "URL: https://www.greentecher.com/inteligencia-artificial-2025/?srsltid=AfmBOoooJyPCWPod-fOqaQf8FjZhFI_Cc1fFP-hAoOQ-S4o2vOrqqoCm\n",
      "Contenido: + Diplomado Internacional en Direcci√≥n y Gesti√≥n de Empresas IT (tecnol√≥gicas) + Diplomado Internacional de Consultor√≠a en Inteligencia Artificial, Automatizaciones y Agentes IA En 2025, la **inteligencia artificial (IA)** sigue transformando el panorama tecnol√≥gico con herramientas avanzadas que est√°n revolucionando c√≥mo las personas trabajan, crean y resuelven problemas. üöÄ Puedes estudiar en Greentech, y aprender de estas tecnolog√≠as, con el Diplomado en Innovaci√≥n, Inteligencia Artificial y Metaverso AQU√ç  **Inteligencia Artificial, IA, automatizaci√≥n, marketing digital, SEO, ChatGPT, MidJourney, DALL¬∑E, Deep Learning, Hugging Face, creaci√≥n de contenido, videos generados por IA, procesamiento de lenguaje natural, productividad, innovaci√≥n.** ai, chatgpt, computadora, futuro, gemini, ia, inteligencia, inteligencia artificial, midjourney, perplexiti, robotica, Tecnolog√≠a + Diplomado Internacional en Direcci√≥n y Gesti√≥n de Empresas IT\n",
      "\n",
      "--- Resultado 3 ---\n",
      "T√≠tulo: La Revoluci√≥n de la Inteligencia Artificial en 2025 - Digital Robots\n",
      "URL: https://www.digital-robots.com/noticias/la-revolucion-de-la-inteligencia-artificial-en-2025-un-futuro-transformador\n",
      "Contenido: From the formation of mixed work teams to the local regulation of technology, we see a scenario in which Artificial Intelligence) will be incorporated in a significant way in our lives and businesses. Artificial Intelligence) is at the height of a dramatic transformation by 2025, with a significant effect on the labor, technology and regulatory sectors. The incorporation of Artificial Intelligence) with **human workforces**, the creation of **specialized models** and local regulations constitute a new episode in the technological progression. **Robotic Process Automation (RPA)** has become a key tool for maximizing **sales conversions** by automating repetitive tasks and optimizing the use of time. RPA technology has revolutionized data processing by automating repetitive tasks, improving efficiency and reducing errors.\n",
      "\n",
      "--- Resultado 4 ---\n",
      "T√≠tulo: La Inteligencia Artificial: El Cambiador de 2025\n",
      "URL: https://edgecore.com/es/la-inteligencia-artificial-el-cambiador-de-2025/\n",
      "Contenido: En 2025, los desarrolladores conceptualizar√°n y adelantar√°n dise√±os que satisfagan los requisitos del futuro. Los centros de datos dise√±ados\n",
      "\n",
      "--- Resultado 5 ---\n",
      "T√≠tulo: El estado del arte de AI en 2025 - YouTube\n",
      "URL: https://www.youtube.com/watch?v=njgs9r-Z5iA\n",
      "Contenido: El estado del arte de AI en 2025\n",
      "Platzi\n",
      "1900000 subscribers\n",
      "8191 likes\n",
      "275147 views\n",
      "14 Jun 2025\n",
      "Este video te lleva por los retos, oportunidades y dilemas √©ticos de la inteligencia artificial en 2025, mostrando c√≥mo est√° transformando industrias, empleos y hasta la educaci√≥n. Si quieres entender el impacto real de AI y por qu√© este es el mejor momento para ser parte de la revoluci√≥n tecnol√≥gica, no te lo pierdas.\n",
      "------------------------------------\n",
      "üëâ Platzi es una plataforma de educaci√≥n Online, con tu suscripci√≥n tienes acceso a  cursos en diferentes √°reas de aprendizaje:\n",
      "\n",
      "- Desarrollo e ingenier√≠a\n",
      "- Inteligencia artificial\n",
      "- Data Science\n",
      "- Dise√±o y UX\n",
      "- Marketing\n",
      "- Negocios y emprendimiento\n",
      "- Producci√≥n audiovisual\n",
      "- Liderazgo y management\n",
      "\n",
      "Cada √°rea est√° compuesta por Rutas de aprendizaje y Escuelas que har√°n de tu perfil uno de los m√°s demandados de la industria. Adem√°s, cuentas con ADA -Asistente De Aprendizaje- creada con inteligencia artificial, grupos de estudio, meetups digitales y tutoriales en nuestro sistema de discusiones.\n",
      "\n",
      "Todo esto y m√°s, en https://platzi.com\n",
      "------------------------------------\n",
      "S√≠guenos\n",
      "Facebook: https://platzi.com/l/fHl6pows/\n",
      "Twitter: https://platzi.com/l/0DJ5PONB/\n",
      "Instagram: https://platzi.com/l/jt260ue0/\n",
      "@recent-platzi\n",
      "463 comments\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ======== Inicializar Tool ========\n",
    "web_tool = get_websearch_tool()\n",
    "\n",
    "# ======== Consulta ========\n",
    "query = \"√öltimos avances en inteligencia artificial en 2025\"\n",
    "print(\"üîç Consulta:\", query)\n",
    "print(\"------------------------------------------------------\")\n",
    "\n",
    "# Ejecutar b√∫squeda\n",
    "response = web_tool.run(query)\n",
    "\n",
    "print(\"üåê Resultados:\")\n",
    "print(response['query'])\n",
    "print(response)\n",
    "for i in range(len(response['results'])):\n",
    "    print(f\"\\n--- Resultado {i+1} ---\")\n",
    "    print(\"T√≠tulo:\", response['results'][i]['title'])\n",
    "    print(\"URL:\", response['results'][i]['url'])\n",
    "    print(\"Contenido:\", response['results'][i]['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63b96bb",
   "metadata": {},
   "source": [
    "# Fase 5.  Perfil, orquestacion y memoria del agente LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39915eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perfil que va a tener el agente conversacional para dar respuestas especializadas.\n",
    "AGENTE_PERFIL = \"\"\"\n",
    "Eres TEC-IA, un asistente especializado en Inteligencia Artificial,\n",
    "entrenado con apuntes del curso del TEC (II Semestre 2025).\n",
    "Tu prop√≥sito es responder de forma clara, concisa y t√©cnica.\n",
    "\n",
    "Reglas:\n",
    "- Usa primero la base de apuntes (RAG Tool) para responder.\n",
    "- Solo usa la WebSearch Tool si el usuario lo solicita expl√≠citamente\n",
    "  (por ejemplo: 'buscar en web:', 'web:', 'internet:', 'tavily:').\n",
    "- Siempre cita de qu√© documento o autor proviene la informaci√≥n de los apuntes.\n",
    "- No inventes citas ni digas 'no encontr√©'; si no hay suficiente informaci√≥n,\n",
    "  pide una reformulaci√≥n.\n",
    "- Mant√©n un tono explicativo y educativo, no rob√≥tico.\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c4f78d",
   "metadata": {},
   "source": [
    "# Orquestador para agente sliding\n",
    "\n",
    "El orquestador se encarga de coordinar las herramientas y el modelo principal para responder a las preguntas del usuario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bd388a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import deque\n",
    "from langchain_openai import ChatOpenAI\n",
    "from agente import AGENTE_PERFIL\n",
    "from fase4_ragtool import RAGTool   \n",
    "from fase4_webtool import get_websearch_tool\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Inicializar modelo principal (orquestador) \n",
    "llm_orq = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0.3)\n",
    "\n",
    "# Inicializar las herramientas con la carpeta donde est√°n los vectores\n",
    "collection_name = \"ai_apuntes_sliding_openai_v1\"\n",
    "persist_dir = \"data/vectorstores/chroma_sliding_openai_v1\"\n",
    "rag_tool = RAGTool(collection_name=collection_name, persist_dir=persist_dir)\n",
    "web_tool = get_websearch_tool()\n",
    "\n",
    "# Memoria de contexto limitada\n",
    "MEMORIA = deque(maxlen=6)\n",
    "\n",
    "def formatear_contexto():\n",
    "    contexto = \"\"\n",
    "    for rol, msg in MEMORIA:\n",
    "        contexto += f\"[{rol.upper()}]: {msg}\\n\"\n",
    "    return contexto.strip()\n",
    "\n",
    "# Detecci√≥n de uso de web\n",
    "def usuario_pide_web(pregunta: str) -> bool:\n",
    "    gatillos = [\"buscar en web:\", \"web:\", \"internet:\", \"tavily:\",\n",
    "                 \"buscar en internet:\", \"consulta en web:\", \"consulta en internet:\", \n",
    "                 \"investiga en web:\", \"investiga en internet:\"]\n",
    "    return any(g in pregunta.lower() for g in gatillos)\n",
    "\n",
    "# Funci√≥n principal del agente \n",
    "def responder_agente(pregunta: str):\n",
    "    usar_web = usuario_pide_web(pregunta)\n",
    "\n",
    "    # 1. Recuperar contexto de memoria\n",
    "    contexto = formatear_contexto()\n",
    "\n",
    "    # 2. Consultar herramienta adecuada\n",
    "    if usar_web:\n",
    "        tool_output = web_tool.run(pregunta)\n",
    "        tool_text = \"\\n\".join([\n",
    "            f\"- {r.get('title','(sin t√≠tulo)')} ‚Äî {r.get('url','')}\\n{r.get('content','')[:350]}...\"\n",
    "            for r in tool_output.get(\"results\", [])\n",
    "        ])\n",
    "    else:\n",
    "        tool_text = rag_tool._run(pregunta)\n",
    "\n",
    "    # 3. Construir prompt final\n",
    "    system_prompt = AGENTE_PERFIL\n",
    "    user_prompt = f\"\"\"\n",
    "Contexto previo:\n",
    "{contexto}\n",
    "\n",
    "Usuario pregunta:\n",
    "{pregunta}\n",
    "\n",
    "Informaci√≥n recuperada:\n",
    "{tool_text}\n",
    "\n",
    "Responde en espa√±ol de manera clara y t√©cnica.\n",
    "Si la informaci√≥n viene de apuntes, cita el documento o autor.\n",
    "Si la informaci√≥n viene de la web, cita la fuente (URL o medio).\n",
    "\"\"\"\n",
    "\n",
    "    completion = llm_orq.invoke([\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ])\n",
    "    respuesta = completion.content\n",
    "\n",
    "    # 4. Actualizar memoria\n",
    "    MEMORIA.append((\"user\", pregunta))\n",
    "    MEMORIA.append((\"assistant\", respuesta))\n",
    "\n",
    "    return respuesta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeedca7b",
   "metadata": {},
   "source": [
    "# Orquestador de agente de parrafos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "349cba92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import deque\n",
    "from langchain_openai import ChatOpenAI\n",
    "from agente import AGENTE_PERFIL\n",
    "from fase4_ragtool import RAGTool   \n",
    "from fase4_webtool import get_websearch_tool\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Inicializar modelo principal (orquestador) \n",
    "llm_orq = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0.3)\n",
    "\n",
    "# Inicializar las herramientas con la carpeta donde est√°n los vectores\n",
    "collection_name = \"ai_apuntes_paragraphs_openai_v1\"\n",
    "persist_dir = \"data/vectorstores/chroma_paragraphs_openai_v1\"\n",
    "rag_tool = RAGTool(collection_name=collection_name, persist_dir=persist_dir)\n",
    "web_tool = get_websearch_tool()\n",
    "\n",
    "# Memoria de contexto limitada\n",
    "MEMORIA = deque(maxlen=6)\n",
    "\n",
    "def formatear_contexto():\n",
    "    contexto = \"\"\n",
    "    for rol, msg in MEMORIA:\n",
    "        contexto += f\"[{rol.upper()}]: {msg}\\n\"\n",
    "    return contexto.strip()\n",
    "\n",
    "# Detecci√≥n de uso de web\n",
    "def usuario_pide_web(pregunta: str) -> bool:\n",
    "    gatillos = [\"buscar en web:\", \"web:\", \"internet:\", \"tavily:\",\n",
    "                 \"buscar en internet:\", \"consulta en web:\", \"consulta en internet:\", \n",
    "                 \"investiga en web:\", \"investiga en internet:\"]\n",
    "    return any(g in pregunta.lower() for g in gatillos)\n",
    "\n",
    "# Funci√≥n principal del agente \n",
    "def responder_agente(pregunta: str):\n",
    "    usar_web = usuario_pide_web(pregunta)\n",
    "\n",
    "    # 1. Recuperar contexto de memoria\n",
    "    contexto = formatear_contexto()\n",
    "\n",
    "    # 2. Consultar herramienta adecuada\n",
    "    if usar_web:\n",
    "        tool_output = web_tool.run(pregunta)\n",
    "        tool_text = \"\\n\".join([\n",
    "            f\"- {r.get('title','(sin t√≠tulo)')} ‚Äî {r.get('url','')}\\n{r.get('content','')[:350]}...\"\n",
    "            for r in tool_output.get(\"results\", [])\n",
    "        ])\n",
    "    else:\n",
    "        tool_text = rag_tool._run(pregunta)\n",
    "\n",
    "    # 3. Construir prompt final\n",
    "    system_prompt = AGENTE_PERFIL\n",
    "    user_prompt = f\"\"\"\n",
    "Contexto previo:\n",
    "{contexto}\n",
    "\n",
    "Usuario pregunta:\n",
    "{pregunta}\n",
    "\n",
    "Informaci√≥n recuperada:\n",
    "{tool_text}\n",
    "\n",
    "Responde en espa√±ol de manera clara y t√©cnica.\n",
    "Si la informaci√≥n viene de apuntes, cita el documento o autor.\n",
    "Si la informaci√≥n viene de la web, cita la fuente (URL o medio).\n",
    "\"\"\"\n",
    "\n",
    "    completion = llm_orq.invoke([\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ])\n",
    "    respuesta = completion.content\n",
    "\n",
    "    # 4. Actualizar memoria\n",
    "    MEMORIA.append((\"user\", pregunta))\n",
    "    MEMORIA.append((\"assistant\", respuesta))\n",
    "\n",
    "    return respuesta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83fc97e",
   "metadata": {},
   "source": [
    "# Fase 6. Aplicaci√≥n metodo de sliding\n",
    "\n",
    "Streamlit no puede ejecutarse en jupyter notebook, por lo que este c√≥digo debe ir en un archivo .py aparte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decfb3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import time\n",
    "import random\n",
    "from orquestador_sliding import responder_agente\n",
    "\n",
    "# CONFIGURACI√ìN DE P√ÅGINA \n",
    "\n",
    "st.set_page_config(page_title=\"TEC-IA Asistente con m√©todo de sliding\", page_icon=\"üß†\", layout=\"centered\")\n",
    "\n",
    "#  ESTILO LIMPIO Y ESTABLE \n",
    "st.markdown(\"\"\"\n",
    "<style>\n",
    ".stApp {\n",
    "    background: #f4f6fa;\n",
    "    font-family: 'Segoe UI', sans-serif;\n",
    "}\n",
    "\n",
    "/* T√≠tulo */\n",
    "h1 {\n",
    "    text-align: center;\n",
    "    font-size: 2.6rem;\n",
    "    color: #002b5c;\n",
    "    margin-bottom: 0.3rem;\n",
    "    font-weight: 700;\n",
    "}\n",
    ".subtitle {\n",
    "    text-align: center;\n",
    "    color: #3b6ea8;\n",
    "    font-size: 1.1rem;\n",
    "    margin-bottom: 1.2rem;\n",
    "}\n",
    "\n",
    "/* Chat container */\n",
    ".chat-box {\n",
    "    background: white;\n",
    "    padding: 18px;\n",
    "    border-radius: 14px;\n",
    "    max-height: 65vh;\n",
    "    overflow-y: auto;\n",
    "    border: 1px solid #d9e2ec;\n",
    "    box-shadow: 0 4px 12px rgba(0,0,0,0.05);\n",
    "    margin-bottom: 15px;\n",
    "}\n",
    "\n",
    "/* Mensajes */\n",
    ".user-msg {\n",
    "    background: #002b5c;\n",
    "    color: white;\n",
    "    padding: 10px 14px;\n",
    "    border-radius: 16px 16px 4px 16px;\n",
    "    margin-left: auto;\n",
    "    max-width: 75%;\n",
    "    margin-bottom: 12px;\n",
    "}\n",
    ".bot-msg {\n",
    "    background: #e8f0fa;\n",
    "    color: #002b5c;\n",
    "    padding: 10px 14px;\n",
    "    border-radius: 16px 16px 16px 4px;\n",
    "    margin-right: auto;\n",
    "    max-width: 75%;\n",
    "    margin-bottom: 12px;\n",
    "}\n",
    "\n",
    "/* Input */\n",
    ".stChatInput > div > div {\n",
    "    background: white !important;\n",
    "    border-radius: 18px !important;\n",
    "    border: 1px solid #d9e2ec;\n",
    "}\n",
    "</style>\n",
    "\"\"\", unsafe_allow_html=True)\n",
    "\n",
    "#  ESTADO DEL CHAT \n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = [\n",
    "        {\"role\": \"assistant\", \"content\": \"¬°Hola! Soy **TEC-IA** ü§ñ. Estoy aqu√≠ para ayudarte con teor√≠a, pr√°ctica y conceptos de Inteligencia Artificial del TEC. Preg√∫ntame algo üëá.\"}\n",
    "    ]\n",
    "\n",
    "# ENCABEZADO \n",
    "st.markdown(\"\"\"\n",
    "<div style=\"text-align:center;\">\n",
    "    <img src=\"https://cdn-icons-png.flaticon.com/512/14313/14313824.png\" width=\"85\">\n",
    "    <h1 style=\"margin-top: 10px; margin-bottom: 4px; font-weight: 700; color:#002b5c;\">\n",
    "        TEC-IA\n",
    "    </h1>\n",
    "    <div style=\"color:#3b6ea8; font-size: 1.1rem; margin-bottom: 14px;\">\n",
    "        Asistente para el curso de Inteligencia Artificial ‚Äî TEC\n",
    "    </div>\n",
    "</div>\n",
    "<hr style=\"border: 0; height: 1px; background: #d9e2ec; margin-top: 4px; margin-bottom: 18px;\">\n",
    "\"\"\", unsafe_allow_html=True)\n",
    "for msg in st.session_state.messages:\n",
    "    if msg[\"role\"] == \"user\":\n",
    "        st.markdown(f\"<div class='user-msg'>{msg['content']}</div>\", unsafe_allow_html=True)\n",
    "    else:\n",
    "        st.markdown(f\"<div class='bot-msg'>{msg['content']}</div>\", unsafe_allow_html=True)\n",
    "\n",
    "st.markdown(\"</div>\", unsafe_allow_html=True)\n",
    "\n",
    "# RESPUESTA SIMULADA \n",
    "def reply(prompt):\n",
    "    time.sleep(1.0)\n",
    "    respuestas = [\n",
    "        f\"Interesante pregunta sobre **{prompt}**. ¬øQuieres que lo explique paso a paso o con un ejemplo pr√°ctico?\",\n",
    "        f\"**{prompt}** se estudia en IA porque ayuda a entender c√≥mo los sistemas pueden aprender patrones.\",\n",
    "        f\"Te explico **{prompt}** de manera sencilla: ...\"\n",
    "    ]\n",
    "    return random.choice(respuestas)\n",
    "\n",
    "# INPUT DEL CHAT\n",
    "prompt = st.chat_input(\"Escribe tu pregunta...\")\n",
    "\n",
    "if prompt:\n",
    "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    answer = responder_agente(prompt)\n",
    "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": answer})\n",
    "    st.rerun()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7763e588",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Popen: returncode: None args: ['streamlit', 'run', 'app.py']>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  You can now view your Streamlit app in your browser.\n",
      "\n",
      "  Local URL: http://localhost:8502\n",
      "  Network URL: http://192.168.100.58:8502\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "subprocess.Popen([\"streamlit\", \"run\", \"app_sliding.py\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3d904f",
   "metadata": {},
   "source": [
    "# Aplicacion metodo de parrafos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb83536",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import time\n",
    "import random\n",
    "from orquestador_paragraph import responder_agente\n",
    "\n",
    "# CONFIGURACI√ìN DE P√ÅGINA \n",
    "\n",
    "st.set_page_config(page_title=\"TEC-IA Asistente con m√©todo de p√°rrafo\", page_icon=\"üß†\", layout=\"centered\")\n",
    "\n",
    "#  ESTILO LIMPIO Y ESTABLE \n",
    "st.markdown(\"\"\"\n",
    "<style>\n",
    ".stApp {\n",
    "    background: #f4f6fa;\n",
    "    font-family: 'Segoe UI', sans-serif;\n",
    "}\n",
    "\n",
    "/* T√≠tulo */\n",
    "h1 {\n",
    "    text-align: center;\n",
    "    font-size: 2.6rem;\n",
    "    color: #002b5c;\n",
    "    margin-bottom: 0.3rem;\n",
    "    font-weight: 700;\n",
    "}\n",
    ".subtitle {\n",
    "    text-align: center;\n",
    "    color: #3b6ea8;\n",
    "    font-size: 1.1rem;\n",
    "    margin-bottom: 1.2rem;\n",
    "}\n",
    "\n",
    "/* Chat container */\n",
    ".chat-box {\n",
    "    background: white;\n",
    "    padding: 18px;\n",
    "    border-radius: 14px;\n",
    "    max-height: 65vh;\n",
    "    overflow-y: auto;\n",
    "    border: 1px solid #d9e2ec;\n",
    "    box-shadow: 0 4px 12px rgba(0,0,0,0.05);\n",
    "    margin-bottom: 15px;\n",
    "}\n",
    "\n",
    "/* Mensajes */\n",
    ".user-msg {\n",
    "    background: #002b5c;\n",
    "    color: white;\n",
    "    padding: 10px 14px;\n",
    "    border-radius: 16px 16px 4px 16px;\n",
    "    margin-left: auto;\n",
    "    max-width: 75%;\n",
    "    margin-bottom: 12px;\n",
    "}\n",
    ".bot-msg {\n",
    "    background: #e8f0fa;\n",
    "    color: #002b5c;\n",
    "    padding: 10px 14px;\n",
    "    border-radius: 16px 16px 16px 4px;\n",
    "    margin-right: auto;\n",
    "    max-width: 75%;\n",
    "    margin-bottom: 12px;\n",
    "}\n",
    "\n",
    "/* Input */\n",
    ".stChatInput > div > div {\n",
    "    background: white !important;\n",
    "    border-radius: 18px !important;\n",
    "    border: 1px solid #d9e2ec;\n",
    "}\n",
    "</style>\n",
    "\"\"\", unsafe_allow_html=True)\n",
    "\n",
    "#  ESTADO DEL CHAT \n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = [\n",
    "        {\"role\": \"assistant\", \"content\": \"¬°Hola! Soy **TEC-IA** ü§ñ. Estoy aqu√≠ para ayudarte con teor√≠a, pr√°ctica y conceptos de Inteligencia Artificial del TEC. Preg√∫ntame algo üëá.\"}\n",
    "    ]\n",
    "\n",
    "# ENCABEZADO \n",
    "st.markdown(\"\"\"\n",
    "<div style=\"text-align:center;\">\n",
    "    <img src=\"https://cdn-icons-png.flaticon.com/512/14313/14313824.png\" width=\"85\">\n",
    "    <h1 style=\"margin-top: 10px; margin-bottom: 4px; font-weight: 700; color:#002b5c;\">\n",
    "        TEC-IA\n",
    "    </h1>\n",
    "    <div style=\"color:#3b6ea8; font-size: 1.1rem; margin-bottom: 14px;\">\n",
    "        Asistente para el curso de Inteligencia Artificial ‚Äî TEC\n",
    "    </div>\n",
    "</div>\n",
    "<hr style=\"border: 0; height: 1px; background: #d9e2ec; margin-top: 4px; margin-bottom: 18px;\">\n",
    "\"\"\", unsafe_allow_html=True)\n",
    "for msg in st.session_state.messages:\n",
    "    if msg[\"role\"] == \"user\":\n",
    "        st.markdown(f\"<div class='user-msg'>{msg['content']}</div>\", unsafe_allow_html=True)\n",
    "    else:\n",
    "        st.markdown(f\"<div class='bot-msg'>{msg['content']}</div>\", unsafe_allow_html=True)\n",
    "\n",
    "st.markdown(\"</div>\", unsafe_allow_html=True)\n",
    "\n",
    "# RESPUESTA SIMULADA \n",
    "def reply(prompt):\n",
    "    time.sleep(1.0)\n",
    "    respuestas = [\n",
    "        f\"Interesante pregunta sobre **{prompt}**. ¬øQuieres que lo explique paso a paso o con un ejemplo pr√°ctico?\",\n",
    "        f\"**{prompt}** se estudia en IA porque ayuda a entender c√≥mo los sistemas pueden aprender patrones.\",\n",
    "        f\"Te explico **{prompt}** de manera sencilla: ...\"\n",
    "    ]\n",
    "    return random.choice(respuestas)\n",
    "\n",
    "# INPUT DEL CHAT\n",
    "prompt = st.chat_input(\"Escribe tu pregunta...\")\n",
    "\n",
    "if prompt:\n",
    "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    answer = responder_agente(prompt)\n",
    "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": answer})\n",
    "    st.rerun()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a48822dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Popen: returncode: None args: ['streamlit', 'run', 'app_paragraph.py']>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  You can now view your Streamlit app in your browser.\n",
      "\n",
      "  Local URL: http://localhost:8503\n",
      "  Network URL: http://192.168.100.58:8503\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "subprocess.Popen([\"streamlit\", \"run\", \"app_paragraph.py\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
