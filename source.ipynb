{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10918002",
   "metadata": {},
   "source": [
    "# Fase 1. Recolecci√≥n y organizaci√≥n de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be5f26d",
   "metadata": {},
   "source": [
    "En docs \"lineamientos_nomenclatura_y_citacion.md\" est√° la informaci√≥n por si quieren guiarse y revisar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c92d8c2",
   "metadata": {},
   "source": [
    "# Fase 2. Procesamiento de texto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9be9e59",
   "metadata": {},
   "source": [
    "## Extracci√≥n del texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "073fa793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 46 archivos extra√≠dos correctamente a data/apuntes_clean/raw\n"
     ]
    }
   ],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "import os, re\n",
    "\n",
    "# Directorio base del proyecto\n",
    "BASE_DIR = \"data\"\n",
    "RAW_PDF_DIR = os.path.join(BASE_DIR, \"apuntes_raw\")\n",
    "OUT_RAW_DIR = os.path.join(BASE_DIR, \"apuntes_clean\", \"raw\")\n",
    "\n",
    "# Crear la carpeta de salida si no existe\n",
    "os.makedirs(OUT_RAW_DIR, exist_ok=True)\n",
    "\n",
    "# Buscar todos los PDFs en la carpeta apuntes_raw\n",
    "pdf_files = [f for f in os.listdir(RAW_PDF_DIR) if f.lower().endswith(\".pdf\")]\n",
    "\n",
    "for pdf_file in pdf_files:\n",
    "    pdf_path = os.path.join(RAW_PDF_DIR, pdf_file)\n",
    "    txt_name = os.path.splitext(pdf_file)[0] + \".txt\"\n",
    "    out_txt_path = os.path.join(OUT_RAW_DIR, txt_name)\n",
    "\n",
    "    reader = PdfReader(pdf_path)\n",
    "    text_pages = []\n",
    "    for page in reader.pages:\n",
    "        text_pages.append(page.extract_text() or \"\")\n",
    "    \n",
    "    # Unir todas las p√°ginas con doble salto de l√≠nea\n",
    "    full_text = \"\\n\\n\".join(text_pages)\n",
    "    with open(out_txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(full_text)\n",
    "\n",
    "print(f\"‚úÖ {len(pdf_files)} archivos extra√≠dos correctamente a {OUT_RAW_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ac2e1a",
   "metadata": {},
   "source": [
    "## Normalizaci√≥n de txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9619af32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 46 archivos corregidos y guardados en data/apuntes_clean/normalized\n"
     ]
    }
   ],
   "source": [
    "import os, re, unicodedata\n",
    "\n",
    "BASE_DIR = \"data\"\n",
    "CLEAN_IN_DIR  = os.path.join(BASE_DIR, \"apuntes_clean\", \"raw\")        # carpeta de entrada\n",
    "CLEAN_OUT_DIR = os.path.join(BASE_DIR, \"apuntes_clean\", \"normalized\") # carpeta de salida\n",
    "os.makedirs(CLEAN_OUT_DIR, exist_ok=True)\n",
    "\n",
    "def quitar_tildes_y_reparar_espacios(texto: str) -> str:\n",
    "    # 1) Normalizaci√≥n Unicode para exponer diacr√≠ticos combinantes\n",
    "    t = unicodedata.normalize(\"NFD\", texto)\n",
    "\n",
    "    # 2) Reemplazos t√≠picos de PDF \n",
    "    t = (t.replace(\"\\u00A0\", \" \")      # NBSP -> espacio normal\n",
    "           .replace(\"\\u00AD\", \"\")      # soft hyphen -> nada\n",
    "           .replace(\"Ô¨Å\", \"fi\").replace(\"Ô¨Ç\", \"fl\")  # ligaduras\n",
    "           .replace(\"\\u0131\", \"i\")     # ƒ± (i sin punto) -> i\n",
    "           .replace(\"Àô\", \"\").replace(\"`\", \"\").replace(\"¬®\", \"\").replace(\"ÀÜ\", \"\"))\n",
    "\n",
    "    # 3) Unir SOLO cuando hay acento suelto entre letras: \"implementaci ¬¥on\" -> \"implementacion\"\n",
    "    t = re.sub(r\"([A-Za-z√±√ë])\\s*[\\u00B4\\u0301]\\s*([A-Za-z√±√ë])\", r\"\\1\\2\", t)\n",
    "\n",
    "    # 4) Convertir virgulilla suelta (~ o \\u02DC) en √ë/√± cuando corresponde (p.ej. \"tama Àúno\" -> \"tama√±o\")\n",
    "    #    a) letra + ~ + n/N\n",
    "    t = re.sub(r\"([A-Za-z√±√ë])\\s*[\\u02DC~]\\s*([Nn])\",\n",
    "               lambda m: m.group(1) + (\"√ë\" if m.group(2).isupper() else \"√±\"),\n",
    "               t)\n",
    "    #    b) ~ al inicio o tras espacio antes de n/N + vocal (p.ej. \" Àúno \" -> \" √±o \")\n",
    "    t = re.sub(r\"(?<!\\S)[\\u02DC~]\\s*([Nn])(?=[aeiou√°√©√≠√≥√∫AEIOU√Å√â√ç√ì√ö])\",\n",
    "               lambda m: (\"√ë\" if m.group(1).isupper() else \"√±\"),\n",
    "               t)\n",
    "\n",
    "    # 5) Eliminar diacr√≠ticos (tildes) PERO conservar √±/√ë\n",
    "    t = ''.join(c for c in t if unicodedata.category(c) != 'Mn' or c.lower() == '√±')\n",
    "\n",
    "    # 6) Limpieza suave: colapsar espacios repetidos y limitar saltos\n",
    "    t = re.sub(r\"[ \\t]+\", \" \", t)\n",
    "    t = re.sub(r\"\\n{3,}\", \"\\n\\n\", t)\n",
    "\n",
    "    return t.strip()\n",
    "\n",
    "# Aplicar a todos los .txt de entrada\n",
    "count = 0\n",
    "for fname in os.listdir(CLEAN_IN_DIR):\n",
    "    if not fname.lower().endswith(\".txt\"):\n",
    "        continue\n",
    "\n",
    "    with open(os.path.join(CLEAN_IN_DIR, fname), \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        raw = f.read()\n",
    "\n",
    "    norm = quitar_tildes_y_reparar_espacios(raw)\n",
    "\n",
    "    with open(os.path.join(CLEAN_OUT_DIR, fname), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(norm)\n",
    "    count += 1\n",
    "\n",
    "print(f\"‚úÖ {count} archivos corregidos y guardados en {CLEAN_OUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31180d1",
   "metadata": {},
   "source": [
    "## Segmentaci√≥n (P√°rrafos y Ventanas Deslizantes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f353734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Segmentaci√≥n regenerada con par√°metros ajustados.\n",
      " - √çndice p√°rrafos:  data/chunks_paragraphs/index_paragraphs.csv\n",
      " - √çndice ventanas:  data/chunks_sliding/index_sliding.csv\n",
      " - Resumen:          data/chunks_summary.csv\n",
      " - Carpeta chunks A: data/chunks_paragraphs\n",
      " - Carpeta chunks B: data/chunks_sliding\n"
     ]
    }
   ],
   "source": [
    "import os, re, csv, shutil\n",
    "from statistics import mean\n",
    "\n",
    "BASE_DIR = \"data\"\n",
    "INPUT_DIR = os.path.join(BASE_DIR, \"apuntes_clean\", \"normalized\")\n",
    "\n",
    "# --- P√°rrafos ---\n",
    "PAR_MIN_CHARS = 480\n",
    "PAR_MAX_CHARS = 2000\n",
    "MERGE_TITLES = True\n",
    "TITLE_MAX_CHARS = 140\n",
    "\n",
    "# --- Ventanas deslizantes ---\n",
    "WIN_WORDS = 240\n",
    "WIN_OVERLAP = 0.20\n",
    "WIN_STRIDE = max(1, int(WIN_WORDS * (1 - WIN_OVERLAP)))\n",
    "\n",
    "# --- Salidas ---\n",
    "OUT_PAR_DIR = os.path.join(BASE_DIR, \"chunks_paragraphs\")\n",
    "OUT_WIN_DIR = os.path.join(BASE_DIR, \"chunks_sliding\")\n",
    "\n",
    "# Limpiar salidas anteriores para √≠ndices consistentes\n",
    "for d in (OUT_PAR_DIR, OUT_WIN_DIR):\n",
    "    if os.path.exists(d):\n",
    "        shutil.rmtree(d)\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "IDX_PAR_CSV = os.path.join(OUT_PAR_DIR, \"index_paragraphs.csv\")\n",
    "IDX_WIN_CSV = os.path.join(OUT_WIN_DIR, \"index_sliding.csv\")\n",
    "SUMMARY_CSV = os.path.join(BASE_DIR, \"chunks_summary.csv\")\n",
    "\n",
    "# ===================== UTILIDADES =====================\n",
    "def read_txt(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        return f.read()\n",
    "\n",
    "def safe_filename_stem(fname):\n",
    "    return os.path.splitext(os.path.basename(fname))[0]\n",
    "\n",
    "def split_paragraphs(text):\n",
    "    raw_pars = [p.strip() for p in re.split(r\"\\n\\s*\\n\", text) if p.strip()]\n",
    "    pars = []\n",
    "    for p in raw_pars:\n",
    "        p = re.sub(r\"[ \\t]+\", \" \", p).strip()\n",
    "        pars.append(p)\n",
    "    return pars\n",
    "\n",
    "def split_sentences(p):\n",
    "    parts = re.split(r\"(?<=[\\.\\?\\!])\\s+\", p)\n",
    "    return [s.strip() for s in parts if s.strip()]\n",
    "\n",
    "def repartition_long_paragraph(p, max_chars):\n",
    "    if len(p) <= max_chars:\n",
    "        return [p]\n",
    "    sent = split_sentences(p)\n",
    "    chunks, buf = [], \"\"\n",
    "    for s in sent:\n",
    "        if not buf:\n",
    "            buf = s\n",
    "        elif len(buf) + 1 + len(s) <= max_chars:\n",
    "            buf = buf + \" \" + s\n",
    "        else:\n",
    "            chunks.append(buf.strip())\n",
    "            buf = s\n",
    "    if buf:\n",
    "        chunks.append(buf.strip())\n",
    "    final = []\n",
    "    for c in chunks:\n",
    "        if len(c) <= max_chars:\n",
    "            final.append(c)\n",
    "        else:\n",
    "            for i in range(0, len(c), max_chars):\n",
    "                final.append(c[i:i+max_chars].strip())\n",
    "    return final\n",
    "\n",
    "def fuse_short_paragraphs(pars, min_chars, merge_titles, title_max):\n",
    "    out = []\n",
    "    i = 0\n",
    "    while i < len(pars):\n",
    "        cur = pars[i]\n",
    "        is_title_like = merge_titles and (len(cur) <= title_max and \"\\n\" not in cur and len(cur.split()) <= 16)\n",
    "        if is_title_like and i + 1 < len(pars):\n",
    "            merged = (cur + \" ‚Äî \" + pars[i+1]).strip()\n",
    "            out.append(merged)\n",
    "            i += 2\n",
    "            continue\n",
    "        if len(cur) < min_chars and i + 1 < len(pars):\n",
    "            merged = (cur + \" \" + pars[i+1]).strip()\n",
    "            out.append(merged)\n",
    "            i += 2\n",
    "        else:\n",
    "            out.append(cur)\n",
    "            i += 1\n",
    "    return out\n",
    "\n",
    "def ensure_dir(d):\n",
    "    if not os.path.exists(d):\n",
    "        os.makedirs(d, exist_ok=True)\n",
    "\n",
    "def write_chunk(path, text):\n",
    "    ensure_dir(os.path.dirname(path))\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text.strip())\n",
    "\n",
    "def word_tokenize(text):\n",
    "    return re.findall(r\"\\S+\", text)\n",
    "\n",
    "# ===================== M√âTODO A: P√ÅRRAFOS =====================\n",
    "par_rows = []\n",
    "summary_rows = []\n",
    "\n",
    "files = [f for f in os.listdir(INPUT_DIR) if f.lower().endswith(\".txt\")]\n",
    "for fname in sorted(files):\n",
    "    path_in = os.path.join(INPUT_DIR, fname)\n",
    "    base = safe_filename_stem(fname)\n",
    "    out_dir_doc = os.path.join(OUT_PAR_DIR, base)\n",
    "    ensure_dir(out_dir_doc)\n",
    "\n",
    "    txt = read_txt(path_in)\n",
    "    pars = split_paragraphs(txt)\n",
    "    pars = fuse_short_paragraphs(pars, PAR_MIN_CHARS, MERGE_TITLES, TITLE_MAX_CHARS)\n",
    "\n",
    "    final_pars = []\n",
    "    for p in pars:\n",
    "        final_pars.extend(repartition_long_paragraph(p, PAR_MAX_CHARS))\n",
    "\n",
    "    lengths = []\n",
    "    for idx, chunk in enumerate(final_pars, start=1):\n",
    "        chunk_name = f\"chunk_{idx:04d}.txt\"\n",
    "        out_path = os.path.join(out_dir_doc, chunk_name)\n",
    "        write_chunk(out_path, chunk)\n",
    "        lengths.append(len(chunk))\n",
    "        par_rows.append({\n",
    "            \"filename_base\": base,\n",
    "            \"method\": \"paragraphs\",\n",
    "            \"chunk_id\": f\"{base}-p-{idx:04d}\",\n",
    "            \"chunk_path\": os.path.relpath(out_path, BASE_DIR).replace(\"\\\\\",\"/\"),\n",
    "            \"char_len\": len(chunk),\n",
    "            \"word_len\": len(chunk.split()),\n",
    "            \"paragraph_idx\": idx\n",
    "        })\n",
    "\n",
    "    summary_rows.append({\n",
    "        \"filename_base\": base,\n",
    "        \"method\": \"paragraphs\",\n",
    "        \"n_chunks\": len(lengths),\n",
    "        \"char_mean\": round(mean(lengths), 1) if lengths else 0,\n",
    "        \"pct_short_<300\": round(100*sum(l<300 for l in lengths)/len(lengths), 1) if lengths else 0\n",
    "    })\n",
    "\n",
    "if par_rows:\n",
    "    with open(IDX_PAR_CSV, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=list(par_rows[0].keys()))\n",
    "        writer.writeheader()\n",
    "        writer.writerows(par_rows)\n",
    "\n",
    "# ===================== M√âTODO B: VENTANAS DESLIZANTES =====================\n",
    "win_rows = []\n",
    "for fname in sorted(files):\n",
    "    path_in = os.path.join(INPUT_DIR, fname)\n",
    "    base = safe_filename_stem(fname)\n",
    "    out_dir_doc = os.path.join(OUT_WIN_DIR, base)\n",
    "    ensure_dir(out_dir_doc)\n",
    "\n",
    "    txt = read_txt(path_in)\n",
    "    words = word_tokenize(txt)\n",
    "    n = len(words)\n",
    "    lengths = []\n",
    "\n",
    "    if n == 0:\n",
    "        summary_rows.append({\n",
    "            \"filename_base\": base,\n",
    "            \"method\": \"sliding\",\n",
    "            \"n_chunks\": 0, \"char_mean\": 0, \"pct_short_<300\": 0\n",
    "            })\n",
    "        continue\n",
    "\n",
    "    idx = 0\n",
    "    win_id = 1\n",
    "    while idx < n:\n",
    "        end = min(n, idx + WIN_WORDS)\n",
    "        w_chunk = words[idx:end]\n",
    "        chunk = \" \".join(w_chunk).strip()\n",
    "        if not chunk:\n",
    "            break\n",
    "\n",
    "        chunk_name = f\"chunk_{win_id:04d}.txt\"\n",
    "        out_path = os.path.join(out_dir_doc, chunk_name)\n",
    "        write_chunk(out_path, chunk)\n",
    "\n",
    "        lengths.append(len(chunk))\n",
    "        win_rows.append({\n",
    "            \"filename_base\": base,\n",
    "            \"method\": \"sliding\",\n",
    "            \"chunk_id\": f\"{base}-w-{win_id:04d}\",\n",
    "            \"chunk_path\": os.path.relpath(out_path, BASE_DIR).replace(\"\\\\\",\"/\"),\n",
    "            \"char_len\": len(chunk),\n",
    "            \"word_len\": len(w_chunk),\n",
    "            \"start_word\": idx,\n",
    "            \"end_word\": end\n",
    "        })\n",
    "\n",
    "        win_id += 1\n",
    "        if end == n:\n",
    "            break\n",
    "        idx += WIN_STRIDE\n",
    "\n",
    "    summary_rows.append({\n",
    "        \"filename_base\": base,\n",
    "        \"method\": \"sliding\",\n",
    "        \"n_chunks\": len(lengths),\n",
    "        \"char_mean\": round(mean(lengths), 1) if lengths else 0,\n",
    "        \"pct_short_<300\": round(100*sum(l<300 for l in lengths)/len(lengths), 1) if lengths else 0\n",
    "    })\n",
    "\n",
    "if win_rows:\n",
    "    with open(IDX_WIN_CSV, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=list(win_rows[0].keys()))\n",
    "        writer.writeheader()\n",
    "        writer.writerows(win_rows)\n",
    "\n",
    "# ===================== RESUMEN =====================\n",
    "if summary_rows:\n",
    "    with open(SUMMARY_CSV, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=list(summary_rows[0].keys()))\n",
    "        writer.writeheader()\n",
    "        writer.writerows(summary_rows)\n",
    "\n",
    "print(\"‚úÖ Segmentaci√≥n regenerada con par√°metros ajustados.\")\n",
    "print(f\" - √çndice p√°rrafos:  {IDX_PAR_CSV}\")\n",
    "print(f\" - √çndice ventanas:  {IDX_WIN_CSV}\")\n",
    "print(f\" - Resumen:          {SUMMARY_CSV}\")\n",
    "print(f\" - Carpeta chunks A: {OUT_PAR_DIR}\")\n",
    "print(f\" - Carpeta chunks B: {OUT_WIN_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b23516",
   "metadata": {},
   "source": [
    "# Fase 3. Tokenizaci√≥n y Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a786f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documentos (unique filename_base): 46\n",
      "Total de chunks en √≠ndice: 386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ingestando chunks en Chroma: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 386/386 [00:07<00:00, 48.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Embeddings generados e indexados.\n",
      "üìö Collection: ai_apuntes_sliding_openai_v1 | count = 386\n",
      "üíæ Persist dir: data/vectorstores/chroma_sliding_openai_v1\n"
     ]
    }
   ],
   "source": [
    "import os, csv, time, math\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# (A) ‚Äî‚Äî CONFIGURACI√ìN GENERAL \n",
    "BASE_DIR       = \"data\"\n",
    "INDEX_CSV      = os.path.join(BASE_DIR, \"chunks_sliding\", \"index_sliding.csv\")   # puedes cambiar a p√°rrafos si quieres\n",
    "PERSIST_DIR    = os.path.join(BASE_DIR, \"vectorstores\", \"chroma_sliding_openai_v1\")\n",
    "COLLECTION_NAME= \"ai_apuntes_sliding_openai_v1\"\n",
    "\n",
    "# Proveedor de embeddings: \"openai\" o \"local\"\n",
    "PROVIDER       = \"openai\"       \n",
    "OPENAI_MODEL   = \"text-embedding-3-small\"      \n",
    "LOCAL_MODEL    = \"all-MiniLM-L6-v2\"           \n",
    "\n",
    "BATCH_SIZE     = 128            # tama√±o de lote para ingesti√≥n\n",
    "MAX_RETRIES    = 5              # reintentos por rate-limit/errores transitorios\n",
    "RETRY_BASE_SEC = 2              # backoff exponencial\n",
    "\n",
    "# (B) ‚Äî‚Äî SETUP DE EMBEDDINGS \n",
    "embed_dims = None\n",
    "\n",
    "if PROVIDER == \"openai\":\n",
    "    from openai import OpenAI\n",
    "    OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "    if not OPENAI_API_KEY:\n",
    "        raise ValueError(\"La variable de entorno OPENAI_API_KEY no est√° configurada.\")\n",
    "    \n",
    "    oai_client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "    def embed_texts(texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"Embeddings con OpenAI + reintentos.\"\"\"\n",
    "        for attempt in range(MAX_RETRIES):\n",
    "            try:\n",
    "                resp = oai_client.embeddings.create(\n",
    "                    model=OPENAI_MODEL,\n",
    "                    input=texts\n",
    "                )\n",
    "                vecs = [d.embedding for d in resp.data]\n",
    "                return vecs\n",
    "            except Exception as e:\n",
    "                wait = RETRY_BASE_SEC * (2 ** attempt)\n",
    "                print(f\"[WARN] Error {e}. Reintentando en {wait}s...\")\n",
    "                if attempt == MAX_RETRIES - 1:\n",
    "                    raise\n",
    "                time.sleep(wait)\n",
    "\n",
    "elif PROVIDER == \"local\":\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    st_model = SentenceTransformer(LOCAL_MODEL)\n",
    "\n",
    "    def embed_texts(texts: List[str]) -> List[List[float]]:\n",
    "        return st_model.encode(texts, convert_to_numpy=False, normalize_embeddings=False).tolist()\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"PROVIDER debe ser 'openai' o 'local'.\")\n",
    "\n",
    "# (C) ‚Äî‚Äî INICIALIZAR CHROMA PERSISTENTE \n",
    "os.makedirs(PERSIST_DIR, exist_ok=True)\n",
    "\n",
    "client = chromadb.PersistentClient(\n",
    "    path=PERSIST_DIR,\n",
    "    settings=Settings(is_persistent=True)\n",
    ")\n",
    "\n",
    "# Crear o recuperar la colecci√≥n\n",
    "try:\n",
    "    collection = client.get_collection(COLLECTION_NAME)\n",
    "except:\n",
    "    collection = client.create_collection(\n",
    "        name=COLLECTION_NAME,\n",
    "        metadata={\"hnsw:space\": \"cosine\"}  # distancia coseno\n",
    "    )\n",
    "\n",
    "# (D) ‚Äî‚Äî UTILIDADES \n",
    "def read_index_rows(csv_path: str):\n",
    "    rows = []\n",
    "    with open(csv_path, \"r\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for r in reader:\n",
    "            rows.append(r)\n",
    "    return rows\n",
    "\n",
    "def load_chunk_text(chunk_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Abre el archivo de texto del chunk, corrigiendo rutas relativas.\n",
    "    \"\"\"\n",
    "    # Si el path ya incluye \"data/\", se queda tal cual\n",
    "    if not os.path.isabs(chunk_path):\n",
    "        # Si empieza por \"data/\", lo consideramos relativo al proyecto\n",
    "        if chunk_path.startswith(\"data/\") or chunk_path.startswith(\".\\\\data\\\\\") or chunk_path.startswith(\".\\\\chunks_\"):\n",
    "            path = os.path.normpath(chunk_path)\n",
    "        else:\n",
    "            # Si viene solo 'chunks_sliding/...', le anteponemos 'data/'\n",
    "            path = os.path.join(\"data\", chunk_path)\n",
    "    else:\n",
    "        path = chunk_path\n",
    "\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"No se encontr√≥ el archivo: {path}\")\n",
    "\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        return f.read().strip()\n",
    "\n",
    "\n",
    "# (E) ‚Äî‚Äî CARGAR √çNDICE Y PREPARAR INGESTA \n",
    "rows = read_index_rows(INDEX_CSV)\n",
    "print(f\"Documentos (unique filename_base): {len(set(r['filename_base'] for r in rows))}\")\n",
    "print(f\"Total de chunks en √≠ndice: {len(rows)}\")\n",
    "\n",
    "# (F) ‚Äî‚Äî INGESTA EN LOTES CON EMBEDDINGS \n",
    "ids, docs, metas = [], [], []\n",
    "\n",
    "def flush_batch():\n",
    "    if not ids:\n",
    "        return\n",
    "    # Calcula embeddings del batch actual\n",
    "    vecs = embed_texts(docs)\n",
    "    # upsert = idempotente: si ya existe el id, lo actualiza\n",
    "    collection.upsert(ids=ids, documents=docs, metadatas=metas, embeddings=vecs)\n",
    "    ids.clear(); docs.clear(); metas.clear()\n",
    "\n",
    "for r in tqdm(rows, desc=\"Ingestando chunks en Chroma\"):\n",
    "    chunk_id   = r[\"chunk_id\"]              # ej: <base>-w-0001\n",
    "    chunk_path = r[\"chunk_path\"]            # ej: data/chunks_sliding/<base>/chunk_0001.txt\n",
    "    text       = load_chunk_text(chunk_path)\n",
    "    if not text:\n",
    "        continue\n",
    "\n",
    "    ids.append(chunk_id)\n",
    "    docs.append(text)\n",
    "    metas.append({\n",
    "        \"filename_base\": r.get(\"filename_base\", \"\"),\n",
    "        \"method\":       r.get(\"method\", \"sliding\"),\n",
    "        \"chunk_path\":   r.get(\"chunk_path\", \"\"),\n",
    "        \"char_len\":     int(r.get(\"char_len\", 0)),\n",
    "        \"word_len\":     int(r.get(\"word_len\", 0)),\n",
    "        \"start_word\":   int(r.get(\"start_word\", 0)),\n",
    "        \"end_word\":     int(r.get(\"end_word\", 0)),\n",
    "    })\n",
    "\n",
    "    if len(ids) >= BATCH_SIZE:\n",
    "        flush_batch()\n",
    "\n",
    "# √∫ltimo lote\n",
    "flush_batch()\n",
    "\n",
    "print(\"‚úÖ Embeddings generados e indexados.\")\n",
    "print(\"üìö Collection:\", COLLECTION_NAME, \"| count =\", collection.count())\n",
    "print(\"üíæ Persist dir:\", PERSIST_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647db0fa",
   "metadata": {},
   "source": [
    "## Prueba de Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12b5eda5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#1  dist=0.5520  base=3_Semana_AI_20250821_1_JulioVarelaVenegas_AlgebraLinealYProgramacionVectorial  palabras=240\n",
      "chunks_sliding/3_Semana_AI_20250821_1_JulioVarelaVenegas_AlgebraLinealYProgramacionVectorial/chunk_0004.txt\n",
      "los vectores son la base para representar conceptos en IA. III-C. Magnitud y Distancias La magnitud indica la distancia entre el punto inicial y final del vector, calculada con la norma ‚à•v‚à•. Distancia Manhattan (L1): suma de los valores abso- lutos de las diferencias en cada eje: d(x,y) =nX i=1|xi‚àíyi| Distancia Euclidiana (L2): hipotenusa del triangulo formado por los vectores: d(x,y) =vuutnX i=1(xi‚àíyi)2 Figura 2. Distancia Euclidiana entre dos puntos. III-D. Propiedades de la Norma La norma cum...\n",
      "\n",
      "#2  dist=0.5734  base=3_Semana_AI_20250819_2_MarianaQuesadaSanchez_AlgebraLinealYAprendizajeSupervisado  palabras=240\n",
      "chunks_sliding/3_Semana_AI_20250819_2_MarianaQuesadaSanchez_AlgebraLinealYAprendizajeSupervisado/chunk_0003.txt\n",
      "Representacion grafica del vector (4,3)en el plano cartesiano. B. Norma o magnitud La norma mide longitud de un vector y se denota como ‚à•x‚à•. Geometricamente, puede interpretarse como la distancia desde el punto de origen hasta el punto final definido porx. De esta manera, la norma proporciona una medida cuantitativa de la magnitud del vector, independientemente de su direccion. Para un vector x= (x1, x2, . . . , x n), las normas mas comunes son: ‚Ä¢Norma L1 o Manhattan: La distancia Manhattan entr...\n",
      "\n",
      "#3  dist=0.5866  base=3_Semana_AI_20250819_1_JavierRojasRojas_VectoresYAprendizajeSupervisado  palabras=240\n",
      "chunks_sliding/3_Semana_AI_20250819_1_JavierRojasRojas_VectoresYAprendizajeSupervisado/chunk_0005.txt\n",
      "las frases en vectores y midiendo eseangulo . 1)Ejemplo de calculo deangulo entre dos vectores :Se consideran los vectores u= (1,2)yv= (3,4). Paso 1: Calcular el producto punto u¬∑v= 1¬∑3 + 2¬∑4 = 3 + 8 = 11 Paso 2: Calcular las normas ‚à•u‚à•=p 12+ 22=‚àö 5,‚à•v‚à•=p 32+ 42=‚àö 25 = 5 Paso 3: Aplicar la identidad del coseno cos(Œ∏) =u¬∑v ‚à•u‚à• ¬∑ ‚à•v‚à•=11‚àö 5¬∑5=11 5‚àö 5‚âà0.9839 Paso 4: Calcular elangulo Œ∏= arccos(0 .9839) ‚âà10.3‚ó¶ G.Vector co-direccional Son dos vectores que siguen la misma direccion, pero con magnitudes...\n",
      "\n",
      "#4  dist=0.5922  base=3_Semana_AI_20250819_2_MarianaQuesadaSanchez_AlgebraLinealYAprendizajeSupervisado  palabras=240\n",
      "chunks_sliding/3_Semana_AI_20250819_2_MarianaQuesadaSanchez_AlgebraLinealYAprendizajeSupervisado/chunk_0004.txt\n",
      "3)Desigualdad triangular: ‚à•x+y‚à• ‚â§ ‚à• x‚à•+‚à•y‚à•para todos los vectores xey. C. Vectores unitarios Un vector unitario es aquel cuya norma es igual a uno. Se obtiene normalizando un vector vmediante su magnitud: u=v ‚à•v‚à• De esta manera, uconserva la direccion de v, pero con longitud unitaria. Fig. 3. Vector unitario. [2] D. Producto punto El producto punto entre dos vectores es la suma de las multiplicaciones de sus componentes, lo que produce un valor real. Esta operacion es fundamental en inteligencia...\n",
      "\n",
      "#5  dist=0.5964  base=3_Semana_AI_20250819_1_JavierRojasRojas_VectoresYAprendizajeSupervisado  palabras=240\n",
      "chunks_sliding/3_Semana_AI_20250819_1_JavierRojasRojas_VectoresYAprendizajeSupervisado/chunk_0003.txt\n",
      "Lanorma de un vector representa su magnitud, es decir, que tan grande es o cuanto mide desde el punto de inicio hasta el punto final. Se denota como ‚à•x‚à•. Existen diferentes formas de medir la norma de un vector: 1)Norma L1: Distancia Manhattan :Ladistancia Man- hattan entre dos puntos se calcula sumando las distancias absolutas de cada componente, desplazandoseunicamente a lo largo de los ejes del espacio desde el punto de inicio hasta el punto final. Esta metrica se utiliza especialmente cuando...\n"
     ]
    }
   ],
   "source": [
    "# Tooo este c√≥digo era una prueba para revisar si los embeddings se hab√≠an hecho bien\n",
    "\n",
    "from openai import OpenAI\n",
    "api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "oai_client = OpenAI(api_key=api_key)\n",
    "OPENAI_MODEL = \"text-embedding-3-small\"\n",
    "\n",
    "QUESTION = \"¬øCual es la formula de la distancia euclidiana?\"\n",
    "TOP_K = 5\n",
    "\n",
    "# 1) Embeber la pregunta con OpenAI\n",
    "qvec = oai_client.embeddings.create(\n",
    "    model=OPENAI_MODEL,\n",
    "    input=QUESTION\n",
    ").data[0].embedding\n",
    "\n",
    "# 2) Consultar \n",
    "res = collection.query(\n",
    "    query_embeddings=[qvec],\n",
    "    n_results=TOP_K,\n",
    "    include=[\"metadatas\", \"distances\", \"documents\"]\n",
    ")\n",
    "\n",
    "for rank, (doc, meta, dist) in enumerate(zip(res[\"documents\"][0],\n",
    "                                             res[\"metadatas\"][0],\n",
    "                                             res[\"distances\"][0]), start=1):\n",
    "    print(f\"\\n#{rank}  dist={dist:.4f}  base={meta.get('filename_base')}  palabras={meta.get('word_len')}\")\n",
    "    print(meta.get(\"chunk_path\"))\n",
    "    print(doc[:500].replace(\"\\n\",\" \") + (\"...\" if len(doc)>500 else \"\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49b0e44",
   "metadata": {},
   "source": [
    "# Fase 4. Herramientas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94564b0",
   "metadata": {},
   "source": [
    "# RAG Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d604e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import BaseTool\n",
    "from typing import Any, Type\n",
    "from pydantic import BaseModel, Field, PrivateAttr\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "# ======== MODELO DE INPUT ========\n",
    "class RAGInput(BaseModel):\n",
    "    query: str = Field(..., description=\"Pregunta o frase a buscar en los apuntes\")\n",
    "\n",
    "# ======== TOOL ========\n",
    "class RAGTool(BaseTool):\n",
    "    name: str = \"RAG_Tool\"\n",
    "    description: str = \"Busca informaci√≥n en la base vectorial local de apuntes de Inteligencia Artificial\"\n",
    "    args_schema: Type[BaseModel] = RAGInput\n",
    "\n",
    "    _client: Any = PrivateAttr()\n",
    "    _collection: Any = PrivateAttr()\n",
    "    _oai_client: Any = PrivateAttr()\n",
    "\n",
    "    def __init__(self, collection_name: str, persist_dir: str, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self._client = chromadb.PersistentClient(path=persist_dir, settings=Settings(is_persistent=True))\n",
    "        self._collection = self._client.get_collection(collection_name)\n",
    "        self._oai_client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "    def _run(self, query: str):\n",
    "        \"\"\"Consulta la base vectorial usando embeddings OpenAI\"\"\"\n",
    "        embedding = self._oai_client.embeddings.create(\n",
    "            model=\"text-embedding-3-small\",\n",
    "            input=query\n",
    "        ).data[0].embedding\n",
    "\n",
    "        results = self._collection.query(\n",
    "            query_embeddings=[embedding],\n",
    "            n_results=5,\n",
    "            include=[\"documents\",  \"distances\", \"metadatas\"]\n",
    "        )\n",
    "        docs = results[\"documents\"][0]\n",
    "        metas = results[\"metadatas\"][0]\n",
    "\n",
    "        combined = [\n",
    "            f\"Documento: {m.get('filename_base','')} | Fragmento: {d[:600]}...\"\n",
    "            for d, m in zip(docs, metas)\n",
    "        ]\n",
    "        return \"\\n\".join(combined)\n",
    "\n",
    "    async def _arun(self, query: str):\n",
    "        raise NotImplementedError(\"RAGTool no soporta ejecuci√≥n as√≠ncrona.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba91464",
   "metadata": {},
   "source": [
    "## Mini-Prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7302470b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Consulta: ¬øQu√© es una red neuronal convolucional?\n",
      "------------------------------------------------------\n",
      "üìò Resultados:\n",
      "Documento: 10_Semana_AI_20251007_1_GianmarcoOportaPerez_RedesNeuronalesConvolucionales | Fragmento: avanzan a traves de las capas convolucionales y de agrupamiento, se reduce su tama√±o espacial, pero aumenta la abstraccion de las caracteristicas aprendidas. Fig. 1. redes convolucionales C. Aplicaciones Comunes Las redes convolucionales se aplican ampliamente en diver- sas tareas de vision artificial, entre las que destacan: ‚Ä¢Clasificacion de imagenes. ‚Ä¢Segmentacion de objetos. ‚Ä¢Segmentacion de instancias. ‚Ä¢Procesamiento general de imagenes. Estas arquitecturas han demostrado una gran eficacia en problemas de reconocimiento visual, deteccion de patrones y procesamiento de se√±ales en el domini...\n",
      "Documento: 11_Semana_AI_20251014_2_ LuisFernandoBenavidesVillegas_ConvolucionesPoolingAutoencoders | Fragmento: de un bloque, promoviendo la reutilizacion de carac- teristicas y reduciendo la redundancia. Esta estructura densa mejora la propagacion del gradiente, optimiza la eficiencia del modelo y mantiene un numero reducido de parametros. II. PROBLEMAS CON LASREDESNEURONALES CONVOLUCIONALES A pesar de su alto desempe√±o, las redes convolucionales se comportan como una ‚Äúcaja negra‚Äù, ya que resulta dificil com- prender que tipo de informacion estan utilizando para tomar sus decisiones. Las representaciones internas que generan son altamente abstractas, lo que plantea un reto importante de interpretabilid...\n",
      "Documento: 8_Semana_AI_20250923_2_FabianDiazBarboza_RedesNeuronalesFullyConnected | Fragmento: ReLU, etc.) permiten que la red modelice relaciones no lineales entre entradas y salidas. 2.Capasyprofundidad:amayorprofundidad,mayor capacidad para representar abstracciones jerarquicas. 3.Diferenciabilidad:la diferenciabilidad de las fun- ciones internas es requisito para aplicar retropropaga- cion y optimizar los parametros mediante gradiente descendente. 6 Conclusiones En conclusion de la clase, las redes neuronales son como una evolucion natural de la regresion logistica: partien- do de la clasificacion binaria, pasando por la extension multinomial y compactando parametros mediante alge- ...\n",
      "Documento: 11_Semana_AI_20251014_1_JuanDiegoJimenezValverde_CNNYAutoencoders | Fragmento: final del modelo. G. Arquitecturas Convolucionales Una red convolucional combina secuencias deconvolucion ‚Üíactivacion (ReLU)‚Üípooling. Este patron se repite varias veces para extraer informacion progresivamente mas abstracta de la imagen. Generalmente, se prefieren filtros peque√±os (como3√ó3) para capturar detalles locales de forma mas eficiente. Elconvolutional stackse forma al aplicar multiples capas de convolucion consecutivas. Por ejemplo, en una imagen de 5√ó5, un filtro3√ó3puede desplazarse para generar una salida de3√ó3. Regla practica:las dimensiones de las imagenes deben ser divisibles ent...\n",
      "Documento: 8_Semana_AI_20250923_1_BrandonEmmanuelSanchezAraya_RedesNeuronalesMNIST | Fragmento: neurona/clase tiene su propio sesgo:b= (b0, . . . , b 9)‚ä§.Cantidad de neuronas=tama√±o deb. IV. EJERCICIO:DE VECTOR A MATRIZ 1) Una sola regresion binaria (vectorx) Sea x=Ô£Æ Ô£ØÔ£ØÔ£∞3 4 5 6Ô£π Ô£∫Ô£∫Ô£ª, w=Ô£Æ Ô£ØÔ£ØÔ£∞3 2 4 5Ô£π Ô£∫Ô£∫Ô£ª, b= 2. Entonces z=w‚ä§x+b= [ 3 2 4 5 ]Ô£Æ Ô£ØÔ£ØÔ£∞3 4 5 6Ô£π Ô£∫Ô£∫Ô£ª+2 = 67+2 = 69,y=œÉ(z). 2) Varias regresiones a la vez Ahora dos regresiones (piensa ‚Äúdos neuronas de salida‚Äù). Apilamos sus pesos en una matrizWy sus sesgos en un vectorb: W=\u00143 2 4 5 4 3 2 1\u0015 ‚ààR2√ó4, b=\u00142 3\u0015 ‚ààR2. Con el mismoxde arriba: z=Wx+b=\u00143 2 4 5 4 3 2 1\u0015Ô£Æ Ô£ØÔ£ØÔ£∞3 4 5 6Ô£π Ô£∫Ô£∫Ô£ª+\u00142 3\u0015 =\u001469 43\u0015 . V. REDNEURONAL Una red neuronal es un model...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "collection_name = \"ai_apuntes_sliding_openai_v1\"\n",
    "persist_dir = \"data/vectorstores/chroma_sliding_openai_v1\"\n",
    "\n",
    "# ======== INICIALIZAR TOOL ========\n",
    "rag_tool = RAGTool(collection_name=collection_name, persist_dir=persist_dir)\n",
    "\n",
    "# ======== PRUEBAS ========\n",
    "query = \"¬øQu√© es una red neuronal convolucional?\"\n",
    "print(\"üîç Consulta:\", query)\n",
    "print(\"------------------------------------------------------\")\n",
    "\n",
    "response = rag_tool._run(query)\n",
    "\n",
    "print(\"üìò Resultados:\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6221620",
   "metadata": {},
   "source": [
    "# Web Search Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ab69b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import Tool\n",
    "from langchain_tavily import TavilySearch\n",
    "\n",
    "def get_websearch_tool():\n",
    "    search = TavilySearch(max_results=5)\n",
    "    return Tool(\n",
    "        name=\"WebSearch_Tool\",\n",
    "        description=\"Busca informaci√≥n en Internet usando Tavily\",\n",
    "        func=search.run\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de27a66a",
   "metadata": {},
   "source": [
    "## Mini-Prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfdf15c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Consulta: √öltimos avances en inteligencia artificial en 2025\n",
      "------------------------------------------------------\n",
      "üåê Resultados:\n",
      "√öltimos avances en inteligencia artificial en 2025\n",
      "{'query': '√öltimos avances en inteligencia artificial en 2025', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://www.bbc.com/mundo/articles/c4gxzx0kpp6o', 'title': 'IA: Qu√© se espera en 2025 de la inteligencia artificial, el ...', 'content': '**La inteligencia artificial (IA) est√° marcando un antes y un despu√©s en la historia de la tecnolog√≠a, y 2025 traer√° m√°s sorpresas.** Un \"centauro doctor + un sistema de IA\" mejora las decisiones que toman los humanos por su cuenta y los sistemas de IA por la suya. Los agentes de IA aut√≥nomos basados en modelos de lenguaje son el objetivo para 2025 de las grandes empresas tecnol√≥gicas como OpenAI (ChatGPT), Meta (LLaMA), Google (Gemini) o Anthropic (Claude). Nos enfrentamos a un dilema crucial: saber cu√°ndo es mejor ser \"autom√°ticos\" en el uso de agentes de IA aut√≥nomos y cu√°ndo necesitamos tomar la decisi√≥n, es decir, recurrir \"al control humano\" o la \"interacci√≥n humano e IA\".', 'score': 0.98598, 'raw_content': None}, {'url': 'https://www.next-step.es/tendencias-y-retos-de-la-ia-en-2025/', 'title': 'Tendencias y Retos de la IA en 2025', 'content': 'Las tendencias emergentes apuntan a una nueva realidad: Asistentes de IA que ser√°n capaces de anticiparse a las necesidades y ofrecer la soluci√≥n a un problema, antes incluso de formular la pregunta; que realizar√°n operaciones empresariales complejas, de forma totalmente automatizada; sistemas inteligentes que toman decisiones de forma aut√≥noma y en tiempo real y hasta algoritmos que gestionar√°n los recursos de forma totalmente eficiente, para impulsar la sostenibilidad. Por ello, el principal reto de las empresas en 2025 va a ser implementar la automatizaci√≥n con los agentes de IA, para ser m√°s competitivas, pero aplicando criterios √©ticos para no desplazar a las personas, sino empoderarlas. **La gobernanza de la inteligencia artificial (IA)** es el pilar fundamental para asegurar que esta tecnolog√≠a transformadora se desarrolle, implemente y utilice de manera responsable, √©tica y efectiva.', 'score': 0.9845, 'raw_content': None}, {'url': 'https://mexico.unir.net/noticias/ingenieria/avances-inteligencia-artificial/', 'title': '√öltimos avances de la inteligencia artificial, lo m√°s ...', 'content': 'Descubre los avances m√°s innovadores en inteligencia artificial y c√≥mo est√°n transformando el mundo en la actualidad y lo que viene. Ahora mismo, los **avances de la inteligencia artificial** en el mundo acad√©mico hacen una diferencia para estudiantes, profesores y poblaci√≥n en general por igual. Los generadores con IA que mencionamos al inicio son **avances actuales de la inteligencia artificial** que sirven para elaborar material audiovisual y composiciones musicales. Adem√°s, en el contexto de mejoras a futuro, se incluyen los **avances de la inteligencia artificial en M√©xico**. Teniendo en cuenta las innovaciones de la IA, parece que apenas estamos viendo la punta del iceberg y que la inteligencia artificial invadir√° todo lo que conocemos.', 'score': 0.97616, 'raw_content': None}, {'url': 'https://www.infobae.com/tecno/2025/08/28/inteligencia-artificial-en-2025-cinco-cosas-que-todos-deberiamos-conocer-ya-mismo-para-salvarnos/', 'title': 'Inteligencia artificial en 2025: cinco cosas que todos ...', 'content': '1. La IA generativa es cada vez m√°s realista ¬∑ 2. Las ‚Äúalucinaciones‚Äù no desaparecer√°n ¬∑ 3. El costo energ√©tico es un desaf√≠o creciente ¬∑ 4. Nadie', 'score': 0.97425, 'raw_content': None}, {'url': 'https://www.hostinger.com/es/tutoriales/estadisticas-y-tendencias-de-ia', 'title': 'IA: 28 estad√≠sticas y tendencias en 2025', 'content': 'Las 10 estad√≠sticas de la inteligencia artificial que debes conocer en 2025 ¬∑ Se espera que el mercado mundial de la IA crezca un 37% anual. ¬∑ De', 'score': 0.97048, 'raw_content': None}], 'response_time': 0.0, 'request_id': 'c0d68a03-cf9b-4afd-9dbb-cec8732184de'}\n",
      "\n",
      "--- Resultado 1 ---\n",
      "T√≠tulo: IA: Qu√© se espera en 2025 de la inteligencia artificial, el ...\n",
      "URL: https://www.bbc.com/mundo/articles/c4gxzx0kpp6o\n",
      "Contenido: **La inteligencia artificial (IA) est√° marcando un antes y un despu√©s en la historia de la tecnolog√≠a, y 2025 traer√° m√°s sorpresas.** Un \"centauro doctor + un sistema de IA\" mejora las decisiones que toman los humanos por su cuenta y los sistemas de IA por la suya. Los agentes de IA aut√≥nomos basados en modelos de lenguaje son el objetivo para 2025 de las grandes empresas tecnol√≥gicas como OpenAI (ChatGPT), Meta (LLaMA), Google (Gemini) o Anthropic (Claude). Nos enfrentamos a un dilema crucial: saber cu√°ndo es mejor ser \"autom√°ticos\" en el uso de agentes de IA aut√≥nomos y cu√°ndo necesitamos tomar la decisi√≥n, es decir, recurrir \"al control humano\" o la \"interacci√≥n humano e IA\".\n",
      "\n",
      "--- Resultado 2 ---\n",
      "T√≠tulo: Tendencias y Retos de la IA en 2025\n",
      "URL: https://www.next-step.es/tendencias-y-retos-de-la-ia-en-2025/\n",
      "Contenido: Las tendencias emergentes apuntan a una nueva realidad: Asistentes de IA que ser√°n capaces de anticiparse a las necesidades y ofrecer la soluci√≥n a un problema, antes incluso de formular la pregunta; que realizar√°n operaciones empresariales complejas, de forma totalmente automatizada; sistemas inteligentes que toman decisiones de forma aut√≥noma y en tiempo real y hasta algoritmos que gestionar√°n los recursos de forma totalmente eficiente, para impulsar la sostenibilidad. Por ello, el principal reto de las empresas en 2025 va a ser implementar la automatizaci√≥n con los agentes de IA, para ser m√°s competitivas, pero aplicando criterios √©ticos para no desplazar a las personas, sino empoderarlas. **La gobernanza de la inteligencia artificial (IA)** es el pilar fundamental para asegurar que esta tecnolog√≠a transformadora se desarrolle, implemente y utilice de manera responsable, √©tica y efectiva.\n",
      "\n",
      "--- Resultado 3 ---\n",
      "T√≠tulo: √öltimos avances de la inteligencia artificial, lo m√°s ...\n",
      "URL: https://mexico.unir.net/noticias/ingenieria/avances-inteligencia-artificial/\n",
      "Contenido: Descubre los avances m√°s innovadores en inteligencia artificial y c√≥mo est√°n transformando el mundo en la actualidad y lo que viene. Ahora mismo, los **avances de la inteligencia artificial** en el mundo acad√©mico hacen una diferencia para estudiantes, profesores y poblaci√≥n en general por igual. Los generadores con IA que mencionamos al inicio son **avances actuales de la inteligencia artificial** que sirven para elaborar material audiovisual y composiciones musicales. Adem√°s, en el contexto de mejoras a futuro, se incluyen los **avances de la inteligencia artificial en M√©xico**. Teniendo en cuenta las innovaciones de la IA, parece que apenas estamos viendo la punta del iceberg y que la inteligencia artificial invadir√° todo lo que conocemos.\n",
      "\n",
      "--- Resultado 4 ---\n",
      "T√≠tulo: Inteligencia artificial en 2025: cinco cosas que todos ...\n",
      "URL: https://www.infobae.com/tecno/2025/08/28/inteligencia-artificial-en-2025-cinco-cosas-que-todos-deberiamos-conocer-ya-mismo-para-salvarnos/\n",
      "Contenido: 1. La IA generativa es cada vez m√°s realista ¬∑ 2. Las ‚Äúalucinaciones‚Äù no desaparecer√°n ¬∑ 3. El costo energ√©tico es un desaf√≠o creciente ¬∑ 4. Nadie\n",
      "\n",
      "--- Resultado 5 ---\n",
      "T√≠tulo: IA: 28 estad√≠sticas y tendencias en 2025\n",
      "URL: https://www.hostinger.com/es/tutoriales/estadisticas-y-tendencias-de-ia\n",
      "Contenido: Las 10 estad√≠sticas de la inteligencia artificial que debes conocer en 2025 ¬∑ Se espera que el mercado mundial de la IA crezca un 37% anual. ¬∑ De\n"
     ]
    }
   ],
   "source": [
    "# ======== Inicializar Tool ========\n",
    "web_tool = get_websearch_tool()\n",
    "\n",
    "# ======== Consulta ========\n",
    "query = \"√öltimos avances en inteligencia artificial en 2025\"\n",
    "print(\"üîç Consulta:\", query)\n",
    "print(\"------------------------------------------------------\")\n",
    "\n",
    "# Ejecutar b√∫squeda\n",
    "response = web_tool.run(query)\n",
    "\n",
    "print(\"üåê Resultados:\")\n",
    "print(response['query'])\n",
    "print(response)\n",
    "for i in range(len(response['results'])):\n",
    "    print(f\"\\n--- Resultado {i+1} ---\")\n",
    "    print(\"T√≠tulo:\", response['results'][i]['title'])\n",
    "    print(\"URL:\", response['results'][i]['url'])\n",
    "    print(\"Contenido:\", response['results'][i]['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63b96bb",
   "metadata": {},
   "source": [
    "# Fase 5.  Perfil, orquestacion y memoria del agente LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39915eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perfil que va a tener el agente conversacional para dar respuestas especializadas.\n",
    "AGENTE_PERFIL = \"\"\"\n",
    "Eres TEC-IA, un asistente especializado en Inteligencia Artificial,\n",
    "entrenado con apuntes del curso del TEC (II Semestre 2025).\n",
    "Tu prop√≥sito es responder de forma clara, concisa y t√©cnica.\n",
    "\n",
    "Reglas:\n",
    "- Usa primero la base de apuntes (RAG Tool) para responder.\n",
    "- Solo usa la WebSearch Tool si el usuario lo solicita expl√≠citamente\n",
    "  (por ejemplo: 'buscar en web:', 'web:', 'internet:', 'tavily:').\n",
    "- Siempre cita de qu√© documento o autor proviene la informaci√≥n de los apuntes.\n",
    "- No inventes citas ni digas 'no encontr√©'; si no hay suficiente informaci√≥n,\n",
    "  pide una reformulaci√≥n.\n",
    "- Mant√©n un tono explicativo y educativo, no rob√≥tico.\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c4f78d",
   "metadata": {},
   "source": [
    "#### Orquestador\n",
    "\n",
    "El orquestador se encarga de coordinar las herramientas y el modelo principal para responder a las preguntas del usuario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd388a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "Collection [ai_apuntes_sliding_openai_v1] does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotFoundError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     14\u001b[39m collection_name = \u001b[33m\"\u001b[39m\u001b[33mai_apuntes_sliding_openai_v1\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     15\u001b[39m persist_dir = \u001b[33m\"\u001b[39m\u001b[33mNotesAgent-main/data/vectorstores/chroma_sliding_openai_v1\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m rag_tool = \u001b[43mRAGTool\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpersist_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpersist_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m web_tool = get_websearch_tool()\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Memoria de contexto limitada\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TEC/2025 II/IA/NotesAgent/fase4_ragtool.py:26\u001b[39m, in \u001b[36mRAGTool.__init__\u001b[39m\u001b[34m(self, collection_name, persist_dir, **kwargs)\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(**kwargs)\n\u001b[32m     25\u001b[39m \u001b[38;5;28mself\u001b[39m._client = chromadb.PersistentClient(path=persist_dir, settings=Settings(is_persistent=\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[38;5;28mself\u001b[39m._collection = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_collection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[38;5;28mself\u001b[39m._oai_client = OpenAI(api_key=os.getenv(\u001b[33m\"\u001b[39m\u001b[33mOPENAI_API_KEY\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml/lib/python3.13/site-packages/chromadb/api/client.py:208\u001b[39m, in \u001b[36mClient.get_collection\u001b[39m\u001b[34m(self, name, embedding_function, data_loader)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_collection\u001b[39m(\n\u001b[32m    201\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    206\u001b[39m     data_loader: Optional[DataLoader[Loadable]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    207\u001b[39m ) -> Collection:\n\u001b[32m--> \u001b[39m\u001b[32m208\u001b[39m     model = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_server\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_collection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtenant\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtenant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    213\u001b[39m     persisted_ef_config = model.configuration_json.get(\u001b[33m\"\u001b[39m\u001b[33membedding_function\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    215\u001b[39m     validate_embedding_function_conflict_on_get(\n\u001b[32m    216\u001b[39m         embedding_function, persisted_ef_config\n\u001b[32m    217\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml/lib/python3.13/site-packages/chromadb/api/rust.py:270\u001b[39m, in \u001b[36mRustBindingsAPI.get_collection\u001b[39m\u001b[34m(self, name, tenant, database)\u001b[39m\n\u001b[32m    263\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_collection\u001b[39m(\n\u001b[32m    265\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    268\u001b[39m     database: \u001b[38;5;28mstr\u001b[39m = DEFAULT_DATABASE,\n\u001b[32m    269\u001b[39m ) -> CollectionModel:\n\u001b[32m--> \u001b[39m\u001b[32m270\u001b[39m     collection = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbindings\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_collection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtenant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    271\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m CollectionModel(\n\u001b[32m    272\u001b[39m         \u001b[38;5;28mid\u001b[39m=collection.id,\n\u001b[32m    273\u001b[39m         name=collection.name,\n\u001b[32m   (...)\u001b[39m\u001b[32m    279\u001b[39m         database=collection.database,\n\u001b[32m    280\u001b[39m     )\n",
      "\u001b[31mNotFoundError\u001b[39m: Collection [ai_apuntes_sliding_openai_v1] does not exist"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from collections import deque\n",
    "from langchain_openai import ChatOpenAI\n",
    "from agente import AGENTE_PERFIL\n",
    "from fase4_ragtool import RAGTool   \n",
    "from fase4_webtool import get_websearch_tool\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Inicializar modelo principal (orquestador) \n",
    "llm_orq = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0.3)\n",
    "\n",
    "# Inicializar las herramientas con la carpeta donde est√°n los vectores\n",
    "collection_name = \"ai_apuntes_sliding_openai_v1\"\n",
    "persist_dir = \"data/vectorstores/chroma_sliding_openai_v1\"\n",
    "rag_tool = RAGTool(collection_name=collection_name, persist_dir=persist_dir)\n",
    "web_tool = get_websearch_tool()\n",
    "\n",
    "# Memoria de contexto limitada\n",
    "MEMORIA = deque(maxlen=6)\n",
    "\n",
    "def formatear_contexto():\n",
    "    contexto = \"\"\n",
    "    for rol, msg in MEMORIA:\n",
    "        contexto += f\"[{rol.upper()}]: {msg}\\n\"\n",
    "    return contexto.strip()\n",
    "\n",
    "# Detecci√≥n de uso de web\n",
    "def usuario_pide_web(pregunta: str) -> bool:\n",
    "    gatillos = [\"buscar en web:\", \"web:\", \"internet:\", \"tavily:\",\n",
    "                 \"buscar en internet:\", \"consulta en web:\", \"consulta en internet:\", \n",
    "                 \"investiga en web:\", \"investiga en internet:\"]\n",
    "    return any(g in pregunta.lower() for g in gatillos)\n",
    "\n",
    "# Funci√≥n principal del agente \n",
    "def responder_agente(pregunta: str):\n",
    "    usar_web = usuario_pide_web(pregunta)\n",
    "\n",
    "    # 1. Recuperar contexto de memoria\n",
    "    contexto = formatear_contexto()\n",
    "\n",
    "    # 2. Consultar herramienta adecuada\n",
    "    if usar_web:\n",
    "        tool_output = web_tool.run(pregunta)\n",
    "        tool_text = \"\\n\".join([\n",
    "            f\"- {r.get('title','(sin t√≠tulo)')} ‚Äî {r.get('url','')}\\n{r.get('content','')[:350]}...\"\n",
    "            for r in tool_output.get(\"results\", [])\n",
    "        ])\n",
    "    else:\n",
    "        tool_text = rag_tool._run(pregunta)\n",
    "\n",
    "    # 3. Construir prompt final\n",
    "    system_prompt = AGENTE_PERFIL\n",
    "    user_prompt = f\"\"\"\n",
    "Contexto previo:\n",
    "{contexto}\n",
    "\n",
    "Usuario pregunta:\n",
    "{pregunta}\n",
    "\n",
    "Informaci√≥n recuperada:\n",
    "{tool_text}\n",
    "\n",
    "Responde en espa√±ol de manera clara y t√©cnica.\n",
    "Si la informaci√≥n viene de apuntes, cita el documento o autor.\n",
    "Si la informaci√≥n viene de la web, cita la fuente (URL o medio).\n",
    "\"\"\"\n",
    "\n",
    "    completion = llm_orq.invoke([\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ])\n",
    "    respuesta = completion.content\n",
    "\n",
    "    # 4. Actualizar memoria\n",
    "    MEMORIA.append((\"user\", pregunta))\n",
    "    MEMORIA.append((\"assistant\", respuesta))\n",
    "\n",
    "    return respuesta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83fc97e",
   "metadata": {},
   "source": [
    "# Fase 6. Aplicaci√≥n\n",
    "\n",
    "Streamlit no puede ejecutarse en jupyter notebook, por lo que este c√≥digo debe ir en un archivo .py aparte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decfb3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import time\n",
    "import random\n",
    "from orquestador import responder_agente\n",
    "\n",
    "# CONFIGURACI√ìN DE P√ÅGINA \n",
    "st.set_page_config(page_title=\"TEC-IA Asistente\", page_icon=\"üß†\", layout=\"centered\")\n",
    "\n",
    "#  ESTILO LIMPIO Y ESTABLE \n",
    "st.markdown(\"\"\"\n",
    "<style>\n",
    ".stApp {\n",
    "    background: #f4f6fa;\n",
    "    font-family: 'Segoe UI', sans-serif;\n",
    "}\n",
    "\n",
    "/* T√≠tulo */\n",
    "h1 {\n",
    "    text-align: center;\n",
    "    font-size: 2.6rem;\n",
    "    color: #002b5c;\n",
    "    margin-bottom: 0.3rem;\n",
    "    font-weight: 700;\n",
    "}\n",
    ".subtitle {\n",
    "    text-align: center;\n",
    "    color: #3b6ea8;\n",
    "    font-size: 1.1rem;\n",
    "    margin-bottom: 1.2rem;\n",
    "}\n",
    "\n",
    "/* Chat container */\n",
    ".chat-box {\n",
    "    background: white;\n",
    "    padding: 18px;\n",
    "    border-radius: 14px;\n",
    "    max-height: 65vh;\n",
    "    overflow-y: auto;\n",
    "    border: 1px solid #d9e2ec;\n",
    "    box-shadow: 0 4px 12px rgba(0,0,0,0.05);\n",
    "    margin-bottom: 15px;\n",
    "}\n",
    "\n",
    "/* Mensajes */\n",
    ".user-msg {\n",
    "    background: #002b5c;\n",
    "    color: white;\n",
    "    padding: 10px 14px;\n",
    "    border-radius: 16px 16px 4px 16px;\n",
    "    margin-left: auto;\n",
    "    max-width: 75%;\n",
    "    margin-bottom: 12px;\n",
    "}\n",
    ".bot-msg {\n",
    "    background: #e8f0fa;\n",
    "    color: #002b5c;\n",
    "    padding: 10px 14px;\n",
    "    border-radius: 16px 16px 16px 4px;\n",
    "    margin-right: auto;\n",
    "    max-width: 75%;\n",
    "    margin-bottom: 12px;\n",
    "}\n",
    "\n",
    "/* Input */\n",
    ".stChatInput > div > div {\n",
    "    background: white !important;\n",
    "    border-radius: 18px !important;\n",
    "    border: 1px solid #d9e2ec;\n",
    "}\n",
    "</style>\n",
    "\"\"\", unsafe_allow_html=True)\n",
    "\n",
    "#  ESTADO DEL CHAT \n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = [\n",
    "        {\"role\": \"assistant\", \"content\": \"¬°Hola! Soy **TEC-IA** ü§ñ. Estoy aqu√≠ para ayudarte con teor√≠a, pr√°ctica y conceptos de Inteligencia Artificial del TEC. Preg√∫ntame algo üëá.\"}\n",
    "    ]\n",
    "\n",
    "# ENCABEZADO \n",
    "st.markdown(\"\"\"\n",
    "<div style=\"text-align:center;\">\n",
    "    <img src=\"https://cdn-icons-png.flaticon.com/512/14313/14313824.png\" width=\"85\">\n",
    "    <h1 style=\"margin-top: 10px; margin-bottom: 4px; font-weight: 700; color:#002b5c;\">\n",
    "        TEC-IA\n",
    "    </h1>\n",
    "    <div style=\"color:#3b6ea8; font-size: 1.1rem; margin-bottom: 14px;\">\n",
    "        Asistente para el curso de Inteligencia Artificial ‚Äî TEC\n",
    "    </div>\n",
    "</div>\n",
    "<hr style=\"border: 0; height: 1px; background: #d9e2ec; margin-top: 4px; margin-bottom: 18px;\">\n",
    "\"\"\", unsafe_allow_html=True)\n",
    "for msg in st.session_state.messages:\n",
    "    if msg[\"role\"] == \"user\":\n",
    "        st.markdown(f\"<div class='user-msg'>{msg['content']}</div>\", unsafe_allow_html=True)\n",
    "    else:\n",
    "        st.markdown(f\"<div class='bot-msg'>{msg['content']}</div>\", unsafe_allow_html=True)\n",
    "\n",
    "st.markdown(\"</div>\", unsafe_allow_html=True)\n",
    "\n",
    "# RESPUESTA SIMULADA \n",
    "def reply(prompt):\n",
    "    time.sleep(1.0)\n",
    "    respuestas = [\n",
    "        f\"Interesante pregunta sobre **{prompt}**. ¬øQuieres que lo explique paso a paso o con un ejemplo pr√°ctico?\",\n",
    "        f\"**{prompt}** se estudia en IA porque ayuda a entender c√≥mo los sistemas pueden aprender patrones.\",\n",
    "        f\"Te explico **{prompt}** de manera sencilla: ...\"\n",
    "    ]\n",
    "    return random.choice(respuestas)\n",
    "\n",
    "# INPUT DEL CHAT\n",
    "prompt = st.chat_input(\"Escribe tu pregunta...\")\n",
    "\n",
    "if prompt:\n",
    "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    answer = responder_agente(prompt)\n",
    "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": answer})\n",
    "    st.rerun()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
