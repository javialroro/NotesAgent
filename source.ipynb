{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10918002",
   "metadata": {},
   "source": [
    "# Fase 1. Recolecci√≥n y organizaci√≥n de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be5f26d",
   "metadata": {},
   "source": [
    "En docs \"lineamientos_nomenclatura_y_citacion.md\" est√° la informaci√≥n por si quieren guiarse y revisar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c92d8c2",
   "metadata": {},
   "source": [
    "# Fase 2. Procesamiento de texto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9be9e59",
   "metadata": {},
   "source": [
    "## Extracci√≥n del texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073fa793",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'PyPDF2'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mPyPDF2\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PdfReader\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01mre\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Directorio base del proyecto\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'PyPDF2'"
     ]
    }
   ],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "import os, re\n",
    "\n",
    "# Directorio base del proyecto\n",
    "BASE_DIR = \"data\"\n",
    "RAW_PDF_DIR = os.path.join(BASE_DIR, \"apuntes_raw\")\n",
    "OUT_RAW_DIR = os.path.join(BASE_DIR, \"apuntes_clean\", \"raw\")\n",
    "\n",
    "# Crear la carpeta de salida si no existe\n",
    "os.makedirs(OUT_RAW_DIR, exist_ok=True)\n",
    "\n",
    "# Buscar todos los PDFs en la carpeta apuntes_raw\n",
    "pdf_files = [f for f in os.listdir(RAW_PDF_DIR) if f.lower().endswith(\".pdf\")]\n",
    "\n",
    "for pdf_file in pdf_files:\n",
    "    pdf_path = os.path.join(RAW_PDF_DIR, pdf_file)\n",
    "    txt_name = os.path.splitext(pdf_file)[0] + \".txt\"\n",
    "    out_txt_path = os.path.join(OUT_RAW_DIR, txt_name)\n",
    "\n",
    "    reader = PdfReader(pdf_path)\n",
    "    text_pages = []\n",
    "    for page in reader.pages:\n",
    "        text_pages.append(page.extract_text() or \"\")\n",
    "    \n",
    "    # Unir todas las p√°ginas con doble salto de l√≠nea\n",
    "    full_text = \"\\n\\n\".join(text_pages)\n",
    "    with open(out_txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(full_text)\n",
    "\n",
    "print(f\"‚úÖ {len(pdf_files)} archivos extra√≠dos correctamente a {OUT_RAW_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ac2e1a",
   "metadata": {},
   "source": [
    "## Normalizaci√≥n de txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9619af32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 46 archivos corregidos y guardados en data\\apuntes_clean\\normalized\n"
     ]
    }
   ],
   "source": [
    "import os, re, unicodedata\n",
    "\n",
    "BASE_DIR = \"data\"\n",
    "CLEAN_IN_DIR  = os.path.join(BASE_DIR, \"apuntes_clean\", \"raw\")        # carpeta de entrada\n",
    "CLEAN_OUT_DIR = os.path.join(BASE_DIR, \"apuntes_clean\", \"normalized\") # carpeta de salida\n",
    "os.makedirs(CLEAN_OUT_DIR, exist_ok=True)\n",
    "\n",
    "def quitar_tildes_y_reparar_espacios(texto: str) -> str:\n",
    "    # 1) Normalizaci√≥n Unicode para exponer diacr√≠ticos combinantes\n",
    "    t = unicodedata.normalize(\"NFD\", texto)\n",
    "\n",
    "    # 2) Reemplazos t√≠picos de PDF \n",
    "    t = (t.replace(\"\\u00A0\", \" \")      # NBSP -> espacio normal\n",
    "           .replace(\"\\u00AD\", \"\")      # soft hyphen -> nada\n",
    "           .replace(\"Ô¨Å\", \"fi\").replace(\"Ô¨Ç\", \"fl\")  # ligaduras\n",
    "           .replace(\"\\u0131\", \"i\")     # ƒ± (i sin punto) -> i\n",
    "           .replace(\"Àô\", \"\").replace(\"`\", \"\").replace(\"¬®\", \"\").replace(\"ÀÜ\", \"\"))\n",
    "\n",
    "    # 3) Unir SOLO cuando hay acento suelto entre letras: \"implementaci ¬¥on\" -> \"implementacion\"\n",
    "    t = re.sub(r\"([A-Za-z√±√ë])\\s*[\\u00B4\\u0301]\\s*([A-Za-z√±√ë])\", r\"\\1\\2\", t)\n",
    "\n",
    "    # 4) Convertir virgulilla suelta (~ o \\u02DC) en √ë/√± cuando corresponde (p.ej. \"tama Àúno\" -> \"tama√±o\")\n",
    "    #    a) letra + ~ + n/N\n",
    "    t = re.sub(r\"([A-Za-z√±√ë])\\s*[\\u02DC~]\\s*([Nn])\",\n",
    "               lambda m: m.group(1) + (\"√ë\" if m.group(2).isupper() else \"√±\"),\n",
    "               t)\n",
    "    #    b) ~ al inicio o tras espacio antes de n/N + vocal (p.ej. \" Àúno \" -> \" √±o \")\n",
    "    t = re.sub(r\"(?<!\\S)[\\u02DC~]\\s*([Nn])(?=[aeiou√°√©√≠√≥√∫AEIOU√Å√â√ç√ì√ö])\",\n",
    "               lambda m: (\"√ë\" if m.group(1).isupper() else \"√±\"),\n",
    "               t)\n",
    "\n",
    "    # 5) Eliminar diacr√≠ticos (tildes) PERO conservar √±/√ë\n",
    "    t = ''.join(c for c in t if unicodedata.category(c) != 'Mn' or c.lower() == '√±')\n",
    "\n",
    "    # 6) Limpieza suave: colapsar espacios repetidos y limitar saltos\n",
    "    t = re.sub(r\"[ \\t]+\", \" \", t)\n",
    "    t = re.sub(r\"\\n{3,}\", \"\\n\\n\", t)\n",
    "\n",
    "    return t.strip()\n",
    "\n",
    "# Aplicar a todos los .txt de entrada\n",
    "count = 0\n",
    "for fname in os.listdir(CLEAN_IN_DIR):\n",
    "    if not fname.lower().endswith(\".txt\"):\n",
    "        continue\n",
    "\n",
    "    with open(os.path.join(CLEAN_IN_DIR, fname), \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        raw = f.read()\n",
    "\n",
    "    norm = quitar_tildes_y_reparar_espacios(raw)\n",
    "\n",
    "    with open(os.path.join(CLEAN_OUT_DIR, fname), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(norm)\n",
    "    count += 1\n",
    "\n",
    "print(f\"‚úÖ {count} archivos corregidos y guardados en {CLEAN_OUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31180d1",
   "metadata": {},
   "source": [
    "## Segmentaci√≥n (P√°rrafos y Ventanas Deslizantes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f353734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Segmentaci√≥n regenerada con par√°metros ajustados.\n",
      " - √çndice p√°rrafos:  data\\chunks_paragraphs\\index_paragraphs.csv\n",
      " - √çndice ventanas:  data\\chunks_sliding\\index_sliding.csv\n",
      " - Resumen:          data\\chunks_summary.csv\n",
      " - Carpeta chunks A: data\\chunks_paragraphs\n",
      " - Carpeta chunks B: data\\chunks_sliding\n"
     ]
    }
   ],
   "source": [
    "import os, re, csv, shutil\n",
    "from statistics import mean\n",
    "\n",
    "BASE_DIR = \"data\"\n",
    "INPUT_DIR = os.path.join(BASE_DIR, \"apuntes_clean\", \"normalized\")\n",
    "\n",
    "# --- P√°rrafos ---\n",
    "PAR_MIN_CHARS = 480\n",
    "PAR_MAX_CHARS = 2000\n",
    "MERGE_TITLES = True\n",
    "TITLE_MAX_CHARS = 140\n",
    "\n",
    "# --- Ventanas deslizantes ---\n",
    "WIN_WORDS = 240\n",
    "WIN_OVERLAP = 0.20\n",
    "WIN_STRIDE = max(1, int(WIN_WORDS * (1 - WIN_OVERLAP)))\n",
    "\n",
    "# --- Salidas ---\n",
    "OUT_PAR_DIR = os.path.join(BASE_DIR, \"chunks_paragraphs\")\n",
    "OUT_WIN_DIR = os.path.join(BASE_DIR, \"chunks_sliding\")\n",
    "\n",
    "# Limpiar salidas anteriores para √≠ndices consistentes\n",
    "for d in (OUT_PAR_DIR, OUT_WIN_DIR):\n",
    "    if os.path.exists(d):\n",
    "        shutil.rmtree(d)\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "IDX_PAR_CSV = os.path.join(OUT_PAR_DIR, \"index_paragraphs.csv\")\n",
    "IDX_WIN_CSV = os.path.join(OUT_WIN_DIR, \"index_sliding.csv\")\n",
    "SUMMARY_CSV = os.path.join(BASE_DIR, \"chunks_summary.csv\")\n",
    "\n",
    "# ===================== UTILIDADES =====================\n",
    "def read_txt(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        return f.read()\n",
    "\n",
    "def safe_filename_stem(fname):\n",
    "    return os.path.splitext(os.path.basename(fname))[0]\n",
    "\n",
    "def split_paragraphs(text):\n",
    "    raw_pars = [p.strip() for p in re.split(r\"\\n\\s*\\n\", text) if p.strip()]\n",
    "    pars = []\n",
    "    for p in raw_pars:\n",
    "        p = re.sub(r\"[ \\t]+\", \" \", p).strip()\n",
    "        pars.append(p)\n",
    "    return pars\n",
    "\n",
    "def split_sentences(p):\n",
    "    parts = re.split(r\"(?<=[\\.\\?\\!])\\s+\", p)\n",
    "    return [s.strip() for s in parts if s.strip()]\n",
    "\n",
    "def repartition_long_paragraph(p, max_chars):\n",
    "    if len(p) <= max_chars:\n",
    "        return [p]\n",
    "    sent = split_sentences(p)\n",
    "    chunks, buf = [], \"\"\n",
    "    for s in sent:\n",
    "        if not buf:\n",
    "            buf = s\n",
    "        elif len(buf) + 1 + len(s) <= max_chars:\n",
    "            buf = buf + \" \" + s\n",
    "        else:\n",
    "            chunks.append(buf.strip())\n",
    "            buf = s\n",
    "    if buf:\n",
    "        chunks.append(buf.strip())\n",
    "    final = []\n",
    "    for c in chunks:\n",
    "        if len(c) <= max_chars:\n",
    "            final.append(c)\n",
    "        else:\n",
    "            for i in range(0, len(c), max_chars):\n",
    "                final.append(c[i:i+max_chars].strip())\n",
    "    return final\n",
    "\n",
    "def fuse_short_paragraphs(pars, min_chars, merge_titles, title_max):\n",
    "    out = []\n",
    "    i = 0\n",
    "    while i < len(pars):\n",
    "        cur = pars[i]\n",
    "        is_title_like = merge_titles and (len(cur) <= title_max and \"\\n\" not in cur and len(cur.split()) <= 16)\n",
    "        if is_title_like and i + 1 < len(pars):\n",
    "            merged = (cur + \" ‚Äî \" + pars[i+1]).strip()\n",
    "            out.append(merged)\n",
    "            i += 2\n",
    "            continue\n",
    "        if len(cur) < min_chars and i + 1 < len(pars):\n",
    "            merged = (cur + \" \" + pars[i+1]).strip()\n",
    "            out.append(merged)\n",
    "            i += 2\n",
    "        else:\n",
    "            out.append(cur)\n",
    "            i += 1\n",
    "    return out\n",
    "\n",
    "def ensure_dir(d):\n",
    "    if not os.path.exists(d):\n",
    "        os.makedirs(d, exist_ok=True)\n",
    "\n",
    "def write_chunk(path, text):\n",
    "    ensure_dir(os.path.dirname(path))\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text.strip())\n",
    "\n",
    "def word_tokenize(text):\n",
    "    return re.findall(r\"\\S+\", text)\n",
    "\n",
    "# ===================== M√âTODO A: P√ÅRRAFOS =====================\n",
    "par_rows = []\n",
    "summary_rows = []\n",
    "\n",
    "files = [f for f in os.listdir(INPUT_DIR) if f.lower().endswith(\".txt\")]\n",
    "for fname in sorted(files):\n",
    "    path_in = os.path.join(INPUT_DIR, fname)\n",
    "    base = safe_filename_stem(fname)\n",
    "    out_dir_doc = os.path.join(OUT_PAR_DIR, base)\n",
    "    ensure_dir(out_dir_doc)\n",
    "\n",
    "    txt = read_txt(path_in)\n",
    "    pars = split_paragraphs(txt)\n",
    "    pars = fuse_short_paragraphs(pars, PAR_MIN_CHARS, MERGE_TITLES, TITLE_MAX_CHARS)\n",
    "\n",
    "    final_pars = []\n",
    "    for p in pars:\n",
    "        final_pars.extend(repartition_long_paragraph(p, PAR_MAX_CHARS))\n",
    "\n",
    "    lengths = []\n",
    "    for idx, chunk in enumerate(final_pars, start=1):\n",
    "        chunk_name = f\"chunk_{idx:04d}.txt\"\n",
    "        out_path = os.path.join(out_dir_doc, chunk_name)\n",
    "        write_chunk(out_path, chunk)\n",
    "        lengths.append(len(chunk))\n",
    "        par_rows.append({\n",
    "            \"filename_base\": base,\n",
    "            \"method\": \"paragraphs\",\n",
    "            \"chunk_id\": f\"{base}-p-{idx:04d}\",\n",
    "            \"chunk_path\": os.path.relpath(out_path, BASE_DIR).replace(\"\\\\\",\"/\"),\n",
    "            \"char_len\": len(chunk),\n",
    "            \"word_len\": len(chunk.split()),\n",
    "            \"paragraph_idx\": idx\n",
    "        })\n",
    "\n",
    "    summary_rows.append({\n",
    "        \"filename_base\": base,\n",
    "        \"method\": \"paragraphs\",\n",
    "        \"n_chunks\": len(lengths),\n",
    "        \"char_mean\": round(mean(lengths), 1) if lengths else 0,\n",
    "        \"pct_short_<300\": round(100*sum(l<300 for l in lengths)/len(lengths), 1) if lengths else 0\n",
    "    })\n",
    "\n",
    "if par_rows:\n",
    "    with open(IDX_PAR_CSV, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=list(par_rows[0].keys()))\n",
    "        writer.writeheader()\n",
    "        writer.writerows(par_rows)\n",
    "\n",
    "# ===================== M√âTODO B: VENTANAS DESLIZANTES =====================\n",
    "win_rows = []\n",
    "for fname in sorted(files):\n",
    "    path_in = os.path.join(INPUT_DIR, fname)\n",
    "    base = safe_filename_stem(fname)\n",
    "    out_dir_doc = os.path.join(OUT_WIN_DIR, base)\n",
    "    ensure_dir(out_dir_doc)\n",
    "\n",
    "    txt = read_txt(path_in)\n",
    "    words = word_tokenize(txt)\n",
    "    n = len(words)\n",
    "    lengths = []\n",
    "\n",
    "    if n == 0:\n",
    "        summary_rows.append({\n",
    "            \"filename_base\": base,\n",
    "            \"method\": \"sliding\",\n",
    "            \"n_chunks\": 0, \"char_mean\": 0, \"pct_short_<300\": 0\n",
    "            })\n",
    "        continue\n",
    "\n",
    "    idx = 0\n",
    "    win_id = 1\n",
    "    while idx < n:\n",
    "        end = min(n, idx + WIN_WORDS)\n",
    "        w_chunk = words[idx:end]\n",
    "        chunk = \" \".join(w_chunk).strip()\n",
    "        if not chunk:\n",
    "            break\n",
    "\n",
    "        chunk_name = f\"chunk_{win_id:04d}.txt\"\n",
    "        out_path = os.path.join(out_dir_doc, chunk_name)\n",
    "        write_chunk(out_path, chunk)\n",
    "\n",
    "        lengths.append(len(chunk))\n",
    "        win_rows.append({\n",
    "            \"filename_base\": base,\n",
    "            \"method\": \"sliding\",\n",
    "            \"chunk_id\": f\"{base}-w-{win_id:04d}\",\n",
    "            \"chunk_path\": os.path.relpath(out_path, BASE_DIR).replace(\"\\\\\",\"/\"),\n",
    "            \"char_len\": len(chunk),\n",
    "            \"word_len\": len(w_chunk),\n",
    "            \"start_word\": idx,\n",
    "            \"end_word\": end\n",
    "        })\n",
    "\n",
    "        win_id += 1\n",
    "        if end == n:\n",
    "            break\n",
    "        idx += WIN_STRIDE\n",
    "\n",
    "    summary_rows.append({\n",
    "        \"filename_base\": base,\n",
    "        \"method\": \"sliding\",\n",
    "        \"n_chunks\": len(lengths),\n",
    "        \"char_mean\": round(mean(lengths), 1) if lengths else 0,\n",
    "        \"pct_short_<300\": round(100*sum(l<300 for l in lengths)/len(lengths), 1) if lengths else 0\n",
    "    })\n",
    "\n",
    "if win_rows:\n",
    "    with open(IDX_WIN_CSV, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=list(win_rows[0].keys()))\n",
    "        writer.writeheader()\n",
    "        writer.writerows(win_rows)\n",
    "\n",
    "# ===================== RESUMEN =====================\n",
    "if summary_rows:\n",
    "    with open(SUMMARY_CSV, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=list(summary_rows[0].keys()))\n",
    "        writer.writeheader()\n",
    "        writer.writerows(summary_rows)\n",
    "\n",
    "print(\"‚úÖ Segmentaci√≥n regenerada con par√°metros ajustados.\")\n",
    "print(f\" - √çndice p√°rrafos:  {IDX_PAR_CSV}\")\n",
    "print(f\" - √çndice ventanas:  {IDX_WIN_CSV}\")\n",
    "print(f\" - Resumen:          {SUMMARY_CSV}\")\n",
    "print(f\" - Carpeta chunks A: {OUT_PAR_DIR}\")\n",
    "print(f\" - Carpeta chunks B: {OUT_WIN_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b23516",
   "metadata": {},
   "source": [
    "# Fase 3. Tokenizaci√≥n y Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a786f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documentos (unique filename_base): 46\n",
      "Total de chunks en √≠ndice: 386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ingestando chunks en Chroma: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 386/386 [00:11<00:00, 33.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Embeddings generados e indexados.\n",
      "üìö Collection: ai_apuntes_sliding_openai_v1 | count = 386\n",
      "üíæ Persist dir: data\\vectorstores\\chroma_sliding_openai_v1\n"
     ]
    }
   ],
   "source": [
    "import os, csv, time, math\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# (A) ‚Äî‚Äî CONFIGURACI√ìN GENERAL \n",
    "BASE_DIR       = \"data\"\n",
    "INDEX_CSV      = os.path.join(BASE_DIR, \"chunks_sliding\", \"index_sliding.csv\")   # puedes cambiar a p√°rrafos si quieres\n",
    "PERSIST_DIR    = os.path.join(BASE_DIR, \"vectorstores\", \"chroma_sliding_openai_v1\")\n",
    "COLLECTION_NAME= \"ai_apuntes_sliding_openai_v1\"\n",
    "\n",
    "# Proveedor de embeddings: \"openai\" o \"local\"\n",
    "PROVIDER       = \"openai\"       \n",
    "OPENAI_MODEL   = \"text-embedding-3-small\"      \n",
    "LOCAL_MODEL    = \"all-MiniLM-L6-v2\"           \n",
    "\n",
    "BATCH_SIZE     = 128            # tama√±o de lote para ingesti√≥n\n",
    "MAX_RETRIES    = 5              # reintentos por rate-limit/errores transitorios\n",
    "RETRY_BASE_SEC = 2              # backoff exponencial\n",
    "\n",
    "# (B) ‚Äî‚Äî SETUP DE EMBEDDINGS \n",
    "embed_dims = None\n",
    "\n",
    "if PROVIDER == \"openai\":\n",
    "    from openai import OpenAI\n",
    "    OPENAI_API_KEY = \"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\" \n",
    "    \n",
    "    oai_client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "    def embed_texts(texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"Embeddings con OpenAI + reintentos.\"\"\"\n",
    "        for attempt in range(MAX_RETRIES):\n",
    "            try:\n",
    "                resp = oai_client.embeddings.create(\n",
    "                    model=OPENAI_MODEL,\n",
    "                    input=texts\n",
    "                )\n",
    "                vecs = [d.embedding for d in resp.data]\n",
    "                return vecs\n",
    "            except Exception as e:\n",
    "                wait = RETRY_BASE_SEC * (2 ** attempt)\n",
    "                print(f\"[WARN] Error {e}. Reintentando en {wait}s...\")\n",
    "                if attempt == MAX_RETRIES - 1:\n",
    "                    raise\n",
    "                time.sleep(wait)\n",
    "\n",
    "elif PROVIDER == \"local\":\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    st_model = SentenceTransformer(LOCAL_MODEL)\n",
    "\n",
    "    def embed_texts(texts: List[str]) -> List[List[float]]:\n",
    "        return st_model.encode(texts, convert_to_numpy=False, normalize_embeddings=False).tolist()\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"PROVIDER debe ser 'openai' o 'local'.\")\n",
    "\n",
    "# (C) ‚Äî‚Äî INICIALIZAR CHROMA PERSISTENTE \n",
    "os.makedirs(PERSIST_DIR, exist_ok=True)\n",
    "\n",
    "client = chromadb.PersistentClient(\n",
    "    path=PERSIST_DIR,\n",
    "    settings=Settings(is_persistent=True)\n",
    ")\n",
    "\n",
    "# Crear o recuperar la colecci√≥n\n",
    "try:\n",
    "    collection = client.get_collection(COLLECTION_NAME)\n",
    "except:\n",
    "    collection = client.create_collection(\n",
    "        name=COLLECTION_NAME,\n",
    "        metadata={\"hnsw:space\": \"cosine\"}  # distancia coseno\n",
    "    )\n",
    "\n",
    "# (D) ‚Äî‚Äî UTILIDADES \n",
    "def read_index_rows(csv_path: str):\n",
    "    rows = []\n",
    "    with open(csv_path, \"r\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for r in reader:\n",
    "            rows.append(r)\n",
    "    return rows\n",
    "\n",
    "def load_chunk_text(chunk_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Abre el archivo de texto del chunk, corrigiendo rutas relativas.\n",
    "    \"\"\"\n",
    "    # Si el path ya incluye \"data/\", se queda tal cual\n",
    "    if not os.path.isabs(chunk_path):\n",
    "        # Si empieza por \"data/\", lo consideramos relativo al proyecto\n",
    "        if chunk_path.startswith(\"data/\") or chunk_path.startswith(\".\\\\data\\\\\") or chunk_path.startswith(\".\\\\chunks_\"):\n",
    "            path = os.path.normpath(chunk_path)\n",
    "        else:\n",
    "            # Si viene solo 'chunks_sliding/...', le anteponemos 'data/'\n",
    "            path = os.path.join(\"data\", chunk_path)\n",
    "    else:\n",
    "        path = chunk_path\n",
    "\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"No se encontr√≥ el archivo: {path}\")\n",
    "\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        return f.read().strip()\n",
    "\n",
    "\n",
    "# (E) ‚Äî‚Äî CARGAR √çNDICE Y PREPARAR INGESTA \n",
    "rows = read_index_rows(INDEX_CSV)\n",
    "print(f\"Documentos (unique filename_base): {len(set(r['filename_base'] for r in rows))}\")\n",
    "print(f\"Total de chunks en √≠ndice: {len(rows)}\")\n",
    "\n",
    "# (F) ‚Äî‚Äî INGESTA EN LOTES CON EMBEDDINGS \n",
    "ids, docs, metas = [], [], []\n",
    "\n",
    "def flush_batch():\n",
    "    if not ids:\n",
    "        return\n",
    "    # Calcula embeddings del batch actual\n",
    "    vecs = embed_texts(docs)\n",
    "    # upsert = idempotente: si ya existe el id, lo actualiza\n",
    "    collection.upsert(ids=ids, documents=docs, metadatas=metas, embeddings=vecs)\n",
    "    ids.clear(); docs.clear(); metas.clear()\n",
    "\n",
    "for r in tqdm(rows, desc=\"Ingestando chunks en Chroma\"):\n",
    "    chunk_id   = r[\"chunk_id\"]              # ej: <base>-w-0001\n",
    "    chunk_path = r[\"chunk_path\"]            # ej: data/chunks_sliding/<base>/chunk_0001.txt\n",
    "    text       = load_chunk_text(chunk_path)\n",
    "    if not text:\n",
    "        continue\n",
    "\n",
    "    ids.append(chunk_id)\n",
    "    docs.append(text)\n",
    "    metas.append({\n",
    "        \"filename_base\": r.get(\"filename_base\", \"\"),\n",
    "        \"method\":       r.get(\"method\", \"sliding\"),\n",
    "        \"chunk_path\":   r.get(\"chunk_path\", \"\"),\n",
    "        \"char_len\":     int(r.get(\"char_len\", 0)),\n",
    "        \"word_len\":     int(r.get(\"word_len\", 0)),\n",
    "        \"start_word\":   int(r.get(\"start_word\", 0)),\n",
    "        \"end_word\":     int(r.get(\"end_word\", 0)),\n",
    "    })\n",
    "\n",
    "    if len(ids) >= BATCH_SIZE:\n",
    "        flush_batch()\n",
    "\n",
    "# √∫ltimo lote\n",
    "flush_batch()\n",
    "\n",
    "print(\"‚úÖ Embeddings generados e indexados.\")\n",
    "print(\"üìö Collection:\", COLLECTION_NAME, \"| count =\", collection.count())\n",
    "print(\"üíæ Persist dir:\", PERSIST_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647db0fa",
   "metadata": {},
   "source": [
    "## Prueba de Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b5eda5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#1  dist=0.3059  base=11_Semana_AI_20251014_3_AlexStevenNaranjoMasƒ±s_EmbeddingsAutoencoders  palabras=240\n",
      "chunks_sliding/11_Semana_AI_20251014_3_AlexStevenNaranjoMasƒ±s_EmbeddingsAutoencoders/chunk_0005.txt\n",
      "2. Estructura basica de un Autoencoder. V. AUTOENCODERS(CODIFICADORESAUTOMATICOS) A. Estructura General y Objetivo Encoder‚ÜíEspacio Latente‚ÜíDecoder Aprenden a reconstruir la entrada. Aunque la se√±al de entre- namiento es auto-supervisada (salida = entrada), se consideran tipicamente metodos no supervisados por no requerir etiquetas externas. B. Componentes y Variantes Encoder:reduce espacialidad y comprime informacion (conv +downsampling). Latente:vector/tensor compacto; su tama√±o controla capaci...\n",
      "\n",
      "#2  dist=0.3154  base=11_Semana_AI_20251014_1_JuanDiegoJimenezValverde_CNNYAutoencoders  palabras=240\n",
      "chunks_sliding/11_Semana_AI_20251014_1_JuanDiegoJimenezValverde_CNNYAutoencoders/chunk_0007.txt\n",
      "sonutiles, por ejemplo, en aplicaciones medicas para resaltar fracturas o anomalias en radiografias. B. Autoencoders Aunque utilizan arquitecturas similares a las redes convolu- cionales, losautoencoderstrabajan sin etiquetas explicitas, por lo que se consideran metodos no supervisados. Su objetivo es reconstruir la entrada original, aprendiendo una repre- sentacion interna comprimida. Fig. 2. Estructura basica de un Autoencoder. El proceso consta de tres partes, como se muestra en la Fig. 2: 1)...\n",
      "\n",
      "#3  dist=0.3202  base=11_Semana_AI_20251014_2_ LuisFernandoBenavidesVillegas_ConvolucionesPoolingAutoencoders  palabras=240\n",
      "chunks_sliding/11_Semana_AI_20251014_2_ LuisFernandoBenavidesVillegas_ConvolucionesPoolingAutoencoders/chunk_0009.txt\n",
      "la separacion entre clases es clara en el espacio reducido, se considera que el modelo ha aprendido una rep- resentacion adecuada. Por el contrario, si las clases aparecen mezcladas, indica que la red no ha logrado distinguir correc- tamente las caracteristicas de cada una. C. Mapas de Activacion Se pueden generarheatmapso mapas de activacion que destacan las zonas especificas de una imagen que influyen mas en la decision del modelo. Estos mapas sonutiles para verificar si la red esta enfocandos...\n",
      "\n",
      "#4  dist=0.3418  base=11_Semana_AI_20251016_2_AndresSanchezRojas_AutoencodersUNetRAGs  palabras=240\n",
      "chunks_sliding/11_Semana_AI_20251016_2_AndresSanchezRojas_AutoencodersUNetRAGs/chunk_0002.txt\n",
      "presenta la conexion con tareas de vision por computador (p. ej., segmentacion y arquitecturas tipo U -Net) y la extension a representaciones para texto mediante tokenizacion y embeddings, asicomo su papel en sistemas mas amplios como RAGs y agentes basados en LLM. El texto ofrece una guia practica con definiciones, formulas y recomendaciones operativas para implementar experimentos en imagenes y texto. II. AUTOENCODERS II-A. Definicion y proposito Los autoencoders son una arquitectura novedosa ...\n",
      "\n",
      "#5  dist=0.3516  base=11_Semana_AI_20251014_2_ LuisFernandoBenavidesVillegas_ConvolucionesPoolingAutoencoders  palabras=240\n",
      "chunks_sliding/11_Semana_AI_20251014_2_ LuisFernandoBenavidesVillegas_ConvolucionesPoolingAutoencoders/chunk_0010.txt\n",
      "Latente‚ÜíDecoder El aprendizaje del autoencoder consiste en minimizar el error de reconstruccionentre la entrada original y la salida reconstruida. Aunque no haya etiquetas externas, el entre- namiento es parcialmente supervisado, ya que la salida se compara directamente con la entrada. A. Encoder Consiste en una serie de bloques convolucionales seguidos de operaciones depooling, con el objetivo de extraer las caracteristicas mas relevantes de la entrada y comprimir la informacion a traves de un ...\n"
     ]
    }
   ],
   "source": [
    "# Tooo este c√≥digo era una prueba para revisar si los embeddings se hab√≠an hecho bien\n",
    "\n",
    "from openai import OpenAI\n",
    "oai_client = OpenAI(api_key=\"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\")\n",
    "OPENAI_MODEL = \"text-embedding-3-small\"\n",
    "\n",
    "QUESTION = \"¬øQu√© es un autoencoder y c√≥mo se entrena?\"\n",
    "TOP_K = 5\n",
    "\n",
    "# 1) Embeber la pregunta con OpenAI\n",
    "qvec = oai_client.embeddings.create(\n",
    "    model=OPENAI_MODEL,\n",
    "    input=QUESTION\n",
    ").data[0].embedding\n",
    "\n",
    "# 2) Consultar \n",
    "res = collection.query(\n",
    "    query_embeddings=[qvec],\n",
    "    n_results=TOP_K,\n",
    "    include=[\"metadatas\", \"distances\", \"documents\"]\n",
    ")\n",
    "\n",
    "for rank, (doc, meta, dist) in enumerate(zip(res[\"documents\"][0],\n",
    "                                             res[\"metadatas\"][0],\n",
    "                                             res[\"distances\"][0]), start=1):\n",
    "    print(f\"\\n#{rank}  dist={dist:.4f}  base={meta.get('filename_base')}  palabras={meta.get('word_len')}\")\n",
    "    print(meta.get(\"chunk_path\"))\n",
    "    print(doc[:500].replace(\"\\n\",\" \") + (\"...\" if len(doc)>500 else \"\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49b0e44",
   "metadata": {},
   "source": [
    "# Fase 4. Herramientas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8f3d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Respuesta RAG:\n",
      " El contexto proporcionado no incluye informaci√≥n sobre el color del cielo. \n",
      "\n",
      "**Sugerencia:**\n",
      "- Puedes investigar sobre la dispersi√≥n de la luz en la atm√≥sfera o consultar fuentes sobre meteorolog√≠a para obtener una respuesta precisa.\n",
      "\n",
      "üîó Fuentes:\n",
      "[Fuente 1] 1_Semana_AI_20250807_1_RodolfoDavidAcunaLopez_IntroduccionIA  |  score=0.224  |  chunks_sliding/1_Semana_AI_20250807_1_RodolfoDavidAcunaLopez_IntroduccionIA/chunk_0005.txt\n",
      "[Fuente 2] 6_Semana_AI_20250911_1_AndreyUrenaBermudez_RegresionLogisticaGradiente  |  score=0.218  |  chunks_sliding/6_Semana_AI_20250911_1_AndreyUrenaBermudez_RegresionLogisticaGradiente/chunk_0001.txt\n",
      "[Fuente 3] 6_Semana_AI_20250911_2_SahidRojasChacon_VerosimilitudRegresionLogistica  |  score=0.217  |  chunks_sliding/6_Semana_AI_20250911_2_SahidRojasChacon_VerosimilitudRegresionLogistica/chunk_0001.txt\n",
      "[Fuente 4] 4_Semana_AI_20250826_2_LuisFelipeCalderonPerez_KNNRegresionLineal  |  score=0.207  |  chunks_sliding/4_Semana_AI_20250826_2_LuisFelipeCalderonPerez_KNNRegresionLineal/chunk_0001.txt\n",
      "[Fuente 5] 6_Semana_AI_20250909_2_AshleyVasquez_RegresionLogisticaSigmoide  |  score=0.202  |  chunks_sliding/6_Semana_AI_20250909_2_AshleyVasquez_RegresionLogisticaSigmoide/chunk_0003.txt\n"
     ]
    }
   ],
   "source": [
    "# ========================= RAG: retrieve + answer =========================\n",
    "# Esta fue una prueba de curiosidad con el RAG jaja, la idea es que quede bien y se formalice como herramienta\n",
    "from openai import OpenAI\n",
    "import textwrap\n",
    "\n",
    "OPENAI_API_KEY = \"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\"  # ya la usas arriba\n",
    "ANSWER_MODEL   = \"gpt-4o-mini\"    \n",
    "\n",
    "oai = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "def embed_query(q: str, model=\"text-embedding-3-small\"):\n",
    "    return oai.embeddings.create(model=model, input=q).data[0].embedding\n",
    "\n",
    "def retrieve(query: str, top_k: int = 5, as_similarity: bool = True):\n",
    "    qvec = embed_query(query)\n",
    "    res = collection.query(\n",
    "        query_embeddings=[qvec],\n",
    "        n_results=top_k,\n",
    "        include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "    )\n",
    "    out = []\n",
    "    for doc, meta, dist in zip(res[\"documents\"][0], res[\"metadatas\"][0], res[\"distances\"][0]):\n",
    "        score = (1 - dist) if as_similarity else dist\n",
    "        out.append({\"text\": doc, \"meta\": meta, \"score\": score})\n",
    "    return out\n",
    "\n",
    "def build_context(chunks):\n",
    "    blocks = []\n",
    "    for i, c in enumerate(chunks, 1):\n",
    "        header = f\"[Fuente {i}] base={c['meta'].get('filename_base')} | palabras={c['meta'].get('word_len')} | score={c['score']:.3f}\"\n",
    "        body = textwrap.shorten(c[\"text\"].replace(\"\\n\", \" \"), width=1200, placeholder=\" ...\")\n",
    "        blocks.append(header + \"\\n\" + body)\n",
    "    return \"\\n\\n\".join(blocks)\n",
    "\n",
    "def answer_rag(query: str, top_k: int = 5):\n",
    "    chunks = retrieve(query, top_k=top_k, as_similarity=True)\n",
    "    context = build_context(chunks)\n",
    "    system = (\n",
    "        \"Eres un asistente t√©cnico. Responde SOLO usando el CONTEXTO si es suficiente. \"\n",
    "        \"Si faltan datos, dilo y sugiere pasos. Devuelve una respuesta clara y breve, con bullets si ayuda.\"\n",
    "    )\n",
    "    user = f\"Pregunta:\\n{query}\\n\\nCONTEXTO:\\n{context}\\n\\nInstrucci√≥n: responde citando [Fuente i] cuando corresponda.\"\n",
    "    resp = oai.chat.completions.create(\n",
    "        model=ANSWER_MODEL,\n",
    "        messages=[{\"role\":\"system\",\"content\":system},{\"role\":\"user\",\"content\":user}],\n",
    "        temperature=0.2\n",
    "    )\n",
    "    return resp.choices[0].message.content, chunks\n",
    "\n",
    "# Ejemplo de uso:\n",
    "query = \"¬øQu√© es un autoencoder y c√≥mo se entrena?\"\n",
    "respuesta, fuentes = answer_rag(query, top_k=5)\n",
    "print(\"üß† Respuesta RAG:\\n\", respuesta)\n",
    "print(\"\\nüîó Fuentes:\")\n",
    "for i, f in enumerate(fuentes, 1):\n",
    "    print(f\"[Fuente {i}] {f['meta'].get('filename_base')}  |  score={f['score']:.3f}  |  {f['meta'].get('chunk_path')}\")\n",
    "# ========================================================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63b96bb",
   "metadata": {},
   "source": [
    "# Fase 5.  Perfil, orquestacion y memoria del agente LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83fc97e",
   "metadata": {},
   "source": [
    "# Fase 6. Aplicaci√≥n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv) Proyecto_Agente",
   "language": "python",
   "name": "proyecto_agente_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
